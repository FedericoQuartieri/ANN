{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-in9ArBRwrp"
      },
      "outputs": [],
      "source": [
        "# OPTIMAL HYPERPARAMETERS - Based on empirical results\n",
        "\n",
        "PATIENCE = 40\n",
        "VERBOSE = 10\n",
        "\n",
        "RNN_TYPE = 'GRU'\n",
        "BIDIRECTIONAL = True\n",
        "L1_LAMBDA = 0\n",
        "LAMBDA_STD = 1\n",
        "POWERFUL_GPU = True  # Use this macro to enable optimizations for powerful GPUs\n",
        "\n",
        "\n",
        "# Reverse mapping from integers to pain level names\n",
        "label_reverse_mapping = {\n",
        "    0: 'no_pain',\n",
        "    1: 'low_pain',\n",
        "    2: 'high_pain'\n",
        "}\n",
        "\n",
        "# Create mapping dictionary for pain levels\n",
        "pain_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2\n",
        "}\n",
        "\n",
        "labels = ['no_pain', 'low_pain', 'high_pain']\n",
        "\n",
        "num_classes = len(labels)\n",
        "\n",
        "\n",
        "\n",
        "#test\n",
        "param_grid = {\n",
        "    # split robusto\n",
        "    'n_val_users':   [55],\n",
        "    'k':             [1],\n",
        "    'epochs':        [1],\n",
        "\n",
        "    # CONTEXT – metti qui i TUOI best\n",
        "    'window_size':   [24],   # <-- sostituisci con il tuo best se diverso\n",
        "    'stride':        [4],    # <-- idem\n",
        "\n",
        "    # ARCH – fissiamo la famiglia che va meglio\n",
        "    'hidden_layers': [2],\n",
        "    'hidden_size':   [128],\n",
        "\n",
        "    # OPT / REGULARIZATION – qui facciamo il vero finer tuning\n",
        "    'batch_size':    [256],\n",
        "    'learning_rate': [1e-3],\n",
        "    'dropout_rate':  [0.3],        # 1  (meno o più reg)\n",
        "    'l2_lambda':     [1e-4]\n",
        "    # se il runner li richiede:\n",
        "    # 'rnn_type':     ['LSTM'],\n",
        "    # 'bidirectional':[True],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-95VNzsCBdWV",
        "outputId": "97f86cf0-b3f2-4404-ee15-aa020b95c073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG TORCH:\n",
            "  torch.__version__     = 2.9.0+cu128\n",
            "  torch.version.cuda    = 12.8\n",
            "  torch.cuda.is_available() = False\n",
            "  torch.cuda.device_count()  = 0\n",
            "  selected device = cpu\n",
            "PyTorch version: 2.9.0+cu128\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os, random, numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def physical_cores():\n",
        "    try:\n",
        "        import psutil\n",
        "        n = psutil.cpu_count(logical=False)\n",
        "        if n:\n",
        "            return n\n",
        "    except Exception:\n",
        "        pass\n",
        "    n = os.cpu_count() or 2\n",
        "    return max(1, n // 2)  # stima fisici se non disponibile\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "CONVERTIBLE = False\n",
        "\n",
        "\n",
        "# Env PRIMA di import torch\n",
        "CORES = physical_cores()\n",
        "OMP = max(1, CORES - 1)\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", str(OMP))\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", str(OMP))\n",
        "os.environ.setdefault(\"MKL_DYNAMIC\", \"FALSE\")\n",
        "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", str(OMP))\n",
        "os.environ.setdefault(\"TORCH_NUM_INTEROP_THREADS\", \"1\")\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "torch.set_num_threads(int(os.environ[\"OMP_NUM_THREADS\"]))\n",
        "try:\n",
        "    torch.set_num_interop_threads(int(os.environ[\"TORCH_NUM_INTEROP_THREADS\"]))\n",
        "except RuntimeError as e:\n",
        "    print(\"skip set_num_interop_threads:\", e)\n",
        "\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import os, subprocess, shlex\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "# Device selection: prefer CUDA when available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "print(\"DEBUG TORCH:\")\n",
        "print(\"  torch.__version__     =\", torch.__version__)\n",
        "print(\"  torch.version.cuda    =\", torch.version.cuda)\n",
        "print(\"  torch.cuda.is_available() =\", torch.cuda.is_available())\n",
        "print(\"  torch.cuda.device_count()  =\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        print(\"  GPU name:\", torch.cuda.get_device_name(0))\n",
        "    except Exception as e:\n",
        "        print(\"  get_device_name error:\", e)\n",
        "print(\"  selected device =\", device)\n",
        "\n",
        "\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if not CONVERTIBLE:\n",
        "\n",
        "    # Configure plot display settings\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.set_style('white')\n",
        "    plt.rc('font', size=14)\n",
        "    %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsV2HL_gD9cl",
        "outputId": "d0164d29-8c35-4e2b-832b-da438ed3c80e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'out/preprocessed_train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mout/preprocessed_train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m y_train = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mout/preprocessed_labels.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# First map the labels in y_train\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Then merge with X_train based on sample_index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ANN/env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ANN/env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ANN/env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ANN/env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ANN/env/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'out/preprocessed_train.csv'"
          ]
        }
      ],
      "source": [
        "X_train = pd.read_csv('out/preprocessed_train.csv')\n",
        "y_train = pd.read_csv('out/preprocessed_labels.csv')\n",
        "\n",
        "\n",
        "# First map the labels in y_train\n",
        "\n",
        "# Then merge with X_train based on sample_index\n",
        "X_train = X_train.merge(\n",
        "    y_train[['sample_index', 'label']],\n",
        "    on='sample_index',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Rename the column\n",
        "\n",
        "# Verify the mapping worked correctly\n",
        "print(\"\\nFirst few rows of X_train with encoded labels:\")\n",
        "print(X_train[['sample_index', 'label']].head(10))\n",
        "\n",
        "print(\"\\nLabel value counts:\")\n",
        "print(X_train['label'].value_counts())\n",
        "\n",
        "print(\"\\nCheck for NaN labels:\")\n",
        "print(f\"NaN count: {X_train['label'].isna().sum()}\")\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acRv6ssvQQSM",
        "outputId": "b675e6a7-3832-4df2-c020-ba88a4244ea0"
      },
      "outputs": [],
      "source": [
        "# Drop joint_30 column (contains only NaN values)\n",
        "print(\"Dropping joint_30 column (all NaN values)...\")\n",
        "for df in [X_train]:\n",
        "    if 'joint_30' in df.columns:\n",
        "        df.drop('joint_30', axis=1, inplace=True)\n",
        "        print(f\"Dropped joint_30 from {df.shape}\")\n",
        "\n",
        "print(\"\\nColumns after dropping joint_30:\")\n",
        "print(f\"X_train columns: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2dx6K_ZQQSN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9NAklF6FP2G",
        "outputId": "17b3ec14-e3ad-4887-e434-64b1a965c1c1"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Data structure ---\")\n",
        "print(\"\\nX_train Info:\")\n",
        "X_train.info(verbose=True)\n",
        "print(f\"\\nMissing values in X_train: {X_train.isnull().sum().sum()}\")\n",
        "print(\"\\ny_train Info:\")\n",
        "y_train.info(verbose=True)\n",
        "print(f\"\\nMissing values in y_train: {y_train.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "NBmYSucNGROn",
        "outputId": "1a3ec232-3dec-46d6-af15-190b94f91158"
      },
      "outputs": [],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3yOX19MQQSR"
      },
      "outputs": [],
      "source": [
        "def build_sequences(df, window=200, stride=50):\n",
        "    \"\"\"\n",
        "    Build sequences from time-series data\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with the data\n",
        "        window: Window size for sequences\n",
        "        stride: Stride for overlapping windows\n",
        "\n",
        "    Returns:\n",
        "        dataset: numpy array of sequences\n",
        "        labels: numpy array of labels\n",
        "    \"\"\"\n",
        "    # Initialise lists to store sequences and their corresponding labels\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over unique IDs in the DataFrame\n",
        "    for sample_id in df['sample_index'].unique():\n",
        "        # Extract sensor data for the current ID\n",
        "        drop_cols = [c for c in ['sample_index', 'time', 'label', 'labels'] if c in df.columns]\n",
        "        temp = df[df['sample_index'] == sample_id].drop(columns=drop_cols).values.astype('float32')\n",
        "\n",
        "        # Retrieve the activity label for the current ID\n",
        "        label_series = df[df['sample_index'] == sample_id]['label']\n",
        "\n",
        "        # Check if label column exists and has values\n",
        "        if label_series.empty:\n",
        "            print(f\"Warning: No label found for sample_id {sample_id}\")\n",
        "            continue\n",
        "\n",
        "        label_value = label_series.values[0]\n",
        "\n",
        "        # Skip samples with NaN labels\n",
        "        if pd.isna(label_value):\n",
        "            print(f\"Warning: NaN label for sample_id {sample_id}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Convert to int and validate\n",
        "        try:\n",
        "            label = int(label_value)\n",
        "            if label < 0 or label > 2:  # Assuming 3 classes: 0, 1, 2\n",
        "                print(f\"Warning: Invalid label {label} for sample_id {sample_id}, skipping...\")\n",
        "                continue\n",
        "        except (ValueError, TypeError) as e:\n",
        "            print(f\"Warning: Cannot convert label {label_value} to int for sample_id {sample_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        # --- NEW: niente padding a multipli di window ---\n",
        "        L = len(temp)\n",
        "\n",
        "        # PADDING SOLO se L < window (così hai almeno 1 finestra)\n",
        "        if L < window:\n",
        "            pad = window - L\n",
        "            temp = np.concatenate([temp, np.zeros((pad, temp.shape[1]), dtype='float32')], axis=0)\n",
        "            L = len(temp)  # ora L == window\n",
        "\n",
        "        # Genera gli start con lo stride, SENZA espandere a multipli di window\n",
        "        starts = list(range(0, max(L - window, 0) + 1, stride))\n",
        "\n",
        "        # (opzionale ma consigliato) ancora l’ultima finestra alla fine se non allineata\n",
        "        last_start = L - window\n",
        "        if last_start >= 0 and (len(starts) == 0 or starts[-1] != last_start):\n",
        "            starts.append(last_start)\n",
        "\n",
        "        for s in starts:\n",
        "            dataset.append(temp[s:s+window])\n",
        "            labels.append(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Convert lists to numpy arrays for further processing\n",
        "    dataset = np.array(dataset, dtype='float32')\n",
        "    labels = np.array(labels, dtype='int64')\n",
        "\n",
        "    print(f\"Built {len(dataset)} sequences with {len(labels)} labels\")\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7SdOdxjQQSS"
      },
      "outputs": [],
      "source": [
        "#BATCH_SIZE = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0ypRlR4QQST"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches ahead\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpR5aspoQQST"
      },
      "outputs": [],
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKJ48pWqQQST",
        "outputId": "559895c2-fdab-4852-b04d-8b5ac0f2e3f9"
      },
      "outputs": [],
      "source": [
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic RNN classifier (RNN, LSTM, GRU).\n",
        "    Uses the last hidden state for classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Map string name to PyTorch RNN class\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout is only applied between layers (if num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "\n",
        "        # Create the recurrent layer\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calculate input size for the final classifier\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size = hidden_size\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        rnn_out, hidden = self.rnn(x)\n",
        "\n",
        "        # LSTM returns (h_n, c_n), we only need h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "\n",
        "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
        "            # Final shape: (batch_size, hidden_size * 2)\n",
        "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
        "        else:\n",
        "            # Take the last layer's hidden state\n",
        "            # Final shape: (batch_size, hidden_size)\n",
        "            hidden_to_classify = hidden[-1]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=0.,\n",
        "    rnn_type='RNN'\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1HTpkmJQQSU"
      },
      "outputs": [],
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S4P5W1AQQSU"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnCsqUqtQQSU"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLYRdcDrQQSU"
      },
      "outputs": [],
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ITUJXpAQQSV"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynGd49opQQSV"
      },
      "source": [
        "# KFOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T-RUCTFQQSV"
      },
      "outputs": [],
      "source": [
        "def k_shuffle_split_cross_validation_round_rnn(df, epochs, criterion, device,\n",
        "                            k, n_val_users, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
        "                            window_size, stride, rnn_type, bidirectional,\n",
        "                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Perform K-fold shuffle split cross-validation with sample-based splitting for Pirate Pain time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['sample_index', 'time', 'label', 'pain_survey_*', 'joint_*', 'n_legs', 'n_hands', 'n_eyes']\n",
        "        epochs: Number of training epochs\n",
        "        criterion: Loss function\n",
        "        device: torch.device for computation\n",
        "        k: Number of cross-validation splits\n",
        "        n_val_users: Number of samples for validation set\n",
        "        n_test_users: Number of samples for test set\n",
        "        batch_size: Batch size for training\n",
        "        hidden_layers: Number of recurrent layers\n",
        "        hidden_size: Hidden state dimensionality\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        dropout_rate: Dropout rate\n",
        "        window_size: Length of sliding windows\n",
        "        stride: Step size for sliding windows\n",
        "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "        bidirectional: Whether to use bidirectional RNN\n",
        "        l1_lambda: L1 regularization coefficient (if used)\n",
        "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
        "        patience: Early stopping patience\n",
        "        evaluation_metric: Metric to monitor for early stopping\n",
        "        mode: 'max' or 'min' for evaluation metric\n",
        "        restore_best_weights: Whether to restore best weights after training\n",
        "        writer: TensorBoard writer\n",
        "        verbose: Verbosity level\n",
        "        seed: Random seed\n",
        "        experiment_name: Name for experiment logging\n",
        "\n",
        "    Returns:\n",
        "        fold_losses: Dict with validation losses for each split\n",
        "        fold_metrics: Dict with validation F1 scores for each split\n",
        "        best_scores: Dict with best F1 score for each split plus mean and std\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise containers for results across all splits\n",
        "    fold_losses = {}\n",
        "    fold_metrics = {}\n",
        "    best_scores = {}\n",
        "\n",
        "\n",
        "    # Define columns to normalize\n",
        "    # Define columns to normalize (all columns except sample_index, time, and label)\n",
        "    exclude_columns = ['sample_index', 'time', 'label']\n",
        "    scale_columns = [col for col in df.columns if col not in exclude_columns]\n",
        "\n",
        "    # Get model architecture parameters\n",
        "    # Count features (excluding sample_index, time, label)\n",
        "    feature_cols = scale_columns  # All features that will be used\n",
        "    in_features = len(feature_cols)\n",
        "    num_classes = 3  # no_pain, low_pain, high_pain\n",
        "\n",
        "    # Initialise model architecture\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=in_features,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=hidden_layers,\n",
        "        num_classes=num_classes,\n",
        "        dropout_rate=dropout_rate,\n",
        "        bidirectional=bidirectional,\n",
        "        rnn_type=rnn_type\n",
        "    ).to(device)\n",
        "\n",
        "    # Store initial weights to reset model for each split\n",
        "    initial_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Iterate through K random splits\n",
        "    for split_idx in range(k):\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"Split {split_idx+1}/{k}\")\n",
        "\n",
        "        # Get unique sample IDs and shuffle them with split-specific seed\n",
        "        unique_samples = df['sample_index'].unique()\n",
        "        random.seed(seed + split_idx)\n",
        "        random.shuffle(unique_samples)\n",
        "\n",
        "        # Calculate the number of samples for the training set\n",
        "        n_train_samples = len(unique_samples) - n_val_users\n",
        "\n",
        "        # Split the shuffled sample IDs into training, validation, and test sets\n",
        "        train_samples = unique_samples[:n_train_samples]\n",
        "        val_samples = unique_samples[n_train_samples:n_train_samples + n_val_users]\n",
        "\n",
        "        # Split the dataset into training, validation, and test sets based on sample IDs\n",
        "        df_train = df[df['sample_index'].isin(train_samples)].copy()\n",
        "        df_val = df[df['sample_index'].isin(val_samples)].copy()\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training set shape: {df_train.shape}\")\n",
        "            print(f\"  Validation set shape: {df_val.shape}\")\n",
        "\n",
        "        # Map pain labels to integers (if not already mapped)\n",
        "        if df_train['label'].dtype == 'object':\n",
        "            df_train['label'] = df_train['label'].map(pain_mapping)\n",
        "            df_val['label'] = df_val['label'].map(pain_mapping)\n",
        "\n",
        "\n",
        "\n",
        "        # Build sequences using the existing build_sequences function\n",
        "        X_train, y_train = build_sequences(df_train, window=window_size, stride=stride)\n",
        "        X_val, y_val = build_sequences(df_val, window=window_size, stride=stride)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training sequences shape: {X_train.shape}\")\n",
        "            print(f\"  Validation sequences shape: {X_val.shape}\")\n",
        "\n",
        "        # Calculate class weights to handle imbalance\n",
        "        class_counts = np.bincount(y_train)\n",
        "        total_samples = len(y_train)\n",
        "        class_weights = total_samples / (num_classes * class_counts)\n",
        "        class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "        \n",
        "        if verbose > 0:\n",
        "            print(f\"  Class distribution in training: {class_counts}\")\n",
        "            print(f\"  Class weights: {class_weights}\")\n",
        "\n",
        "        # Create weighted loss function for this split\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "        # Create PyTorch datasets\n",
        "        train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "        val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "        # Reset model to initial weights for fair comparison across splits\n",
        "        model.load_state_dict(initial_state)\n",
        "\n",
        "        # Define optimizer with L2 regularization\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "        # Enable mixed precision training for GPU acceleration\n",
        "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # Create directory for model checkpoints\n",
        "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
        "\n",
        "        # Train model on current split with weighted loss\n",
        "        model, training_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=epochs,\n",
        "            criterion=criterion_weighted,  # Use weighted criterion\n",
        "            optimizer=optimizer,\n",
        "            scaler=split_scaler,\n",
        "            device=device,\n",
        "            writer=writer,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            l1_lambda=l1_lambda,\n",
        "            evaluation_metric=evaluation_metric,\n",
        "            mode=mode,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n",
        "        )\n",
        "\n",
        "        # Store results for this split\n",
        "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
        "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
        "        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n",
        "\n",
        "    # Compute mean and standard deviation of best scores across splits\n",
        "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Best score: {best_scores['mean']:.4f}±{best_scores['std']:.4f}\")\n",
        "\n",
        "    return fold_losses, fold_metrics, best_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yt6vaquQQSV"
      },
      "outputs": [],
      "source": [
        "def grid_search_cv_rnn(df, param_grid, fixed_params, cv_params, verbose=True):\n",
        "    \"\"\"\n",
        "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
        "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
        "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
        "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
        "        verbose: Print progress for each configuration\n",
        "\n",
        "    Returns:\n",
        "        results: Dict with scores for each configuration\n",
        "        best_config: Dict with best hyperparameter combination\n",
        "        best_score: Best mean F1 score achieved\n",
        "    \"\"\"\n",
        "    # Generate all parameter combinations\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    combinations = list(product(*param_values))\n",
        "\n",
        "    results = {}\n",
        "    best_score = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    total = len(combinations)\n",
        "\n",
        "    for idx, combo in enumerate(combinations, 1):\n",
        "        # Create current configuration dict\n",
        "        current_config = dict(zip(param_names, combo))\n",
        "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nConfiguration {idx}/{total}:\")\n",
        "            for param, value in current_config.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "\n",
        "        # Merge current config with fixed parameters\n",
        "        run_params = {**fixed_params, **current_config}\n",
        "\n",
        "        # Execute cross-validation\n",
        "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "            df=df,\n",
        "            experiment_name=config_str,\n",
        "            **run_params,\n",
        "            **cv_params\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results[config_str] = fold_scores\n",
        "\n",
        "\n",
        "        current_score = fold_scores[\"mean\"] - LAMBDA_STD * fold_scores[\"std\"]\n",
        "        fold_scores[\"mean_minus_lambda_std\"] = current_score\n",
        "\n",
        "\n",
        "        # Track best configuration using mean - std\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_config = current_config.copy()\n",
        "            if verbose:\n",
        "                print(f\"  NEW BEST SCORE! (mean - std = {current_score:.4f})\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"  F1 Score: {fold_scores['mean']:.4f}±{fold_scores['std']:.4f} \"\n",
        "                f\"(mean - std = {current_score:.4f})\"\n",
        "            )\n",
        "\n",
        "    return results, best_config, best_score\n",
        "\n",
        "\n",
        "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
        "\n",
        "    Args:\n",
        "        results: Dict of results from grid_search_cv_rnn\n",
        "        k_splits: Number of CV splits used\n",
        "        top_n: Number of top configurations to display\n",
        "        figsize: Figure size tuple\n",
        "    \"\"\"\n",
        "    # Sort by mean score\n",
        "    config_scores = {name: data['mean'] for name, data in results.items()}\n",
        "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top N\n",
        "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
        "\n",
        "    # Prepare boxplot data\n",
        "    boxplot_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
        "    replacements = {\n",
        "        'batch_size_': 'BS=',\n",
        "        'learning_rate_': '\\nLR=',\n",
        "        'hidden_layers_': '\\nHL=',\n",
        "        'hidden_size_': '\\nHS=',\n",
        "        'dropout_rate_': '\\nDR=',\n",
        "        'window_size_': '\\nWS=',\n",
        "        'stride_': '\\nSTR=',\n",
        "        'rnn_type_': '\\nRNN=',\n",
        "        'bidirectional_': '\\nBIDIR=',\n",
        "        'l1_lambda_': '\\nL1=',\n",
        "        'l2_lambda_': '\\nL2='\n",
        "    }\n",
        "\n",
        "    # Replacements for separators\n",
        "    separator_replacements = {\n",
        "        '_learning_rate_': '\\nLR=',\n",
        "        '_hidden_layers_': '\\nHL=',\n",
        "        '_hidden_size_': '\\nHS=',\n",
        "        '_dropout_rate_': '\\nDR=',\n",
        "        '_window_size_': '\\nWS=',\n",
        "        '_stride_': '\\nSTR=',\n",
        "        '_rnn_type_': '\\nRNN=',\n",
        "        '_bidirectional_': '\\nBIDIR=',\n",
        "        '_l1_lambda_': '\\nL1=',\n",
        "        '_l2_lambda_': '\\nL2=',\n",
        "        '_': ''\n",
        "    }\n",
        "\n",
        "    for config_name, mean_score in top_configs:\n",
        "        # Extract best score from each split (auto-detect number of splits)\n",
        "        split_scores = []\n",
        "        for i in range(k_splits):\n",
        "            if f'split_{i}' in results[config_name]:\n",
        "                split_scores.append(results[config_name][f'split_{i}'])\n",
        "        boxplot_data.append(split_scores)\n",
        "\n",
        "        # Verify we have the expected number of splits\n",
        "        if len(split_scores) != k_splits:\n",
        "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
        "\n",
        "        # Create readable label using the replacements dictionary\n",
        "        readable_label = config_name\n",
        "        for old, new in replacements.items():\n",
        "            readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        # Apply separator replacements\n",
        "        for old, new in separator_replacements.items():\n",
        "             readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        labels.append(f\"{readable_label}\\n(μ={mean_score:.3f})\")\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
        "                    showmeans=True, meanline=True)\n",
        "\n",
        "    # Styling\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    # Highlight best configuration\n",
        "    ax.get_xticklabels()[0].set_fontweight('bold')\n",
        "\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "    plt.xticks(rotation=0, ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhgn3M-iQQSW"
      },
      "outputs": [],
      "source": [
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdF2fIryQQSW",
        "outputId": "7ef98fef-a588-4c3c-a11b-ce040603db17"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def timing(label=\"Block\"):\n",
        "    t0 = perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        print(f\"{label} took {perf_counter() - t0:.2f}s\")\n",
        "\n",
        "with timing(\"Grid search\"):\n",
        "    fixed_params = {\n",
        "        'l1_lambda': L1_LAMBDA,\n",
        "        'rnn_type': RNN_TYPE,\n",
        "        'bidirectional': BIDIRECTIONAL,\n",
        "    }\n",
        "    cv_params = {\n",
        "        'criterion': criterion,\n",
        "        'device': device,\n",
        "        'patience': PATIENCE,\n",
        "        'verbose': 0,\n",
        "        'seed': SEED,\n",
        "        'evaluation_metric': 'val_f1',\n",
        "        'mode': 'max',\n",
        "        'restore_best_weights': True,\n",
        "        'writer': None,\n",
        "    }\n",
        "    results, best_config, best_score = grid_search_cv_rnn(\n",
        "        df=X_train,\n",
        "        param_grid=param_grid,\n",
        "        fixed_params=fixed_params,\n",
        "        cv_params=cv_params,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "705XWZjNQQSW",
        "outputId": "db2b8d06-5ff1-4528-bb4d-a5fdfb7defad"
      },
      "outputs": [],
      "source": [
        "plot_top_configurations_rnn(results, k_splits=param_grid['k'][0], top_n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 1. CV SUMMARY: Mean, Std, 95% CI, Lower Bound\n",
        "# ============================================\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compute statistics for each configuration\n",
        "config_stats = []\n",
        "k = param_grid['k'][0]\n",
        "\n",
        "for config_name, fold_scores in results.items():\n",
        "    # Extract individual fold scores\n",
        "    split_scores = [fold_scores[f'split_{i}'] for i in range(k) if f'split_{i}' in fold_scores]\n",
        "    \n",
        "    mean_f1 = fold_scores['mean']\n",
        "    std_f1 = fold_scores['std']\n",
        "    \n",
        "    # 95% Confidence Interval (t-distribution)\n",
        "    # For k=5: df=4, t_critical ≈ 2.776\n",
        "    t_critical = stats.t.ppf(0.975, k-1)\n",
        "    margin_error = t_critical * (std_f1 / np.sqrt(k))\n",
        "    ci_lower = mean_f1 - margin_error\n",
        "    ci_upper = mean_f1 + margin_error\n",
        "    \n",
        "    # Conservative estimate: mean - std\n",
        "    conservative_estimate = mean_f1 - std_f1\n",
        "    \n",
        "    config_stats.append({\n",
        "        'config': config_name,\n",
        "        'mean': mean_f1,\n",
        "        'std': std_f1,\n",
        "        'ci_lower': ci_lower,\n",
        "        'ci_upper': ci_upper,\n",
        "        'conservative': conservative_estimate,\n",
        "        'cv_score': f\"{mean_f1:.4f}±{std_f1:.4f}\"\n",
        "    })\n",
        "\n",
        "# Sort by mean F1 (descending)\n",
        "config_stats.sort(key=lambda x: x['mean'], reverse=True)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "stats_df = pd.DataFrame(config_stats)\n",
        "\n",
        "print(f\"\\nTop 10 Configurations (sorted by mean F1):\\n\")\n",
        "print(f\"{'Rank':<5} {'Mean F1':<9} {'Std':<8} {'95% CI':<20} {'Conservative':<13} {'Config'}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for idx, row in stats_df.head(10).iterrows():\n",
        "    rank = idx + 1\n",
        "    ci_str = f\"[{row['ci_lower']:.4f}, {row['ci_upper']:.4f}]\"\n",
        "    # Shorten config name for display\n",
        "    short_config = row['config'][:50] + \"...\" if len(row['config']) > 50 else row['config']\n",
        "    \n",
        "    print(f\"{rank:<5} {row['mean']:.4f}    {row['std']:.4f}   {ci_str:<20} {row['conservative']:.4f}        {short_config}\")\n",
        "\n",
        "# Highlight best configuration\n",
        "best_row = stats_df.iloc[0]\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST CONFIGURATION DETAILS:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Configuration: {best_row['config']}\")\n",
        "print(f\"Mean F1:       {best_row['mean']:.4f}\")\n",
        "print(f\"Std Dev:       {best_row['std']:.4f}\")\n",
        "print(f\"95% CI:        [{best_row['ci_lower']:.4f}, {best_row['ci_upper']:.4f}]\")\n",
        "print(f\"Conservative:  {best_row['conservative']:.4f}\")\n",
        "print(f\"\\n📊 Expected Test F1 Range:\")\n",
        "print(f\"  - Optimistic:    {best_row['ci_upper']:.4f}\")\n",
        "print(f\"  - Realistic:     {best_row['mean']:.4f}\")\n",
        "print(f\"  - Conservative:  {best_row['conservative']:.4f}\")\n",
        "print(f\"  - Lower Bound:   {best_row['ci_lower']:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkklZJpQQSb"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Esv7nvO4QQSb",
        "outputId": "eb85a3c0-176a-4a46-8a5c-a2ca5aa41d94"
      },
      "outputs": [],
      "source": [
        "# filepath: /home/federico/Desktop/ANN/grid_search.ipynb\n",
        "\n",
        "# ========================================\n",
        "# STEP 1: PREPARE FINAL TRAINING\n",
        "# ========================================\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Merge best config with fixed params\n",
        "final_best_params = {**fixed_params, **best_config}\n",
        "print(\"\\nBest Configuration Found:\")\n",
        "for key, value in final_best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\nBest CV F1 Score: {best_score:.4f}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 2: CREATE FINAL TRAIN/VAL SPLIT\n",
        "# (Use minimal validation set, maximize training data)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING FINAL TRAIN/VAL SPLIT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use MINIMAL validation set (e.g., 10% of n_val_users from grid search)\n",
        "# This maximizes training data while keeping a small val set for early stopping\n",
        "unique_samples = X_train['sample_index'].unique()\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "np.random.shuffle(unique_samples)\n",
        "\n",
        "\n",
        "# Use smaller validation set than CV (e.g., 20-30 samples instead of 45)\n",
        "n_val_users_final = max(20, int(final_best_params['n_val_users']))  # Half of CV val size\n",
        "n_train_samples = len(unique_samples) - n_val_users_final\n",
        "\n",
        "train_samples = unique_samples[:n_train_samples]\n",
        "val_samples = unique_samples[n_train_samples:]\n",
        "\n",
        "df_train_final = X_train[X_train['sample_index'].isin(train_samples)].copy()\n",
        "df_val_final = X_train[X_train['sample_index'].isin(val_samples)].copy()\n",
        "\n",
        "print(f\"Training samples: {len(train_samples)} ({df_train_final.shape[0]} timesteps)\")\n",
        "print(f\"Validation samples: {len(val_samples)} ({df_val_final.shape[0]} timesteps)\")\n",
        "print(f\"Training data: {len(train_samples)/(len(train_samples)+len(val_samples))*100:.1f}% of total\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 3: PREPROCESS DATA\n",
        "# ========================================\n",
        "# Map labels (if needed)\n",
        "if df_train_final['label'].dtype == 'object':\n",
        "    df_train_final['label'] = df_train_final['label'].map(pain_mapping)\n",
        "    df_val_final['label'] = df_val_final['label'].map(pain_mapping)\n",
        "\n",
        "# Define columns to normalize\n",
        "# Define columns to normalize (all columns except sample_index, time, and label)\n",
        "exclude_columns = ['sample_index', 'time', 'label']\n",
        "scale_columns = [col for col in df_train_final.columns if col not in exclude_columns]\n",
        "\n",
        "\n",
        "# Build sequences\n",
        "print(\"\\nBuilding sequences...\")\n",
        "X_train_final, y_train_final = build_sequences(\n",
        "    df_train_final, \n",
        "    window=final_best_params['window_size'], \n",
        "    stride=final_best_params['stride']\n",
        ")\n",
        "X_val_final, y_val_final = build_sequences(\n",
        "    df_val_final, \n",
        "    window=final_best_params['window_size'], \n",
        "    stride=final_best_params['stride']\n",
        ")\n",
        "\n",
        "print(f\"Training sequences: {X_train_final.shape}\")\n",
        "print(f\"Validation sequences: {X_val_final.shape}\")\n",
        "\n",
        "# Calculate class weights for final training to handle imbalance\n",
        "class_counts_final = np.bincount(y_train_final)\n",
        "total_samples_final = len(y_train_final)\n",
        "class_weights_final = total_samples_final / (num_classes * class_counts_final)\n",
        "class_weights_tensor_final = torch.FloatTensor(class_weights_final).to(device)\n",
        "\n",
        "print(f\"\\nClass distribution in final training set: {class_counts_final}\")\n",
        "print(f\"Class weights: {class_weights_final}\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_ds_final = TensorDataset(\n",
        "    torch.from_numpy(X_train_final.astype(np.float32)), \n",
        "    torch.from_numpy(y_train_final.astype(np.int64))\n",
        ")\n",
        "val_ds_final = TensorDataset(\n",
        "    torch.from_numpy(X_val_final.astype(np.float32)), \n",
        "    torch.from_numpy(y_val_final.astype(np.int64))\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader_final = make_loader(\n",
        "    train_ds_final, \n",
        "    batch_size=final_best_params['batch_size'], \n",
        "    shuffle=True, \n",
        "    drop_last=False\n",
        ")\n",
        "val_loader_final = make_loader(\n",
        "    val_ds_final, \n",
        "    batch_size=final_best_params['batch_size'], \n",
        "    shuffle=False, \n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# STEP 4: INITIALIZE FINAL MODEL\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INITIALIZING FINAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_model = RecurrentClassifier(\n",
        "    input_size=X_train_final.shape[2],\n",
        "    hidden_size=final_best_params['hidden_size'],\n",
        "    num_layers=final_best_params['hidden_layers'],\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=final_best_params['dropout_rate'],\n",
        "    bidirectional=final_best_params['bidirectional'],\n",
        "    rnn_type=final_best_params['rnn_type']\n",
        ").to(device)\n",
        "\n",
        "recurrent_summary(final_model, input_size=(final_best_params['window_size'], X_train_final.shape[2]))\n",
        "\n",
        "# ========================================\n",
        "# STEP 5: TRAIN FINAL MODEL\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING FINAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "optimizer_final = torch.optim.AdamW(\n",
        "    final_model.parameters(), \n",
        "    lr=final_best_params['learning_rate'], \n",
        "    weight_decay=final_best_params['l2_lambda']\n",
        ")\n",
        "scaler_final = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "# Create directory for final model\n",
        "os.makedirs(\"models/final_model\", exist_ok=True)\n",
        "\n",
        "# Create weighted loss function for final training\n",
        "criterion_final = nn.CrossEntropyLoss(weight=class_weights_tensor_final)\n",
        "\n",
        "# Train with MORE epochs than CV (since we have more data)\n",
        "final_epochs = int(final_best_params['epochs'])  # 1.5x more epochs\n",
        "final_patience = PATIENCE\n",
        "print(f\"Training for {final_epochs} epochs\")\n",
        "\n",
        "final_model, training_history = fit(\n",
        "    model=final_model,\n",
        "    train_loader=train_loader_final,\n",
        "    val_loader=val_loader_final,\n",
        "    epochs=final_epochs,\n",
        "    criterion=criterion_final,  # Use weighted criterion\n",
        "    optimizer=optimizer_final,\n",
        "    scaler=scaler_final,\n",
        "    device=device,\n",
        "    l1_lambda=final_best_params['l1_lambda'],\n",
        "    l2_lambda=0,  # Already in optimizer weight_decay\n",
        "    patience=final_patience,\n",
        "    evaluation_metric=\"val_f1\",\n",
        "    mode='max',\n",
        "    restore_best_weights=True,\n",
        "    writer=None,\n",
        "    verbose=VERBOSE,\n",
        "    experiment_name=\"final_model/best\"\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Final model trained!\")\n",
        "print(f\"Best Validation F1: {max(training_history['val_f1']):.4f}\")\n",
        "print(f\"CV F1 Score: {best_score:.4f}\")\n",
        "print(f\"Improvement: {max(training_history['val_f1']) - best_score:+.4f}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 6: LOAD TEST DATA\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPARING TEST DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_test = pd.read_csv('out/preprocessed_test.csv')\n",
        "\n",
        "\n",
        "\n",
        "df_test_original = X_test.copy()\n",
        "df_test_processed = X_test.copy()\n",
        "\n",
        "\n",
        "# Build test sequences\n",
        "df_test_processed['label'] = 0  # Dummy label\n",
        "X_test_sequences, _ = build_sequences(\n",
        "    df_test_processed,\n",
        "    window=final_best_params['window_size'],\n",
        "    stride=final_best_params['stride']\n",
        ")\n",
        "\n",
        "print(f\"Test sequences: {X_test_sequences.shape}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 7: MAKE PREDICTIONS\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MAKING PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load the best model weights\n",
        "final_model.load_state_dict(torch.load(\"models/final_model/best_model.pt\", map_location=device))\n",
        "final_model.eval()\n",
        "\n",
        "# Create test loader\n",
        "test_ds = TensorDataset(torch.from_numpy(X_test_sequences.astype(np.float32)))\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=final_best_params['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Get predictions\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for (xb,) in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = final_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_predictions.append(preds)\n",
        "\n",
        "all_predictions = np.concatenate(all_predictions)\n",
        "print(f\"Total window predictions: {len(all_predictions)}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 8: MAP PREDICTIONS TO SAMPLES  (coerente con build_sequences)\n",
        "# ========================================\n",
        "sample_predictions = []\n",
        "pred_idx = 0\n",
        "\n",
        "unique_samples = df_test_processed['sample_index'].unique()\n",
        "window = final_best_params['window_size']\n",
        "stride = final_best_params['stride']\n",
        "\n",
        "def n_windows_like_training(n_timestamps: int, window: int, stride: int) -> int:\n",
        "    \"\"\"Conta le finestre esattamente come in build_sequences:\n",
        "       - se L < window: 1 finestra (padding solo fino a window)\n",
        "       - altrimenti sliding con stride\n",
        "       - ancora l’ultima finestra alla fine se non già inclusa\n",
        "    \"\"\"\n",
        "    L = n_timestamps\n",
        "    if L <= 0:\n",
        "        return 0\n",
        "    if L < window:\n",
        "        return 1\n",
        "\n",
        "    starts = list(range(0, max(L - window, 0) + 1, stride))\n",
        "    last_start = L - window\n",
        "    if last_start >= 0 and (len(starts) == 0 or starts[-1] != last_start):\n",
        "        starts.append(last_start)\n",
        "    return len(starts)\n",
        "\n",
        "print(f\"\\nMapping {len(all_predictions)} windows to {len(unique_samples)} samples...\")\n",
        "\n",
        "for sample_id in unique_samples:\n",
        "    L = (df_test_processed['sample_index'] == sample_id).sum()\n",
        "    n_windows = n_windows_like_training(L, window, stride)\n",
        "\n",
        "    end_idx = min(pred_idx + n_windows, len(all_predictions))\n",
        "    sample_window_predictions = all_predictions[pred_idx:end_idx]\n",
        "\n",
        "    if sample_window_predictions.size > 0:\n",
        "        final_prediction = np.bincount(sample_window_predictions).argmax()\n",
        "    else:\n",
        "        final_prediction = 0  # fallback prudente\n",
        "\n",
        "    sample_predictions.append({\n",
        "        'sample_index': sample_id,\n",
        "        'predicted_label_id': final_prediction,\n",
        "        'predicted_label': label_reverse_mapping[final_prediction]\n",
        "    })\n",
        "\n",
        "    pred_idx = end_idx\n",
        "\n",
        "# sicurezza: deve consumare tutte le finestre\n",
        "assert pred_idx == len(all_predictions), \\\n",
        "    f\"Consumed {pred_idx} preds but have {len(all_predictions)}\"\n",
        "# ========================================\n",
        "# STEP 9: SAVE RESULTS\n",
        "# ========================================\n",
        "predictions_df = pd.DataFrame(sample_predictions)\n",
        "\n",
        "output_df = df_test_original[['sample_index']].drop_duplicates().merge(\n",
        "    predictions_df,\n",
        "    on='sample_index',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Handle missing predictions\n",
        "if output_df['predicted_label'].isna().any():\n",
        "    print(f\"WARNING: {output_df['predicted_label'].isna().sum()} samples missing predictions\")\n",
        "    output_df['predicted_label'].fillna('no_pain', inplace=True)\n",
        "\n",
        "# Save CSV\n",
        "output_filename = 'pirate_pain_test_predictions.csv'\n",
        "output_df[['sample_index', 'predicted_label']].to_csv(output_filename, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTIONS SAVED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Output file: {output_filename}\")\n",
        "print(f\"Total samples: {len(output_df)}\")\n",
        "print(f\"\\n📊 Performance Comparison:\")\n",
        "print(f\"  CV Mean F1: {best_score:.4f}\")\n",
        "print(f\"  Final Model Val F1: {max(training_history['val_f1']):.4f}\")\n",
        "print(f\"  Improvement: {max(training_history['val_f1']) - best_score:+.4f} ({((max(training_history['val_f1'])/best_score - 1)*100):+.1f}%)\")\n",
        "\n",
        "print(\"\\n📈 Prediction Distribution:\")\n",
        "print(output_df['predicted_label'].value_counts())\n",
        "print(\"\\nPercentages:\")\n",
        "for pain_level in ['no_pain', 'low_pain', 'high_pain']:\n",
        "    count = (output_df['predicted_label'] == pain_level).sum()\n",
        "    percentage = (count / len(output_df)) * 100\n",
        "    print(f\"  {pain_level}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\n🔍 First 10 predictions:\")\n",
        "print(output_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-KsexxeUae42",
        "outputId": "2768c8b8-d33c-4d47-dd4c-a70f3bc2d804"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 3. PER-CLASS METRICS & CONFUSION MATRIX\n",
        "# ============================================\n",
        "# Evaluate the final model on validation set\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'final_model' in globals() and 'val_loader_final' in globals():\n",
        "    # Get predictions on validation set\n",
        "    final_model.eval()\n",
        "    all_val_preds = []\n",
        "    all_val_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader_final:\n",
        "            inputs = inputs.to(device)\n",
        "            logits = final_model(inputs)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_val_preds.append(preds)\n",
        "            all_val_targets.append(targets.numpy())\n",
        "    \n",
        "    y_true = np.concatenate(all_val_targets)\n",
        "    y_pred = np.concatenate(all_val_preds)\n",
        "    \n",
        "    # Classification Report\n",
        "    print(\"\\n📋 Classification Report (Validation Set):\\n\")\n",
        "    target_names = ['no_pain', 'low_pain', 'high_pain']\n",
        "    report = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
        "    print(report)\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=target_names, \n",
        "                yticklabels=target_names,\n",
        "                cbar_kws={'label': 'Count'},\n",
        "                ax=ax)\n",
        "    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Confusion Matrix - Validation Set', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Compute per-class metrics\n",
        "    print(\"\\n📊 Per-Class Analysis:\")\n",
        "    for i, class_name in enumerate(target_names):\n",
        "        # True Positives, False Positives, False Negatives\n",
        "        tp = cm[i, i]\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        tn = cm.sum() - tp - fp - fn\n",
        "        \n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        support = cm[i, :].sum()\n",
        "        \n",
        "        print(f\"\\n  {class_name}:\")\n",
        "        print(f\"    Support:   {support} samples ({support/len(y_true)*100:.1f}%)\")\n",
        "        print(f\"    Precision: {precision:.4f}\")\n",
        "        print(f\"    Recall:    {recall:.4f}\")\n",
        "        print(f\"    F1 Score:  {f1:.4f}\")\n",
        "        \n",
        "        # Check for class imbalance issues\n",
        "        if f1 < 0.7 and support > len(y_true) * 0.1:  # Low F1 for non-trivial class\n",
        "            print(f\"    ⚠️  LOW PERFORMANCE - Consider class balancing or more training data\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "else:\n",
        "    print(\"⚠️  Final model or validation loader not available.\")\n",
        "    print(\"   Run the final model training cell first.\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 4. OVERFITTING DIAGNOSTIC SUMMARY\n",
        "# ============================================\n",
        "# Consolidate all metrics and provide actionable recommendations\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"OVERFITTING DIAGNOSTIC SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. CV Statistics Summary\n",
        "if 'results' in globals() and 'best_config' in globals():\n",
        "    # Build config string from best_config dict\n",
        "    config_parts = []\n",
        "    for key in sorted(best_config.keys()):\n",
        "        config_parts.append(f\"{key}_{best_config[key]}\")\n",
        "    config_str = \"_\".join(config_parts)\n",
        "    \n",
        "    # Find matching result entry\n",
        "    matching_key = None\n",
        "    for k in results.keys():\n",
        "        if config_str in k or all(f\"{key}_{best_config[key]}\" in k for key in best_config.keys()):\n",
        "            matching_key = k\n",
        "            break\n",
        "    \n",
        "    if matching_key is None:\n",
        "        print(f\"⚠️  Could not find exact match for best_config in results.\")\n",
        "        print(f\"   Looking for config containing: {config_str[:80]}...\")\n",
        "        # Use best_score entry from results\n",
        "        if 'best_score' in globals() and results:\n",
        "            # Find entry with mean closest to best_score\n",
        "            best_match = None\n",
        "            min_diff = float('inf')\n",
        "            for k, v in results.items():\n",
        "                if isinstance(v, dict) and 'mean' in v:\n",
        "                    diff = abs(v['mean'] - best_score)\n",
        "                    if diff < min_diff:\n",
        "                        min_diff = diff\n",
        "                        best_match = k\n",
        "            if best_match:\n",
        "                matching_key = best_match\n",
        "                print(f\"   Using closest match by mean F1: {matching_key[:80]}...\")\n",
        "    \n",
        "    if matching_key:\n",
        "        entry = results[matching_key]\n",
        "        # Extract fold scores (split_0, split_1, ...)\n",
        "        f1_scores = []\n",
        "        for key in sorted(entry.keys()):\n",
        "            if key.startswith('split_'):\n",
        "                f1_scores.append(entry[key])\n",
        "        \n",
        "        if len(f1_scores) > 0:\n",
        "            f1_scores = np.array(f1_scores)\n",
        "            best_cv_mean = np.mean(f1_scores)\n",
        "            best_cv_std = np.std(f1_scores, ddof=1)\n",
        "            cv_variability = best_cv_std / best_cv_mean if best_cv_mean > 0 else 0\n",
        "\n",
        "            print(f\"\\n1️⃣ Cross-Validation Stability:\")\n",
        "            print(f\"   Best Config Mean F1:  {best_cv_mean:.4f} ± {best_cv_std:.4f}\")\n",
        "            print(f\"   Coefficient of Var:   {cv_variability:.2%}\")\n",
        "\n",
        "            if cv_variability < 0.05:\n",
        "                print(\"   ✅ STABLE - Low variance across folds (< 5%)\")\n",
        "            elif cv_variability < 0.10:\n",
        "                print(\"   ⚠️  MODERATE - Some variance across folds (5-10%)\")\n",
        "            else:\n",
        "                print(\"   ❌ UNSTABLE - High variance across folds (> 10%)\")\n",
        "                print(\"      → Consider: more training data, ensemble methods, or hyperparameter tuning\")\n",
        "        else:\n",
        "            print(\"⚠️  No split scores found in results entry.\")\n",
        "    else:\n",
        "        print(\"⚠️  Could not locate results entry. Skipping CV stability analysis.\")\n",
        "else:\n",
        "    print(\"⚠️  'results' or 'best_config' not found. Skipping CV stability analysis.\")\n",
        "\n",
        "# 2. Learning Curves Summary\n",
        "if 'training_history' in globals():\n",
        "    # Check for correct keys (train_loss vs train_losses, etc.)\n",
        "    loss_key = 'train_loss' if 'train_loss' in training_history else 'train_losses'\n",
        "    val_loss_key = 'val_loss' if 'val_loss' in training_history else 'val_losses'\n",
        "    f1_key = 'train_f1' if 'train_f1' in training_history else 'train_f1_scores'\n",
        "    val_f1_key = 'val_f1' if 'val_f1' in training_history else 'val_f1_scores'\n",
        "    \n",
        "    if loss_key in training_history and val_loss_key in training_history:\n",
        "        # Final epoch metrics\n",
        "        final_train_loss = training_history[loss_key][-1]\n",
        "        final_val_loss = training_history[val_loss_key][-1]\n",
        "        final_train_f1 = training_history[f1_key][-1]\n",
        "        final_val_f1 = training_history[val_f1_key][-1]\n",
        "\n",
        "        # Best epoch metrics\n",
        "        best_epoch_idx = int(np.argmax(training_history[val_f1_key]))\n",
        "        best_val_f1 = training_history[val_f1_key][best_epoch_idx]\n",
        "        best_train_f1 = training_history[f1_key][best_epoch_idx]\n",
        "\n",
        "        # Compute gaps\n",
        "        loss_gap = abs(final_train_loss - final_val_loss)\n",
        "        loss_gap_pct = (loss_gap / final_val_loss * 100) if final_val_loss > 0 else 0\n",
        "        f1_gap = abs(final_train_f1 - final_val_f1)\n",
        "        f1_gap_pct = (f1_gap / final_val_f1 * 100) if final_val_f1 > 0 else 0\n",
        "\n",
        "        print(f\"\\n2️⃣ Train-Validation Gap Analysis:\")\n",
        "        print(f\"   Final Train Loss:  {final_train_loss:.4f}\")\n",
        "        print(f\"   Final Val Loss:    {final_val_loss:.4f}\")\n",
        "        print(f\"   Loss Gap:          {loss_gap:.4f} ({loss_gap_pct:.2f}%)\")\n",
        "        print(f\"   Final Train F1:    {final_train_f1:.4f}\")\n",
        "        print(f\"   Final Val F1:      {final_val_f1:.4f}\")\n",
        "        print(f\"   F1 Gap:            {f1_gap:.4f} ({f1_gap_pct:.2f}%)\")\n",
        "\n",
        "        if f1_gap_pct < 3:\n",
        "            overfitting_status = \"✅ MINIMAL OVERFITTING\"\n",
        "            overfitting_action = \"Model generalizes well. Proceed with confidence.\"\n",
        "        elif f1_gap_pct < 8:\n",
        "            overfitting_status = \"⚠️  MODERATE OVERFITTING\"\n",
        "            overfitting_action = \"Consider: higher dropout, more L2 regularization, or data augmentation.\"\n",
        "        else:\n",
        "            overfitting_status = \"❌ SIGNIFICANT OVERFITTING\"\n",
        "            overfitting_action = \"Action needed: increase dropout (0.6-0.7), add L2 (3e-4), or get more data.\"\n",
        "\n",
        "        print(f\"\\n   {overfitting_status} ({f1_gap_pct:.2f}% gap)\")\n",
        "        print(f\"   → {overfitting_action}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  training_history keys don't match expected format. Skipping learning curve analysis.\")\n",
        "else:\n",
        "    print(\"\\n⚠️  'training_history' not found. Skipping learning curve analysis.\")\n",
        "\n",
        "# 3. Expected Test Performance\n",
        "if 'f1_scores' in locals() and len(f1_scores) > 0:\n",
        "    # Conservative estimates\n",
        "    conservative_f1 = best_cv_mean - best_cv_std\n",
        "\n",
        "    # Confidence interval (95%)\n",
        "    from scipy.stats import t\n",
        "    n_folds = len(f1_scores)\n",
        "    try:\n",
        "        t_critical = t.ppf(0.975, n_folds - 1)\n",
        "        margin_error = t_critical * (best_cv_std / np.sqrt(n_folds))\n",
        "        ci_lower = best_cv_mean - margin_error\n",
        "        ci_upper = best_cv_mean + margin_error\n",
        "    except Exception:\n",
        "        ci_lower, ci_upper = best_cv_mean - best_cv_std, best_cv_mean + best_cv_std\n",
        "\n",
        "    print(f\"\\n3️⃣ Expected Test Set Performance:\")\n",
        "    print(f\"   Optimistic:        {best_cv_mean:.4f} (CV mean)\")\n",
        "    print(f\"   Realistic:         {ci_lower:.4f} - {ci_upper:.4f} (95% CI)\")\n",
        "    print(f\"   Conservative:      {conservative_f1:.4f} (mean - 1 std)\")\n",
        "    print(f\"   Lower Bound:       {np.min(f1_scores):.4f} (worst fold)\")\n",
        "\n",
        "    print(f\"\\n   🎯 EXPECTED TEST F1: {ci_lower:.4f} to {best_cv_mean:.4f}\")\n",
        "    print(f\"      (Use conservative estimate {conservative_f1:.4f} for planning)\")\n",
        "\n",
        "# 4. Final Recommendations\n",
        "print(f\"\\n4️⃣ Next Steps:\")\n",
        "print(\"   \" + \"─\"*70)\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "# Based on CV stability\n",
        "if 'f1_scores' in locals() and 'cv_variability' in locals():\n",
        "    if cv_variability >= 0.10:\n",
        "        recommendations.append(\"🔄 High fold variance detected - consider stratified k-fold or more folds\")\n",
        "\n",
        "# Based on overfitting\n",
        "if 'f1_gap_pct' in locals():\n",
        "    if f1_gap_pct >= 8:\n",
        "        recommendations.append(\"🛡️  Significant overfitting - increase dropout to 0.6-0.7 or L2 to 2e-4-3e-4\")\n",
        "    elif f1_gap_pct >= 3:\n",
        "        recommendations.append(\"⚖️  Moderate overfitting - monitor next grid search for lower train-val gap\")\n",
        "\n",
        "# Based on per-class performance\n",
        "if 'final_model' in globals() and 'val_loader_final' in globals():\n",
        "    recommendations.append(\"🔍 Review per-class metrics above - consider class balancing if any class < 0.7 F1\")\n",
        "\n",
        "# General next action\n",
        "if len(recommendations) == 0:\n",
        "    recommendations.append(\"✅ Model looks good - run the new 18-config grid search!\")\n",
        "else:\n",
        "    recommendations.append(\"⏭️  Address issues above, then run the new 18-config grid search\")\n",
        "\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"   {i}. {rec}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 2. LEARNING CURVES: Train vs Val (Loss & F1)\n",
        "# ============================================\n",
        "# Note: Uses training_history from the FINAL model training\n",
        "# To see curves from CV, you'd need to modify k_shuffle_split to return histories\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LEARNING CURVES - Detecting Overfitting\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'training_history' in globals() and training_history:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    \n",
        "    # Plot 1: Loss curves\n",
        "    epochs_range = range(1, len(training_history['train_loss']) + 1)\n",
        "    axes[0].plot(epochs_range, training_history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
        "    axes[0].plot(epochs_range, training_history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Highlight best epoch if early stopping was used\n",
        "    if len(training_history['val_loss']) < int(final_best_params['epochs'] * 1.5):\n",
        "        best_epoch_idx = np.argmin(training_history['val_loss'])\n",
        "        axes[0].axvline(x=best_epoch_idx + 1, color='green', linestyle='--', \n",
        "                        label=f'Best Epoch ({best_epoch_idx + 1})', alpha=0.7)\n",
        "        axes[0].legend(fontsize=11)\n",
        "    \n",
        "    # Plot 2: F1 Score curves\n",
        "    axes[1].plot(epochs_range, training_history['train_f1'], 'b-', label='Training F1', linewidth=2)\n",
        "    axes[1].plot(epochs_range, training_history['val_f1'], 'r-', label='Validation F1', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
        "    axes[1].set_title('Training vs Validation F1 Score', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    # Highlight best epoch for F1\n",
        "    best_f1_epoch_idx = np.argmax(training_history['val_f1'])\n",
        "    axes[1].axvline(x=best_f1_epoch_idx + 1, color='green', linestyle='--', \n",
        "                    label=f'Best Epoch ({best_f1_epoch_idx + 1})', alpha=0.7)\n",
        "    axes[1].legend(fontsize=11)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Compute overfitting metrics\n",
        "    final_train_loss = training_history['train_loss'][-1]\n",
        "    final_val_loss = training_history['val_loss'][-1]\n",
        "    final_train_f1 = training_history['train_f1'][-1]\n",
        "    final_val_f1 = training_history['val_f1'][-1]\n",
        "    \n",
        "    best_val_f1 = max(training_history['val_f1'])\n",
        "    best_train_f1_at_best_val = training_history['train_f1'][np.argmax(training_history['val_f1'])]\n",
        "    \n",
        "    print(f\"\\n📊 Overfitting Analysis:\")\n",
        "    print(f\"  Final Epoch:\")\n",
        "    print(f\"    Train Loss: {final_train_loss:.4f}  |  Val Loss: {final_val_loss:.4f}\")\n",
        "    print(f\"    Train F1:   {final_train_f1:.4f}  |  Val F1:   {final_val_f1:.4f}\")\n",
        "    print(f\"    Loss Gap:   {(final_val_loss - final_train_loss):.4f} ({((final_val_loss/final_train_loss - 1)*100):+.1f}%)\")\n",
        "    print(f\"    F1 Gap:     {(final_train_f1 - final_val_f1):.4f} ({((final_train_f1 - final_val_f1)/final_val_f1*100):+.1f}%)\")\n",
        "    \n",
        "    print(f\"\\n  At Best Val F1 Epoch:\")\n",
        "    print(f\"    Train F1:   {best_train_f1_at_best_val:.4f}\")\n",
        "    print(f\"    Val F1:     {best_val_f1:.4f}\")\n",
        "    print(f\"    F1 Gap:     {(best_train_f1_at_best_val - best_val_f1):.4f}\")\n",
        "    \n",
        "    # Overfitting diagnosis\n",
        "    f1_gap_pct = ((best_train_f1_at_best_val - best_val_f1) / best_val_f1) * 100\n",
        "    \n",
        "    print(f\"\\n🔍 Overfitting Diagnosis:\")\n",
        "    if f1_gap_pct < 3:\n",
        "        print(f\"  ✅ MINIMAL OVERFITTING (gap: {f1_gap_pct:.1f}%)\")\n",
        "        print(f\"     Model generalizes well. Test performance should be close to val performance.\")\n",
        "    elif f1_gap_pct < 8:\n",
        "        print(f\"  ⚠️  MODERATE OVERFITTING (gap: {f1_gap_pct:.1f}%)\")\n",
        "        print(f\"     Some overfitting present. Expect slight drop on test set.\")\n",
        "    else:\n",
        "        print(f\"  🚨 SIGNIFICANT OVERFITTING (gap: {f1_gap_pct:.1f}%)\")\n",
        "        print(f\"     High risk! Consider: more regularization, less model complexity, more data.\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"⚠️  No training_history available. Run final model training first.\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = shutil.make_archive('/content/models', 'zip', '/content', 'models')\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "30ZrMlvpQQSc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
