{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v-in9ArBRwrp"
      },
      "outputs": [],
      "source": [
        "# OPTIMAL HYPERPARAMETERS - Based on empirical results\n",
        "\n",
        "PATIENCE = 40\n",
        "VERBOSE = 10\n",
        "\n",
        "RNN_TYPE = 'GRU'\n",
        "BIDIRECTIONAL = True\n",
        "L1_LAMBDA = 0\n",
        "\n",
        "\n",
        "# Reverse mapping from integers to pain level names\n",
        "label_reverse_mapping = {\n",
        "    0: 'no_pain',\n",
        "    1: 'low_pain',\n",
        "    2: 'high_pain'\n",
        "}\n",
        "\n",
        "# Create mapping dictionary for pain levels\n",
        "pain_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2\n",
        "}\n",
        "\n",
        "labels = ['no_pain', 'low_pain', 'high_pain']\n",
        "\n",
        "num_classes = len(labels)\n",
        "\n",
        "# # Define parameters to search\n",
        "# param_grid = {\n",
        "#     'window_size': [50, 100, 160],\n",
        "#     'stride': [25],\n",
        "#     'n_val_users' : [45],\n",
        "#     'hidden_size': [64, 128],\n",
        "#     'hidden_layers': [1, 2],\n",
        "#     'batch_size': [64, 256],\n",
        "#     'learning_rate' : [1e-3, 3e-4],\n",
        "#     'dropout_rate': [0.0, 0.3, 0.5],\n",
        "#     'l2_lambda': [0, 1e-4, 1e-3],\n",
        "#     'k' : [5],\n",
        "#     'epochs': [400]\n",
        "# }\n",
        "\n",
        "\n",
        "# # Define parameters to search\n",
        "# param_grid = {\n",
        "#     'window_size': [50, 160],\n",
        "#     'stride': [25],\n",
        "#     'n_val_users' : [45],\n",
        "#     'hidden_size': [64, 128],\n",
        "#     'hidden_layers': [1, 2],\n",
        "#     'batch_size': [64, 256],\n",
        "#     'learning_rate' : [1e-3, 3e-4],\n",
        "#     'dropout_rate': [0.0, 0.3],\n",
        "#     'l2_lambda': [0, 1e-4],\n",
        "#     'k' : [5],\n",
        "#     'epochs': [400]\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "# param_grid = {\n",
        "\n",
        "#     'window_size':   [100, 160],      # 2\n",
        "#     'stride':        [25],\n",
        "#     'n_val_users':   [45],\n",
        "\n",
        "#     'hidden_size':   [64, 128],       # 2\n",
        "#     'hidden_layers': [2],\n",
        "\n",
        "#     'batch_size':    [128],\n",
        "#     'learning_rate': [1e-3, 8e-4, 6e-4, 4e-4, 3e-4],  # 5 (log-ish sweep)\n",
        "#     'dropout_rate':  [0.3],\n",
        "#     'l2_lambda':     [1e-4],\n",
        "\n",
        "#     'k':             [5],             # single subject-level split for speed\n",
        "#     'epochs':        [200]            # rely on early stopping\n",
        "# }\n",
        "\n",
        "\n",
        "#test\n",
        "param_grid = {\n",
        "\n",
        "    'window_size':   [100],      # 2\n",
        "    'stride':        [25],\n",
        "    'n_val_users':   [45],\n",
        "\n",
        "    'hidden_size':   [64],       # 2\n",
        "    'hidden_layers': [2],\n",
        "\n",
        "    'batch_size':    [128],\n",
        "    'learning_rate': [1e-3],  # 5 (log-ish sweep)\n",
        "    'dropout_rate':  [0.3],\n",
        "    'l2_lambda':     [1e-4],\n",
        "\n",
        "    'k':             [2],             # single subject-level split for speed\n",
        "    'epochs':        [2]            # rely on early stopping\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-95VNzsCBdWV",
        "outputId": "97f86cf0-b3f2-4404-ee15-aa020b95c073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG TORCH:\n",
            "  torch.__version__     = 2.9.0+cu128\n",
            "  torch.version.cuda    = 12.8\n",
            "  torch.cuda.is_available() = False\n",
            "  torch.cuda.device_count()  = 0\n",
            "  selected device = cpu\n",
            "PyTorch version: 2.9.0+cu128\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os, random, numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def physical_cores():\n",
        "    try:\n",
        "        import psutil\n",
        "        n = psutil.cpu_count(logical=False)\n",
        "        if n:\n",
        "            return n\n",
        "    except Exception:\n",
        "        pass\n",
        "    n = os.cpu_count() or 2\n",
        "    return max(1, n // 2)  # stima fisici se non disponibile\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "CONVERTIBLE = False\n",
        "\n",
        "\n",
        "# Env PRIMA di import torch\n",
        "CORES = physical_cores()\n",
        "OMP = max(1, CORES - 1)\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", str(OMP))\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", str(OMP))\n",
        "os.environ.setdefault(\"MKL_DYNAMIC\", \"FALSE\")\n",
        "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", str(OMP))\n",
        "os.environ.setdefault(\"TORCH_NUM_INTEROP_THREADS\", \"1\")\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "torch.set_num_threads(int(os.environ[\"OMP_NUM_THREADS\"]))\n",
        "try:\n",
        "    torch.set_num_interop_threads(int(os.environ[\"TORCH_NUM_INTEROP_THREADS\"]))\n",
        "except RuntimeError as e:\n",
        "    print(\"skip set_num_interop_threads:\", e)\n",
        "\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import os, subprocess, shlex\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "# Device selection: prefer CUDA when available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "print(\"DEBUG TORCH:\")\n",
        "print(\"  torch.__version__     =\", torch.__version__)\n",
        "print(\"  torch.version.cuda    =\", torch.version.cuda)\n",
        "print(\"  torch.cuda.is_available() =\", torch.cuda.is_available())\n",
        "print(\"  torch.cuda.device_count()  =\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        print(\"  GPU name:\", torch.cuda.get_device_name(0))\n",
        "    except Exception as e:\n",
        "        print(\"  get_device_name error:\", e)\n",
        "print(\"  selected device =\", device)\n",
        "\n",
        "\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if not CONVERTIBLE:\n",
        "\n",
        "    # Configure plot display settings\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.set_style('white')\n",
        "    plt.rc('font', size=14)\n",
        "    %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsV2HL_gD9cl",
        "outputId": "d0164d29-8c35-4e2b-832b-da438ed3c80e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows of X_train with encoded labels:\n",
            "   sample_index  label\n",
            "0             0      0\n",
            "1             0      0\n",
            "2             0      0\n",
            "3             0      0\n",
            "4             0      0\n",
            "5             0      0\n",
            "6             0      0\n",
            "7             0      0\n",
            "8             0      0\n",
            "9             0      0\n",
            "\n",
            "Label value counts:\n",
            "label\n",
            "0    81760\n",
            "1    15040\n",
            "2     8960\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Check for NaN labels:\n",
            "NaN count: 0\n"
          ]
        }
      ],
      "source": [
        "X_train = pd.read_csv('pirate_pain_train.csv')\n",
        "y_train = pd.read_csv('pirate_pain_train_labels.csv')\n",
        "\n",
        "\n",
        "# First map the labels in y_train\n",
        "y_train['label_encoded'] = y_train['label'].map(pain_mapping)\n",
        "\n",
        "# Then merge with X_train based on sample_index\n",
        "X_train = X_train.merge(\n",
        "    y_train[['sample_index', 'label_encoded']],\n",
        "    on='sample_index',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Rename the column\n",
        "X_train.rename(columns={'label_encoded': 'label'}, inplace=True)\n",
        "\n",
        "# Verify the mapping worked correctly\n",
        "print(\"\\nFirst few rows of X_train with encoded labels:\")\n",
        "print(X_train[['sample_index', 'label']].head(10))\n",
        "\n",
        "print(\"\\nLabel value counts:\")\n",
        "print(X_train['label'].value_counts())\n",
        "\n",
        "print(\"\\nCheck for NaN labels:\")\n",
        "print(f\"NaN count: {X_train['label'].isna().sum()}\")\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acRv6ssvQQSM",
        "outputId": "b675e6a7-3832-4df2-c020-ba88a4244ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping joint_30 column (all NaN values)...\n",
            "Dropped joint_30 from (105760, 40)\n",
            "\n",
            "Columns after dropping joint_30:\n",
            "X_train columns: 40\n"
          ]
        }
      ],
      "source": [
        "# Drop joint_30 column (contains only NaN values)\n",
        "print(\"Dropping joint_30 column (all NaN values)...\")\n",
        "for df in [X_train]:\n",
        "    if 'joint_30' in df.columns:\n",
        "        df.drop('joint_30', axis=1, inplace=True)\n",
        "        print(f\"Dropped joint_30 from {df.shape}\")\n",
        "\n",
        "print(\"\\nColumns after dropping joint_30:\")\n",
        "print(f\"X_train columns: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2dx6K_ZQQSN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X-Pan_2kQQSO"
      },
      "outputs": [],
      "source": [
        "# First: Convert categorical variables to binary (two -> 1, others -> 0)\n",
        "binary_cols = ['n_hands', 'n_eyes', 'n_legs']\n",
        "for col in binary_cols:\n",
        "    for df_ in [X_train]:\n",
        "        df_[col] = df_[col].map(lambda x: 1 if str(x).lower().strip() == 'two' else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9NAklF6FP2G",
        "outputId": "17b3ec14-e3ad-4887-e434-64b1a965c1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data structure ---\n",
            "\n",
            "X_train Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 105760 entries, 0 to 105759\n",
            "Data columns (total 40 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   sample_index   105760 non-null  int64  \n",
            " 1   time           105760 non-null  int64  \n",
            " 2   pain_survey_1  105760 non-null  int64  \n",
            " 3   pain_survey_2  105760 non-null  int64  \n",
            " 4   pain_survey_3  105760 non-null  int64  \n",
            " 5   pain_survey_4  105760 non-null  int64  \n",
            " 6   n_legs         105760 non-null  int64  \n",
            " 7   n_hands        105760 non-null  int64  \n",
            " 8   n_eyes         105760 non-null  int64  \n",
            " 9   joint_00       105760 non-null  float64\n",
            " 10  joint_01       105760 non-null  float64\n",
            " 11  joint_02       105760 non-null  float64\n",
            " 12  joint_03       105760 non-null  float64\n",
            " 13  joint_04       105760 non-null  float64\n",
            " 14  joint_05       105760 non-null  float64\n",
            " 15  joint_06       105760 non-null  float64\n",
            " 16  joint_07       105760 non-null  float64\n",
            " 17  joint_08       105760 non-null  float64\n",
            " 18  joint_09       105760 non-null  float64\n",
            " 19  joint_10       105760 non-null  float64\n",
            " 20  joint_11       105760 non-null  float64\n",
            " 21  joint_12       105760 non-null  float64\n",
            " 22  joint_13       105760 non-null  float64\n",
            " 23  joint_14       105760 non-null  float64\n",
            " 24  joint_15       105760 non-null  float64\n",
            " 25  joint_16       105760 non-null  float64\n",
            " 26  joint_17       105760 non-null  float64\n",
            " 27  joint_18       105760 non-null  float64\n",
            " 28  joint_19       105760 non-null  float64\n",
            " 29  joint_20       105760 non-null  float64\n",
            " 30  joint_21       105760 non-null  float64\n",
            " 31  joint_22       105760 non-null  float64\n",
            " 32  joint_23       105760 non-null  float64\n",
            " 33  joint_24       105760 non-null  float64\n",
            " 34  joint_25       105760 non-null  float64\n",
            " 35  joint_26       105760 non-null  float64\n",
            " 36  joint_27       105760 non-null  float64\n",
            " 37  joint_28       105760 non-null  float64\n",
            " 38  joint_29       105760 non-null  float64\n",
            " 39  label          105760 non-null  int64  \n",
            "dtypes: float64(30), int64(10)\n",
            "memory usage: 32.3 MB\n",
            "\n",
            "Missing values in X_train: 0\n",
            "\n",
            "y_train Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 661 entries, 0 to 660\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   sample_index   661 non-null    int64 \n",
            " 1   label          661 non-null    object\n",
            " 2   label_encoded  661 non-null    int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 15.6+ KB\n",
            "\n",
            "Missing values in y_train: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Data structure ---\")\n",
        "print(\"\\nX_train Info:\")\n",
        "X_train.info(verbose=True)\n",
        "print(f\"\\nMissing values in X_train: {X_train.isnull().sum().sum()}\")\n",
        "print(\"\\ny_train Info:\")\n",
        "y_train.info(verbose=True)\n",
        "print(f\"\\nMissing values in y_train: {y_train.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "NBmYSucNGROn",
        "outputId": "1a3ec232-3dec-46d6-af15-190b94f91158"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_21</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>330.000000</td>\n",
              "      <td>79.500000</td>\n",
              "      <td>1.633746</td>\n",
              "      <td>1.654851</td>\n",
              "      <td>1.653640</td>\n",
              "      <td>1.663134</td>\n",
              "      <td>0.990923</td>\n",
              "      <td>0.990923</td>\n",
              "      <td>0.990923</td>\n",
              "      <td>0.943095</td>\n",
              "      <td>...</td>\n",
              "      <td>3.972126e-05</td>\n",
              "      <td>4.176794e-05</td>\n",
              "      <td>3.561780e-05</td>\n",
              "      <td>3.138109e-05</td>\n",
              "      <td>1.024604e-04</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.058244</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.062273</td>\n",
              "      <td>0.311649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>190.814948</td>\n",
              "      <td>46.187338</td>\n",
              "      <td>0.682423</td>\n",
              "      <td>0.669639</td>\n",
              "      <td>0.666649</td>\n",
              "      <td>0.661994</td>\n",
              "      <td>0.094841</td>\n",
              "      <td>0.094841</td>\n",
              "      <td>0.094841</td>\n",
              "      <td>0.202051</td>\n",
              "      <td>...</td>\n",
              "      <td>4.974496e-03</td>\n",
              "      <td>5.472244e-03</td>\n",
              "      <td>1.235450e-03</td>\n",
              "      <td>4.062914e-04</td>\n",
              "      <td>3.206128e-03</td>\n",
              "      <td>0.060293</td>\n",
              "      <td>0.079819</td>\n",
              "      <td>0.060773</td>\n",
              "      <td>0.072597</td>\n",
              "      <td>0.619651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.510494e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.063144e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>165.000000</td>\n",
              "      <td>39.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.828277</td>\n",
              "      <td>...</td>\n",
              "      <td>6.545878e-08</td>\n",
              "      <td>3.321650e-07</td>\n",
              "      <td>3.275038e-07</td>\n",
              "      <td>2.841805e-07</td>\n",
              "      <td>7.161332e-07</td>\n",
              "      <td>0.009885</td>\n",
              "      <td>0.012652</td>\n",
              "      <td>0.016290</td>\n",
              "      <td>0.019638</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>330.000000</td>\n",
              "      <td>79.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.005126</td>\n",
              "      <td>...</td>\n",
              "      <td>8.302747e-07</td>\n",
              "      <td>1.095971e-06</td>\n",
              "      <td>1.024209e-06</td>\n",
              "      <td>8.746147e-07</td>\n",
              "      <td>3.126723e-06</td>\n",
              "      <td>0.021898</td>\n",
              "      <td>0.031739</td>\n",
              "      <td>0.031843</td>\n",
              "      <td>0.039041</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>495.000000</td>\n",
              "      <td>119.250000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.081039</td>\n",
              "      <td>...</td>\n",
              "      <td>2.800090e-06</td>\n",
              "      <td>3.079465e-06</td>\n",
              "      <td>3.021830e-06</td>\n",
              "      <td>2.507548e-06</td>\n",
              "      <td>9.946107e-06</td>\n",
              "      <td>0.048579</td>\n",
              "      <td>0.071051</td>\n",
              "      <td>0.058741</td>\n",
              "      <td>0.079518</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>660.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.407968</td>\n",
              "      <td>...</td>\n",
              "      <td>1.442198e+00</td>\n",
              "      <td>1.305001e+00</td>\n",
              "      <td>2.742411e-01</td>\n",
              "      <td>3.643074e-02</td>\n",
              "      <td>9.473540e-01</td>\n",
              "      <td>1.223617</td>\n",
              "      <td>1.187419</td>\n",
              "      <td>1.412037</td>\n",
              "      <td>1.370765</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sample_index           time  pain_survey_1  pain_survey_2  \\\n",
              "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
              "mean      330.000000      79.500000       1.633746       1.654851   \n",
              "std       190.814948      46.187338       0.682423       0.669639   \n",
              "min         0.000000       0.000000       0.000000       0.000000   \n",
              "25%       165.000000      39.750000       2.000000       2.000000   \n",
              "50%       330.000000      79.500000       2.000000       2.000000   \n",
              "75%       495.000000     119.250000       2.000000       2.000000   \n",
              "max       660.000000     159.000000       2.000000       2.000000   \n",
              "\n",
              "       pain_survey_3  pain_survey_4         n_legs        n_hands  \\\n",
              "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
              "mean        1.653640       1.663134       0.990923       0.990923   \n",
              "std         0.666649       0.661994       0.094841       0.094841   \n",
              "min         0.000000       0.000000       0.000000       0.000000   \n",
              "25%         2.000000       2.000000       1.000000       1.000000   \n",
              "50%         2.000000       2.000000       1.000000       1.000000   \n",
              "75%         2.000000       2.000000       1.000000       1.000000   \n",
              "max         2.000000       2.000000       1.000000       1.000000   \n",
              "\n",
              "              n_eyes       joint_00  ...      joint_21      joint_22  \\\n",
              "count  105760.000000  105760.000000  ...  1.057600e+05  1.057600e+05   \n",
              "mean        0.990923       0.943095  ...  3.972126e-05  4.176794e-05   \n",
              "std         0.094841       0.202051  ...  4.974496e-03  5.472244e-03   \n",
              "min         0.000000       0.000000  ...  0.000000e+00  1.510494e-07   \n",
              "25%         1.000000       0.828277  ...  6.545878e-08  3.321650e-07   \n",
              "50%         1.000000       1.005126  ...  8.302747e-07  1.095971e-06   \n",
              "75%         1.000000       1.081039  ...  2.800090e-06  3.079465e-06   \n",
              "max         1.000000       1.407968  ...  1.442198e+00  1.305001e+00   \n",
              "\n",
              "           joint_23      joint_24      joint_25       joint_26       joint_27  \\\n",
              "count  1.057600e+05  1.057600e+05  1.057600e+05  105760.000000  105760.000000   \n",
              "mean   3.561780e-05  3.138109e-05  1.024604e-04       0.041905       0.058244   \n",
              "std    1.235450e-03  4.062914e-04  3.206128e-03       0.060293       0.079819   \n",
              "min    0.000000e+00  1.063144e-08  0.000000e+00       0.000203       0.000000   \n",
              "25%    3.275038e-07  2.841805e-07  7.161332e-07       0.009885       0.012652   \n",
              "50%    1.024209e-06  8.746147e-07  3.126723e-06       0.021898       0.031739   \n",
              "75%    3.021830e-06  2.507548e-06  9.946107e-06       0.048579       0.071051   \n",
              "max    2.742411e-01  3.643074e-02  9.473540e-01       1.223617       1.187419   \n",
              "\n",
              "            joint_28       joint_29          label  \n",
              "count  105760.000000  105760.000000  105760.000000  \n",
              "mean        0.049886       0.062273       0.311649  \n",
              "std         0.060773       0.072597       0.619651  \n",
              "min         0.000000       0.000000       0.000000  \n",
              "25%         0.016290       0.019638       0.000000  \n",
              "50%         0.031843       0.039041       0.000000  \n",
              "75%         0.058741       0.079518       0.000000  \n",
              "max         1.412037       1.370765       2.000000  \n",
              "\n",
              "[8 rows x 40 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W3yOX19MQQSR"
      },
      "outputs": [],
      "source": [
        "def build_sequences(df, window=200, stride=50):\n",
        "    \"\"\"\n",
        "    Build sequences from time-series data\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with the data\n",
        "        window: Window size for sequences\n",
        "        stride: Stride for overlapping windows\n",
        "\n",
        "    Returns:\n",
        "        dataset: numpy array of sequences\n",
        "        labels: numpy array of labels\n",
        "    \"\"\"\n",
        "    # Initialise lists to store sequences and their corresponding labels\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over unique IDs in the DataFrame\n",
        "    for sample_id in df['sample_index'].unique():\n",
        "        # Extract sensor data for the current ID\n",
        "        drop_cols = [c for c in ['sample_index', 'time', 'label', 'labels'] if c in df.columns]\n",
        "        temp = df[df['sample_index'] == sample_id].drop(columns=drop_cols).values.astype('float32')\n",
        "\n",
        "        # Retrieve the activity label for the current ID\n",
        "        label_series = df[df['sample_index'] == sample_id]['label']\n",
        "\n",
        "        # Check if label column exists and has values\n",
        "        if label_series.empty:\n",
        "            print(f\"Warning: No label found for sample_id {sample_id}\")\n",
        "            continue\n",
        "\n",
        "        label_value = label_series.values[0]\n",
        "\n",
        "        # Skip samples with NaN labels\n",
        "        if pd.isna(label_value):\n",
        "            print(f\"Warning: NaN label for sample_id {sample_id}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Convert to int and validate\n",
        "        try:\n",
        "            label = int(label_value)\n",
        "            if label < 0 or label > 2:  # Assuming 3 classes: 0, 1, 2\n",
        "                print(f\"Warning: Invalid label {label} for sample_id {sample_id}, skipping...\")\n",
        "                continue\n",
        "        except (ValueError, TypeError) as e:\n",
        "            print(f\"Warning: Cannot convert label {label_value} to int for sample_id {sample_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        # --- NEW: niente padding a multipli di window ---\n",
        "        L = len(temp)\n",
        "\n",
        "        # PADDING SOLO se L < window (così hai almeno 1 finestra)\n",
        "        if L < window:\n",
        "            pad = window - L\n",
        "            temp = np.concatenate([temp, np.zeros((pad, temp.shape[1]), dtype='float32')], axis=0)\n",
        "            L = len(temp)  # ora L == window\n",
        "\n",
        "        # Genera gli start con lo stride, SENZA espandere a multipli di window\n",
        "        starts = list(range(0, max(L - window, 0) + 1, stride))\n",
        "\n",
        "        # (opzionale ma consigliato) ancora l’ultima finestra alla fine se non allineata\n",
        "        last_start = L - window\n",
        "        if last_start >= 0 and (len(starts) == 0 or starts[-1] != last_start):\n",
        "            starts.append(last_start)\n",
        "\n",
        "        for s in starts:\n",
        "            dataset.append(temp[s:s+window])\n",
        "            labels.append(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Convert lists to numpy arrays for further processing\n",
        "    dataset = np.array(dataset, dtype='float32')\n",
        "    labels = np.array(labels, dtype='int64')\n",
        "\n",
        "    print(f\"Built {len(dataset)} sequences with {len(labels)} labels\")\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S7SdOdxjQQSS"
      },
      "outputs": [],
      "source": [
        "#BATCH_SIZE = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G0ypRlR4QQST"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches ahead\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GpR5aspoQQST"
      },
      "outputs": [],
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKJ48pWqQQST",
        "outputId": "559895c2-fdab-4852-b04d-8b5ac0f2e3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "rnn (RNN)                 [[-1, 128], [2, -1]]         54,912         \n",
            "classifier (Linear)       [-1]                         387            \n",
            "===============================================================================\n",
            "Total params: 55,299\n",
            "Trainable params: 55,299\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic RNN classifier (RNN, LSTM, GRU).\n",
        "    Uses the last hidden state for classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Map string name to PyTorch RNN class\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout is only applied between layers (if num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "\n",
        "        # Create the recurrent layer\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calculate input size for the final classifier\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size = hidden_size\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        rnn_out, hidden = self.rnn(x)\n",
        "\n",
        "        # LSTM returns (h_n, c_n), we only need h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "\n",
        "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
        "            # Final shape: (batch_size, hidden_size * 2)\n",
        "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
        "        else:\n",
        "            # Take the last layer's hidden state\n",
        "            # Final shape: (batch_size, hidden_size)\n",
        "            hidden_to_classify = hidden[-1]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=0.,\n",
        "    rnn_type='RNN'\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o1HTpkmJQQSU"
      },
      "outputs": [],
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5S4P5W1AQQSU"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZnCsqUqtQQSU"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FLYRdcDrQQSU"
      },
      "outputs": [],
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9ITUJXpAQQSV"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynGd49opQQSV"
      },
      "source": [
        "# KFOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8T-RUCTFQQSV"
      },
      "outputs": [],
      "source": [
        "def k_shuffle_split_cross_validation_round_rnn(df, epochs, criterion, device,\n",
        "                            k, n_val_users, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
        "                            window_size, stride, rnn_type, bidirectional,\n",
        "                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Perform K-fold shuffle split cross-validation with sample-based splitting for Pirate Pain time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['sample_index', 'time', 'label', 'pain_survey_*', 'joint_*', 'n_legs', 'n_hands', 'n_eyes']\n",
        "        epochs: Number of training epochs\n",
        "        criterion: Loss function\n",
        "        device: torch.device for computation\n",
        "        k: Number of cross-validation splits\n",
        "        n_val_users: Number of samples for validation set\n",
        "        n_test_users: Number of samples for test set\n",
        "        batch_size: Batch size for training\n",
        "        hidden_layers: Number of recurrent layers\n",
        "        hidden_size: Hidden state dimensionality\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        dropout_rate: Dropout rate\n",
        "        window_size: Length of sliding windows\n",
        "        stride: Step size for sliding windows\n",
        "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "        bidirectional: Whether to use bidirectional RNN\n",
        "        l1_lambda: L1 regularization coefficient (if used)\n",
        "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
        "        patience: Early stopping patience\n",
        "        evaluation_metric: Metric to monitor for early stopping\n",
        "        mode: 'max' or 'min' for evaluation metric\n",
        "        restore_best_weights: Whether to restore best weights after training\n",
        "        writer: TensorBoard writer\n",
        "        verbose: Verbosity level\n",
        "        seed: Random seed\n",
        "        experiment_name: Name for experiment logging\n",
        "\n",
        "    Returns:\n",
        "        fold_losses: Dict with validation losses for each split\n",
        "        fold_metrics: Dict with validation F1 scores for each split\n",
        "        best_scores: Dict with best F1 score for each split plus mean and std\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise containers for results across all splits\n",
        "    fold_losses = {}\n",
        "    fold_metrics = {}\n",
        "    best_scores = {}\n",
        "\n",
        "\n",
        "    # Define columns to normalize\n",
        "    pain_survey_columns = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes']\n",
        "    joint_columns = [f'joint_{i:02d}' for i in range(30)]  # joint_00 through joint_29\n",
        "    scale_columns = pain_survey_columns + joint_columns\n",
        "\n",
        "    # Get model architecture parameters\n",
        "    # Count features (excluding sample_index, time, label)\n",
        "    feature_cols = scale_columns  # All features that will be used\n",
        "    in_features = len(feature_cols)\n",
        "    num_classes = 3  # no_pain, low_pain, high_pain\n",
        "\n",
        "    # Initialise model architecture\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=in_features,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=hidden_layers,\n",
        "        num_classes=num_classes,\n",
        "        dropout_rate=dropout_rate,\n",
        "        bidirectional=bidirectional,\n",
        "        rnn_type=rnn_type\n",
        "    ).to(device)\n",
        "\n",
        "    # Store initial weights to reset model for each split\n",
        "    initial_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Iterate through K random splits\n",
        "    for split_idx in range(k):\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"Split {split_idx+1}/{k}\")\n",
        "\n",
        "        # Get unique sample IDs and shuffle them with split-specific seed\n",
        "        unique_samples = df['sample_index'].unique()\n",
        "        random.seed(seed + split_idx)\n",
        "        random.shuffle(unique_samples)\n",
        "\n",
        "        # Calculate the number of samples for the training set\n",
        "        n_train_samples = len(unique_samples) - n_val_users\n",
        "\n",
        "        # Split the shuffled sample IDs into training, validation, and test sets\n",
        "        train_samples = unique_samples[:n_train_samples]\n",
        "        val_samples = unique_samples[n_train_samples:n_train_samples + n_val_users]\n",
        "\n",
        "        # Split the dataset into training, validation, and test sets based on sample IDs\n",
        "        df_train = df[df['sample_index'].isin(train_samples)].copy()\n",
        "        df_val = df[df['sample_index'].isin(val_samples)].copy()\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training set shape: {df_train.shape}\")\n",
        "            print(f\"  Validation set shape: {df_val.shape}\")\n",
        "\n",
        "        # Map pain labels to integers (if not already mapped)\n",
        "        if df_train['label'].dtype == 'object':\n",
        "            df_train['label'] = df_train['label'].map(pain_mapping)\n",
        "            df_val['label'] = df_val['label'].map(pain_mapping)\n",
        "\n",
        "        # Normalise features using training set statistics\n",
        "        train_max = df_train[scale_columns].max()\n",
        "        train_min = df_train[scale_columns].min()\n",
        "\n",
        "        for column in scale_columns:\n",
        "            df_train[column] = (df_train[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "            df_val[column] = (df_val[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "\n",
        "        # Build sequences using the existing build_sequences function\n",
        "        X_train, y_train = build_sequences(df_train, window=window_size, stride=stride)\n",
        "        X_val, y_val = build_sequences(df_val, window=window_size, stride=stride)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training sequences shape: {X_train.shape}\")\n",
        "            print(f\"  Validation sequences shape: {X_val.shape}\")\n",
        "            print(f\"  Test sequences shape: {X_test.shape}\")\n",
        "\n",
        "        # Create PyTorch datasets\n",
        "        train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "        val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "        # Reset model to initial weights for fair comparison across splits\n",
        "        model.load_state_dict(initial_state)\n",
        "\n",
        "        # Define optimizer with L2 regularization\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "        # Enable mixed precision training for GPU acceleration\n",
        "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # Create directory for model checkpoints\n",
        "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
        "\n",
        "        # Train model on current split\n",
        "        model, training_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=epochs,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scaler=split_scaler,\n",
        "            device=device,\n",
        "            writer=writer,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            l1_lambda=l1_lambda,\n",
        "            evaluation_metric=evaluation_metric,\n",
        "            mode=mode,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n",
        "        )\n",
        "\n",
        "        # Store results for this split\n",
        "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
        "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
        "        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n",
        "\n",
        "    # Compute mean and standard deviation of best scores across splits\n",
        "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Best score: {best_scores['mean']:.4f}±{best_scores['std']:.4f}\")\n",
        "\n",
        "    return fold_losses, fold_metrics, best_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4yt6vaquQQSV"
      },
      "outputs": [],
      "source": [
        "def grid_search_cv_rnn(df, param_grid, fixed_params, cv_params, verbose=True):\n",
        "    \"\"\"\n",
        "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
        "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
        "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
        "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
        "        verbose: Print progress for each configuration\n",
        "\n",
        "    Returns:\n",
        "        results: Dict with scores for each configuration\n",
        "        best_config: Dict with best hyperparameter combination\n",
        "        best_score: Best mean F1 score achieved\n",
        "    \"\"\"\n",
        "    # Generate all parameter combinations\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    combinations = list(product(*param_values))\n",
        "\n",
        "    results = {}\n",
        "    best_score = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    total = len(combinations)\n",
        "\n",
        "    for idx, combo in enumerate(combinations, 1):\n",
        "        # Create current configuration dict\n",
        "        current_config = dict(zip(param_names, combo))\n",
        "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nConfiguration {idx}/{total}:\")\n",
        "            for param, value in current_config.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "\n",
        "        # Merge current config with fixed parameters\n",
        "        run_params = {**fixed_params, **current_config}\n",
        "\n",
        "        # Execute cross-validation\n",
        "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "            df=df,\n",
        "            experiment_name=config_str,\n",
        "            **run_params,\n",
        "            **cv_params\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results[config_str] = fold_scores\n",
        "\n",
        "        # Track best configuration\n",
        "        if fold_scores[\"mean\"] > best_score:\n",
        "            best_score = fold_scores[\"mean\"]\n",
        "            best_config = current_config.copy()\n",
        "            if verbose:\n",
        "                print(\"  NEW BEST SCORE!\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  F1 Score: {fold_scores['mean']:.4f}±{fold_scores['std']:.4f}\")\n",
        "\n",
        "    return results, best_config, best_score\n",
        "\n",
        "\n",
        "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
        "\n",
        "    Args:\n",
        "        results: Dict of results from grid_search_cv_rnn\n",
        "        k_splits: Number of CV splits used\n",
        "        top_n: Number of top configurations to display\n",
        "        figsize: Figure size tuple\n",
        "    \"\"\"\n",
        "    # Sort by mean score\n",
        "    config_scores = {name: data['mean'] for name, data in results.items()}\n",
        "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top N\n",
        "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
        "\n",
        "    # Prepare boxplot data\n",
        "    boxplot_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
        "    replacements = {\n",
        "        'batch_size_': 'BS=',\n",
        "        'learning_rate_': '\\nLR=',\n",
        "        'hidden_layers_': '\\nHL=',\n",
        "        'hidden_size_': '\\nHS=',\n",
        "        'dropout_rate_': '\\nDR=',\n",
        "        'window_size_': '\\nWS=',\n",
        "        'stride_': '\\nSTR=',\n",
        "        'rnn_type_': '\\nRNN=',\n",
        "        'bidirectional_': '\\nBIDIR=',\n",
        "        'l1_lambda_': '\\nL1=',\n",
        "        'l2_lambda_': '\\nL2='\n",
        "    }\n",
        "\n",
        "    # Replacements for separators\n",
        "    separator_replacements = {\n",
        "        '_learning_rate_': '\\nLR=',\n",
        "        '_hidden_layers_': '\\nHL=',\n",
        "        '_hidden_size_': '\\nHS=',\n",
        "        '_dropout_rate_': '\\nDR=',\n",
        "        '_window_size_': '\\nWS=',\n",
        "        '_stride_': '\\nSTR=',\n",
        "        '_rnn_type_': '\\nRNN=',\n",
        "        '_bidirectional_': '\\nBIDIR=',\n",
        "        '_l1_lambda_': '\\nL1=',\n",
        "        '_l2_lambda_': '\\nL2=',\n",
        "        '_': ''\n",
        "    }\n",
        "\n",
        "    for config_name, mean_score in top_configs:\n",
        "        # Extract best score from each split (auto-detect number of splits)\n",
        "        split_scores = []\n",
        "        for i in range(k_splits):\n",
        "            if f'split_{i}' in results[config_name]:\n",
        "                split_scores.append(results[config_name][f'split_{i}'])\n",
        "        boxplot_data.append(split_scores)\n",
        "\n",
        "        # Verify we have the expected number of splits\n",
        "        if len(split_scores) != k_splits:\n",
        "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
        "\n",
        "        # Create readable label using the replacements dictionary\n",
        "        readable_label = config_name\n",
        "        for old, new in replacements.items():\n",
        "            readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        # Apply separator replacements\n",
        "        for old, new in separator_replacements.items():\n",
        "             readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        labels.append(f\"{readable_label}\\n(μ={mean_score:.3f})\")\n",
        "\n",
        "\n",
        "    if (not CONVERTIBLE):\n",
        "        # Create plot\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
        "                        showmeans=True, meanline=True)\n",
        "\n",
        "        # Styling\n",
        "        for patch in bp['boxes']:\n",
        "            patch.set_facecolor('lightblue')\n",
        "            patch.set_alpha(0.7)\n",
        "\n",
        "        # Highlight best configuration\n",
        "        ax.get_xticklabels()[0].set_fontweight('bold')\n",
        "\n",
        "        ax.set_ylabel('F1 Score')\n",
        "        ax.set_xlabel('Configuration')\n",
        "        ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
        "        ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "        plt.xticks(rotation=0, ha='center')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lhgn3M-iQQSW"
      },
      "outputs": [],
      "source": [
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdF2fIryQQSW",
        "outputId": "7ef98fef-a588-4c3c-a11b-ce040603db17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Configuration 1/1:\n",
            "  window_size: 100\n",
            "  stride: 25\n",
            "  n_val_users: 45\n",
            "  hidden_size: 64\n",
            "  hidden_layers: 2\n",
            "  batch_size: 128\n",
            "  learning_rate: 0.001\n",
            "  dropout_rate: 0.3\n",
            "  l2_lambda: 0.0001\n",
            "  k: 2\n",
            "  epochs: 2\n",
            "Built 2464 sequences with 2464 labels\n",
            "Built 180 sequences with 180 labels\n",
            "Training 2 epochs...\n",
            "Best model restored from epoch 1 with val_f1 0.6806\n",
            "Built 2464 sequences with 2464 labels\n",
            "Built 180 sequences with 180 labels\n",
            "Training 2 epochs...\n",
            "Best model restored from epoch 1 with val_f1 0.5911\n",
            "  NEW BEST SCORE!\n",
            "  F1 Score: 0.6358±0.0448\n",
            "Grid search took 43.91s\n"
          ]
        }
      ],
      "source": [
        "from time import perf_counter\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def timing(label=\"Block\"):\n",
        "    t0 = perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        print(f\"{label} took {perf_counter() - t0:.2f}s\")\n",
        "\n",
        "with timing(\"Grid search\"):\n",
        "    fixed_params = {\n",
        "        'l1_lambda': L1_LAMBDA,\n",
        "        'rnn_type': RNN_TYPE,\n",
        "        'bidirectional': BIDIRECTIONAL,\n",
        "    }\n",
        "    cv_params = {\n",
        "        'criterion': criterion,\n",
        "        'device': device,\n",
        "        'patience': PATIENCE,\n",
        "        'verbose': 0,\n",
        "        'seed': SEED,\n",
        "        'evaluation_metric': 'val_f1',\n",
        "        'mode': 'max',\n",
        "        'restore_best_weights': True,\n",
        "        'writer': None,\n",
        "    }\n",
        "    results, best_config, best_score = grid_search_cv_rnn(\n",
        "        df=X_train,\n",
        "        param_grid=param_grid,\n",
        "        fixed_params=fixed_params,\n",
        "        cv_params=cv_params,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "705XWZjNQQSW",
        "outputId": "db2b8d06-5ff1-4528-bb4d-a5fdfb7defad"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABWEAAAKmCAYAAAAsI1zEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAA1K1JREFUeJzs3XlcVFXjx/Evu7IoKLgBJqaiAu5aZuaCqflYaaZli+aupaVZmW1mZaU9Wmm5ZKVpaeW+565lpYiYimvuILiziIAsM78/fGZ+jIDAyAjo5/16+ZK5595zz525cxm+c+45dkaj0SgAAAAAAAAAgE3YF3UDAAAAAAAAAOBORggLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAbOLYsWN6+eWX1bx5c9WpU0eBgYGaMmWKJOnNN9+0eAxpx44dCgwMVNu2bYu6KUCx8fzzzyswMFCLFy8u6qZYCAwMVGBgoKKjoy2WT5kyRYGBgXrzzTeLqGW5K85tAwriZudy27ZtFRgYqB07dhRBywDg5hyLugEAgNwFBgZatd0nn3yiJ554opBbY53o6Gj9/fffioyMVGRkpA4fPqz09HQ1a9ZMc+fOtbreN998U0uWLLFYZmdnJ1dXV/n7++uBBx5Q7969ValSpRy3X7x4sUaPHi1JKleunNavXy93d/cc1+3Xr5+2bdumoUOHatiwYRZlWV+jSZMm6T//+U+OdaxatUqvvvqqfH19tWnTpnwfZ1aZmZlavXq1Nm3apL179+ry5ctKT0+Xl5eXAgMD1apVK3Xu3FleXl5W1V+YLl++rGeeeUbx8fHy9PRUSEiIHBwcVLly5aJuWpHYsWOHwsLCVKdOHbVr166om1PiREdHKzQ0NM/1QkNDNXXqVPPjCxcu6K+//jJffw4ePKiUlJRbeh+apKena/Hixfrtt9906NAhXblyRaVLl1a5cuXk7++vxo0bq2XLlgoODr6l/ZRUOV2jnZyc5O7urnLlyql27dpq3LixOnfurLJly96WNpm+9Ondu7fKlClzW/Z5uyxevFhnzpxRu3btVKdOnaJuTqGLiorSww8/LKPRqHvuuUfr1q0r6iaVKOfOndOGDRu0Y8cOHThwQOfOnZO9vb0qVaqk5s2bq1evXqpevbrV9e/evVs///yzIiIidP78eRmNRpUrV04+Pj6qX7++mjRpotDQUDk5ORXiURXMwYMHtWHDBvn6+habz8gA7j6EsABQjDVq1CjH5REREZKkatWqqVy5ctnKy5cvb9N2FcQPP/ygOXPm2Kz+8uXL65577pEkGQwGxcbG6tChQzp06JB+/fVXfffdd2rQoMFN67h8+bJmzZqVLWAtqC+//FIdOnSQo2Ph/3o9cOCARowYoZMnT0qS3N3d5efnJycnJ50/f15//PGH/vjjD33xxRcaP358kQd9q1evVnx8vIKCgjR//ny5uLhYlPv4+CggIKBYBMa3Q1hYmL766it17do119emdOnSCggIUMWKFW9z60qW4OBgOTs751hWo0YNi8erVq3SJ598UuhtuHTpkvr3768DBw5Ikjw8PBQQECAnJyedPXvW/H7csWOHZs+eXej7L0myXqONRqOuXLmi2NhYHTt2TKtWrdL48eM1cOBADR48OMdrZ+XKlRUQECAPD49bbstXX30lSeratesth7ABAQGSVKShUlZLlixRWFiYfH19cw1hvby8FBAQIB8fn9vculu3aNEiGY1GSdKpU6e0c+dONW3atIhbVXL06NFDZ8+elSSVKVNGNWrUUHJysqKionTy5EktWrRI48aN02OPPVbguj///HPNmDFDRqNRTk5OqlSpkry8vJSQkKDIyEjt3btXc+fO1datW3P9Yrww+fv7y9nZWaVLl7ZYfvDgQX311Vdq1qwZISyAIkMICwDF2Pz583Ncbup9OWjQoGL/QdLLy0utWrVSUFCQgoODFR4eru+//77Q6n/ooYf06aefWizbu3evRowYoejoaI0aNUpr1qyRvX3OI/A4ODgoMzNTs2bN0rPPPptjqJ0fDg4OOnXqlBYsWKCePXtaVUdudu/erT59+iglJUXBwcF69dVXdd9991kEFsePH9eCBQs0b948HTx4sMhD2KNHj0qS7r///mwBrCSNHDlSI0eOvN3NKtbq1aun3377raibUex9+eWX8vPzy9e67u7uat68uYKDgxUcHKzY2Nhs1wtrvP322zpw4IC8vb31/vvvq23btnJwcDCXnz59WmvXrlVUVNQt76uky+kabTAYdODAAf34449asmSJpkyZosOHD+vLL7/Mdq2eMGHC7WxuvpXE9+pzzz2n5557rqibUWAGg0FLly6VJJUtW1YJCQlatGgRIWwBODk5qUePHurevbuCg4PN77OYmBiNHj1a27dv1+jRo1W3bt1sX2bdzLp16zR9+nRJUv/+/dWvXz+Lz1FJSUn6448/tGjRItnZ2RXuQeXihx9+uC37AQBrMCYsAMCmXnzxRX3zzTd65ZVXFBoaelt6PtarV0/vvfeeJOnkyZOKjIzMdd2qVauqYcOGunr1qsVtzAX1+OOPS5K+/vprpaSkWF3PjZKSkvTyyy8rJSVFDz30kObPn68WLVpk6zFWvXp1jRo1SkuWLNG9995baPu31rVr1yQpW08U4HZ68sknNXv2bL322mvq2LGjKlSocMt1XrhwQVu2bJEkvfPOO3r44YctAljp+nVlwIAB+uCDD255f3cie3t7BQcH69NPP9V///tf2dnZad26dYQnyNGff/6p2NhYlS5dWu+++64kae3atUpKSirilpUcv/76qz788EPVq1fP4ouOKlWqaMqUKfLy8lJGRoYWLFhQoHpN67dq1Uqvv/56ti+y3d3d9cgjj+jbb7/lLg8AECEsANyRfv/9dw0ePFgPPPCAgoOD1aJFC7344ov6+++/c1z/xgmBFi9erO7du6thw4Zq1KiRevXqpd9///12HsIta9iwofnnGydOuZGpR+bPP/+c57q56datm6pVq6YLFy4U6vAL8+bN0/nz5+Xh4aEJEybkehu2SfXq1dWpU6dsy2NjY/Xhhx+qQ4cOqlevnho3bqwnn3xS33//vTkwvVHWyS1iYmL01ltvmce4bNu2rT799NNsfwSbJtwyTaLz1VdfmSewyTp+bl4Tc0VGRmrIkCG67777VL9+fT366KOaPXu2DAZDrhP15GfSmdwm7Mi6bVpamqZPn65HH31UDRs2tGj32bNn9cMPP6hfv35q166d6tWrp0aNGumJJ57Q1KlTcwwFAgMDzbdBL1myxOL5yFp3XhNzpaena968eerZs6eaNm2qkJAQhYaG6t1339WpU6dy3CbrcWVmZmr27Nl69NFHVb9+fTVt2lSDBg3K9UsKg8GgBQsW6LnnnlOzZs0UFBSk++67T506dTL3nLobRUdHm2+LrlWrllV1GI1GbdiwQUOGDNGDDz6o4OBgPfDAA+rRo4e+/vprnT9/Pts2cXFxmjRpkjp37qwGDRqoYcOGevTRRzV58mRduXIlx/1kfa+cP3/e3Gs3ODhYzz//vMW6p06d0vvvv68OHTqofv36atiwobp166bZs2crLS3NquPMj0cffVTdunWTJM2cOTPbvm42Mdf+/fs1cuRItWnTRsHBwWrYsKHatm2rfv366fvvvze/Tqb3gUloaKjFezDrdSjrNeL48eMaNWqUHnroIQUFBVlcW3KbmCura9eumYepqVevnpo3b65XX31Vx48fz3H9vCYhi46OzvW6ERYWJkkaPXq0xbFlfZ3zukbe6jkWHx+vcePGmc+xli1b6p133tGFCxdyfY7yY9GiRZKkdu3a6ZFHHpG3t7eSk5O1Zs2am25X0PdZ1td07969evnll9WiRQvVqVPH4hyx5losXQ+O+/Xrp+bNmysoKEhNmzZVhw4d9Oqrr2r9+vXZ1s/v+Z0fN7vLp0yZMubPTMeOHct3ndL1Xv+SddfCrOdjQd8rN5PT7/m2bdua5wEICwvL9ns46/u4MJ93ALgRwxEAwB1m3Lhx5hCwfPnyql27tqKjo7Vx40Zt3LhRQ4YM0fDhw3Pd/tNPP9WsWbPk7e2t6tWrKyoqSjt27NCOHTv05ptvqk+fPrfpSG5N1t6oefXGbNq0qVq1aqWtW7dq8uTJVt3+6ujoqOHDh2v48OH69ttv9fTTTxfKZDPLli2TJD322GNW9yIOCwvTkCFDlJSUJCcnJ9WsWVMpKSnat2+f9u3bpxUrVui7777L9Y+0w4cPa+jQoUpNTVXNmjXl5OSkmJgYzZo1S7t379ZPP/1k7plbrVo1NWrUSKdOndKlS5dUuXLlAk/GtWHDBg0fPlzp6elydXVVjRo1FB8fr08++cQ8HrKtXLt2Tc8//7z++ecfVa1aVdWrVzePwytdv83x+++/V6lSpeTt7a1atWopPj5ehw4d0v79+7Vq1SrNmzfP4rVv1KiRYmNjFRsbazE+ZkEkJSVp0KBBCg8Pl3R9zDt/f38dP35cv/76q5YvX67PP/881wA3IyNDAwcO1LZt23TPPfeoWrVqOn78uLZs2aLt27dr7ty5qlevnsU2o0aN0vLlyyVJFSpUkL+/v5KSkszjeaanp+v+++8v8LGUdFkn8IuIiChwz/PU1FS9+uqr2rhxoyTJ09NTgYGBSkxM1IEDB7Rnzx5VrlzZYqiZo0ePqm/fvjp37pwcHBxUo0YNGY1GHT16VEeOHNHSpUs1a9asXM+tU6dOacKECUpMTNS9996rGjVqWIxlunz5cr399ttKS0tTqVKlVLVqVaWkpOjAgQOKjIzUb7/9pm+//TbXyQtvVa9evbRw4UJdunRJe/bsyddt5r///rtefPFF83UiICBAjo6OOnv2rLZt26Zt27apV69ecnR0VOXKldWoUSPz9ePGcYVzukb9888/mjZtmjIzM1WjRg2VLVu2QLdTp6enq3fv3tq9e7fuuece1ahRQ//++69WrVqlzZs3a+bMmWrSpEm+68uNh4eHGjVqpCNHjigpKSnbWPH5Dcdu9Rw7e/asunTpogsXLqh69ery9fXV6dOntWDBAm3fvl1Lly616vyJj483v1e6du0qR0dHPfroo5o1a5YWLVqk7t2757idNe8zk3Xr1mnixIlydnZWQECA3N3dza+9tdfiL774QtOmTZN0PRANDAzUtWvXdPbsWa1atUqxsbF6+OGHzesX5PwuDKYvY11dXQu0nek1jYiIkNFotGrIgdvxXgkODpaTk5NOnjwpd3f3bO8L07BJt/t5B3D34eoBAHeQJUuWaM6cOXJwcNC7776rp556Svb29srMzNScOXM0fvx4TZs2TbVr11bHjh2zbX/u3DnNmTNHY8eO1VNPPSU7OztlZGTo66+/1tSpUzVhwgQ1bNgwz4muioMNGzZIuj5Wa9ZeQ7l59dVX9fvvv2vFihXq37+/Vb06OnbsqODgYEVGRmrGjBl64403ClxHVvHx8RZjq1rj8uXLeuWVV5SUlKQ2bdrok08+MYe5+/fv10svvaQDBw5o9OjRmjFjRo51TJgwQZ06ddK7775rnhzn77//1pAhQ/TPP/9o2bJl5p5sgwcP1uDBg80zo3fr1q1AE56dP39eb775ptLT0/Xoo49q7NixcnNzk3T9j6NXXnlF6enpVj0X+bF27VpVrFhRCxcuVEhIiKTrf8ybPPTQQ2rTpo0aN25scQt6bGysPvjgA23atEkTJ060uA19/vz5mjJlir766qscx8fMj3Hjxik8PFzlypXTlClTzH+QJiUlacyYMVq5cqVGjhypFStW5Dhe6m+//aYKFSpo0aJFCg4OlnT93HjxxRe1e/duTZgwQT/++KN5/YMHD2r58uVyd3fX1KlTdd9995nLjEajwsPDb7l3W0lVo0YN+fv7KyoqSh9//LHOnj2r9u3bq2bNmrmOPZ3V+++/r40bN6pMmTL68MMP1b59e/N2qamp+u233yxew7S0NA0dOlTnzp1T/fr19cUXX6hKlSqSrvdCGzZsmA4dOqSXX35ZixcvzjY0gnS9h+l9992n8ePHm4dkMJ3Xu3bt0ujRo2VnZ6e33npLPXv2NAeUJ06c0Ouvv67du3fr448/1scff3xrT14uatWqZR7rc/fu3fkKYSdOnKj09HT1799fw4YNU6lSpcxlMTExWrVqlfl5ffLJJ/Xkk0+afxfkZ1zhL7/8Uo888ojGjBljnsQr67UgL2vXrpW7u7vmzZunxo0bS7p+TR81apS2bNmi4cOHa82aNbc84VjdunU1f/58Pf/88woLC7NqrPjCOMdM14lff/3VfI4dOHBAAwYMUFRUlL7//nu9/PLLBT6+FStWKC0tTRUrVlTz5s0lSV26dDF/CXjs2LEcvwgp6Pssq//+97/q3bu3hg8fbg7nTK+9Ndfiy5cv65tvvpGjo6P++9//qmPHjhZhZWRkpA4ePGjRhoKc37fq3Llz5p7UBR1nt02bNoqMjNSuXbv04osv6plnnlHjxo0LFObejvfK5MmTtXjxYvO4t3Pnzs1xvdv5vAO4O3EFAYA7iGlM06eeeko9e/Y0f1B0cHBQnz599Oijj0q6Pm5pTjIyMtStWzc9/fTT5j8QHB0d9corr6hFixYyGAzmCRiKI4PBoDNnzmjWrFn673//K+l6zxnTH5M3U7t2bXXu3FkGg0GTJk2yav92dnbmoQ1+/PFHnTt3zqp6TLJu7+/vb1Ud8+fP1+XLl1WuXDl9/vnnFr1pg4KCzDPHb9myJdfb0v39/TVu3DiLP4CaN2+uJ598UpK0efNmq9qWk59//llXrlxRQECAPv30U3MAK10PQN944w2bhrCZmZmaOHGiOYCVZPFHWPPmzdWsWbNsIUTlypU1adIkOTk5acWKFcrMzCy0NkVHR5snpXnvvfcsegS5u7tr/Pjx8vPzU3JysmbNmpVjHenp6ZowYYI5gJWu98Yyja8YHh5ucbux6RbQ+++/3yKAla6f502bNs1x2Ivb4cZbyU3/CqNXYX7Y2dlp/PjxKlOmjJKTk/XVV1/pscceU+PGjfXMM89owoQJ5l5yNzp06JCWLFki6Xoo0LFjR4s/6EuVKqUuXbpYHMvq1at14sQJOTk5afLkyRbXs6pVq+qLL76Qg4ODDh06ZP7y6UZly5bV5MmTLcbENZ3X//3vf5WRkaHXXntNvXv3tughGhAQoClTpsjV1VVLly695Wtabuzs7My9US9dupSvbUzn6ODBgy3eo9L1cS4HDBhwS2FJQECA+XU2uXE/N5Oenq63337bHCpJ13tjTpo0SWXLltWFCxe0cOFCq9tXmArjHHN3d9fnn39ucY7VrVtX/fv3l2T97wnTUASPP/64+fWsXbu26tata1GelTXvs6yaN2+uUaNGWUwqWapUKauvxadPn1ZmZqZq1qypRx55JFtv0eDg4Gw9em19fmc1duxYpaeny8fHx/yFan7169fPHNxu2rRJ/fv3V+PGjdWpUyeNGjVKS5cu1dWrV29aR3F6r9zO5x3A3YkrCADcIY4dO2Yemyu3IQP69esnSTpy5IhiYmJyXKd37943Xf7XX3/ZNAQrqKxjbNapU8c8VqmdnZ369++vsWPH5ruuV155RU5OTtq8ebN27dplVXseeOABNW/eXNeuXct1vNP8yjq+aEFvETTZunWrpOvBfE7DMjRv3tz8x6xpsqEbPfXUUxa3LpuYekTfbAy8gvrjjz8kSU888USOt/t17do1z3Fxb8W9996rRo0a3XSdpKQk/frrrxo9erT69eunZ555Rj179lTfvn1lZ2en5ORkiyEMbtUff/whg8GgKlWqqEOHDtnKHR0dze/P3F7D3ELKunXrytnZWUaj0Xz9kGQOYfbs2aOoqKhCOIrCExwcrEaNGmX7dzt76Ddu3FgrV65U3759zZPNJCcna9euXfruu+/07LPP6umnn7Z4TiWZx31s2LChuVdfXkzv4UceeUSVKlXKVh4QEGC+9Tm3179Dhw459iI7d+6cIiIi5OjoaP5S5UaVK1dWcHCwMjMztXPnzny12Rqma1xegY2J6RxdsWKFTdrTpUuXW7rl2MfHJ8cvKtzc3MzPdW6v1+1WGOdY586dLQJrk1v5PXHw4EFzD9EuXbpYlJkeL1++PNuXXta8z7LKLYi09lpsOldPnjypffv25asNtj6/Tb7++mtt3LhRdnZ2+vTTTwv8WaN06dKaPXu2xo0bp4YNG8re3l4Gg0HHjh3T0qVLNWrUKLVt29YcXuekOL1XbtfzDuDuxXAEAHCHOHHihCSZx/PLSY0aNeTg4KDMzEwdP348Ww9RR0dHBQQE5LqtdH3csOjo6FzXu92yjrGZmpqqU6dO6erVq3J1dVWzZs0K9Ee0v7+/evTooZ9++kkTJ07UvHnzrGrTyJEj9eSTT2rx4sXq27evqlevblU9WcfPS05OtqoO03lxs+EVatWqpQMHDpjXvVG1atVyXF6+fHlJ+Q9N8sPUhtq1a+dYXqpUKQUEBOjw4cOFts+sTOd5bnbu3KlXXnklz9568fHxhdYm03Ny77335toDx/T6RkdHKy0tLVtQndtraGdnp/Llyys2NtbidWzQoIGaNWumsLAwdejQQY0bN1bTpk3VoEEDNW7c2KKH8u2Wn1vJb4eKFStq1KhRGjVqlKKiohQZGamIiAht2bJFp0+f1u7du9WrVy8tW7bMPEbwkSNHJFlOHJgX0+tfs2bNXNepVauW1q9fn+t7OLdtDx06JEmyt7fXgAEDcq3f9KVCbGxsfppsFdP5l99bjgcMGKC3335bY8eO1axZs/TAAw+oYcOGatq0qXx9fW+5PTd7vvOjevXqOd62n7Xugk6CZCuFcY7l9pnA29tbknW/J0y9XOvVq5dtyIFHH31Un332mS5cuKCtW7dajMFqzfssq9yeB2uvxRUqVNDjjz+uZcuWqXv37qpXr56aNWumBg0aqGnTpjmOH2/r81u6fqfM5MmTJUnvvvuuHnzwQavqMX2J8+STT+rKlSuKjIxUZGSktm3bprCwMPPQAqVKlcpxKKzi9F65Hc87gLsbISwA3CFMf+CYgrGcODo6ysvLSxcvXszxDyIvL69cPwib/pDKuq/i4MYxNlNTUzVt2jRNnz5dL730kn755RcFBQXlu74XX3xRS5Ys0a5du7R582a1adOmwG0KCQlRhw4dtHbtWn3xxRfmP3IKytTDTpKioqJUp06dAtdheq2yvn438vHxsVj3RrlNbGaLW/JMYfPNQj5bBoA36wWUlJSkl19+WZcvX1bz5s01cOBABQYGqkyZMuaewq1bt1ZsbKwyMjIKrU2m18X0OuUka9nVq1ezhbA3Oy7T65h1xmc7OztNnz5d33zzjZYuXaqwsDDzmIGlSpXSf/7zH7322ms3nXE7q549e+a4fPLkyTc9rpLCNDnPI488ojfffFMzZ87U559/rtjYWP3yyy8aOHCgpP/v3V6QsQ0L8voX9D2ckJAg6fqYoPmZ9K4gY6IWhNFoNAe8N/sdltWTTz6psmXL6rvvvtOePXv0888/6+eff5Yk1a9fXyNHjsw2lEZB5DWhY15uds21xRdYt8KW55g1EzVJ189JU2/EG3vBSteHU3nooYe0ceNGLVq0yCKEteZ9llVux3Ir1+Jx48apVq1aWrBggfbs2aM9e/ZIuv65rG3btho1apTFl0u2Pr8XLFhgvlPozTff1LPPPmt1XVl5eHioefPmat68uQYMGKDIyEgNGDBAly9f1hdffJFjCFuc3iu2ft4BgBAWAO4QpmDqZj30MjIyFBcXZ7F+VnFxccrMzMwxiL148WK2fRVHpUqV0ogRI3T69GmtXr1ao0eP1tKlS/MdGHp7e6t3796aNm2aJk2apFatWlnVjuHDh2vDhg1au3at9u7da1Udnp6eqlGjho4ePart27erffv2Ba7Dzc1NiYmJFq/fjUwTLBWH19XV1VVXrly56R9cuZWZ/tjPGibeyNoexdL1W3YvX76sypUra/r06dnGizMajeZQqzCZXpebTYSVtaywXkc3NzeNGDFCI0aM0KlTpxQREaG//vpL69ev16JFi3T8+HH99NNPuX5xk1VuAZ9pRu47iYODgwYPHqx169Zp//79+ueff8xlpt7tWcffzUtBXv+CvvamcL5KlSqFOrZzQR0+fFiJiYmSVKBhJR5++GE9/PDDunLliiIiIhQeHq7ffvtNe/bsUb9+/bRw4cJce9Xb2s2uuabf07m9Xrldw1JSUm69YTmw5TlmrQ0bNpjvKPjggw8sJju80datW3Xp0iVzYGfN+yw/buVa7OTkpP79+6t///46e/asIiIitH37dv32229at26dDh06pKVLl1psY6vze+nSpXrvvfdkNBr12muv5TqEVWEIDg7Wiy++qI8++kgnTpxQYmJitmErbuW9YgvF+boCoORjTFgAuEOYbnlPTU3NNg6hydGjR81jp+U0m3BGRkauY1kePXpUkuTi4lIsbgXOy6hRo+Ts7KzDhw/fdCyynPTv31+enp46cuSIli9fbtX+q1evbh5XztqJviTpsccek3R93DtTgF7Qdkj/f3tmTkxl1g6bUJhMt7SabpO+UWpqaq7nqKn3Um5/0CUkJFj1HJpER0dLut7TOacJeo4cOZJryGttbzDp/1+XY8eOyWAw5LiO6TX09/e3yZi599xzj7p27arPPvtMv/zyi+zs7LR79+5sM3rn5vDhwzn+KwnXEmuZhknJOoZ2YGCgJGn37t35rsf0+v/777+5rmPte9jUnrNnzxbqEBoFNWfOHEnXexHWq1evwNt7eHioVatWGjlypNasWaMGDRooPT1dCxYsKOym5tvx48dznaDP9Fre+HvYFIrn9mVqYY41nZUtzzFrmYYicHV1lbe3d67/nJyclJ6ermXLlpm3teZ9lh+FdS2uVKmSOnXqpA8++EArVqyQu7u7Tp8+rW3btuW4fmGe3ytWrNDo0aNlMBj06quv3nQYksJiuhZKynFOAWveK9Yo6O/h4nhdAVDyEcICwB2ievXq5g+6uc2Qblpeq1Yt80zUN/rhhx9yXG76I/mBBx7IcZKm4qZSpUrq0aOHJGnq1KkFuj3c3d1dgwcPliRNmTLF6onIhg4dqlKlSunvv//Wn3/+aVUdzzzzjHx8fHTlyhWNGjVKaWlpN13/+PHjWrNmjfmxqSfvL7/8kmMvqu3bt+vAgQMW6xalli1bSrreUyen12zZsmW59p40nf8HDx7M8XmydoxfE1PwmlsvqO+++y7Pba3pydayZUvZ29srJiZGa9euzVaekZFhfn+2bt26wPUXVGBgoPk233Pnztl8f8VNcnJynj2q09LSzLcbZx2Pt3379uYAe8eOHfnan+l9uWbNGp09ezZb+alTp7Rp0yZJBX/9/f39FRQUJIPBkOvvDVtbsWKFOXAbOHDgLX+J4OjoaA5ybzw/TV/U2GpYhawuXLig3377Ldvyq1evmo/3xtfLdA3LLTycP39+rvszHZs11xhbnmPWOHv2rP766y9J0oQJE/Tnn3/m+s90G/3ixYvN21vzPssPW1yLK1asaP4yKj/X05ud33lZvXq1Ro0aJYPBoFdeeUWDBg0q0PY5uVkvVhPTRKeenp7y8vLKVm7Ne8Uat/J7+FaedwDIihAWAO4gQ4YMkXQ9cPv555/NtzQaDAb98MMP5p4iL730Uo7bOzo6auHChfr111/N22ZkZOirr77Stm3bZG9vXygf2m8X0x/0UVFRBe4N++yzz6pKlSqKjo5WeHi4VfuvWLGinnvuOUkq8P5NPDw89MUXX8jFxUVbt27VM888o7/++itbr5HTp0/rs88+0xNPPGHutSxJTz/9tMqVK6fLly9rxIgRFj1BDx48qLfeekuS1KZNGwUHB1vVxsL09NNPy8PDQ8eOHdPbb79tEXZt27ZN48ePz/VLgPvvv1+urq66dOmSJkyYYPEcrV69WjNmzLilLxCaNm0q6XpA8ssvv5iXp6Wl6YsvvtCKFStyrd8Uruzdu7fAY9v5+vqax0T88MMPLc7HpKQkjR49WlFRUXJ1ddULL7xQoLpzs2zZMn355ZfZJkNJT0/Xt99+q8TERDk4OKhu3bqFsr+SJDo6Wm3atNGkSZN06NChbLeOHzlyRMOGDdOZM2fME9aY1KpVS127dpUkvfzyy1q/fr3F9teuXdOyZcssXuNHHnlEAQEBSk9P1yuvvKKYmBhzWVRUlIYPH67MzEzVrl1boaGhBT6eN998U46OjpoxY4Y+//xz87AAWdu0detWvfzyywWuOzcGg0GRkZEaPXq0XnvtNUlSx44d9fzzz+dre9P4zH/88Ue2L1wiIyPNX0SFhIRYlJkmrTQFfLbk5OSkcePGWQSqCQkJeu211xQfHy8fHx/z3RImpnFNN2/erFWrVpmXX7t2TZMmTTKPy5wT07Ht2LEj116aubH1OVZQixcvlsFgULly5fIM35544glJ13tMmr74sOZ9lh/WXov/+usvjRs3Tvv377doh8Fg0PLly829PU3nq7Xn981s2LBBr7/+ujIzMzVs2DC9+OKLBTr23AwZMkTDhg3T5s2bs4WbV65c0TfffGP+gvLJJ5/McWgoa94r1jD9Hj569GiOX6ba4nkHgBsxJiwA3EG6du2qAwcOaM6cORozZoymTJmiypUr68yZM7p8+bIkafDgwTlOjCBdDw3bt2+vd999V5MnT1alSpUUFRVlvk311VdfLfBsw7t27bL4sG/qxRgREWExuUHnzp317rvvFqjuvFSsWFHdu3fXTz/9pGnTpqlLly5ydMzfrz5nZ2cNHTpUb731Vq63yeXHwIED9euvv2YLNgqiSZMm+umnn/Tqq69q37596tOnj9zd3eXr6ysnJyedP39e58+fl3Q9tM06EVm5cuX05ZdfasiQIdq8ebMeeugh1axZUykpKTp+/LgkqU6dOvr444+tbl9hqlChgj799FO98sorWrp0qdatW6fq1asrISFBUVFR6tChgy5fvqydO3dm+2PO3d1dr776qj766CPNnTtXy5YtU9WqVXXu3DlduHBBL7/8shYtWqQzZ85Y1ba6deuaZ7h+77339NVXX6lChQo6deqUrly5ouHDh2vBggU51t+iRQt5e3srJiZGrVu3VkBAgFxcXCRJc+fOzXPfb7/9tk6dOqVdu3bp2Wef1T333GMOq1NSUlSqVClNnDix0G7vj4uL09SpUzV16lR5enrK19dXRqNR0dHR5nP5tddey7VHfXERGxtrMamPqVd7bGysxfWnUaNGmjZtWr7qtLOzU3x8vGbMmKEZM2aY34uOjo46f/68+Y97Z2dnffDBB+Zbo03GjBmj+Ph4bdq0SUOHDpWnp6f8/f2VmJiomJgYpaen65NPPlGTJk3M9UyZMkV9+/bVP//8o3bt2qlGjRoyGo06evSoDAaDfH19NXny5HyNz3ujZs2a6bPPPtPbb7+t6dOn69tvv1VAQIDc3NyUkJCg6Ohoq+8GkKTff//dPDGb0WhUUlKSYmJizF9GlCpVSoMHD9bAgQPzfbuwwWDQ2rVrtXbtWjk5Oemee+4xfwFjev/Vr19fvXr1stiuS5cuGj9+vMaNG6f58+erfPnysrOzU9euXc1hXmHp0KGDzpw5o6efflrVqlWTm5ubjh49qmvXrql06dKaOHFitrEx77//frVv317r1q3Tq6++qvHjx8vHx0cnTpxQRkaG3nvvvVx/Tz722GP66aeftHbtWrVu3dp8TtauXVtvv/32Tdtq63OsIIxGo5YsWWI+pry+OAsMDFRwcLAiIyO1aNEi1a9fX1LB32f5Zc21ODk5WXPmzNGcOXPk7u4uf39/2dvbKzY21vzZ7Pnnnzd/vrL2/L6Z4cOHKyMjQ87OzuZexDnx8fEp0GSiBoNB69at07p16+Tg4CB/f3+VKVNGCQkJio2NNYeZ7dq1y/WLHGveK9aoU6eOatWqpSNHjujhhx/Wvffeax4CZNKkSXJxcSn05x0AbkQICwB3mLffflsPPvig5s+frz179ujgwYMqW7asQkND9fzzz6t58+Y33f7NN99UzZo1NX/+fHMvuGbNmql///5W3a6ekZGR41iDNy631cy3gwYN0oIFCxQdHa0lS5aoe/fu+d62S5cu+v777y16lhZU2bJlNWDAAE2cONHqOqTrPS/WrFmjVatWadOmTdq3b59Onz6tjIwMeXl56aGHHlKrVq306KOPqmzZshbbNmvWTCtWrNB3332n33//Xf/++68cHR0VHBysTp066dlnn81xjNOi0q5dO/3yyy/66quvFBERoX///VdVq1bVG2+8oRdeeMH8GpomX8nq+eefV/ny5TVr1iwdOXJEJ06cUJ06dTRmzBg9/PDD5lsbrfXJJ5+oZs2aWrRokaKjo3Xt2jXVrVtXvXr1Urt27XIdK87V1VWzZ8/WlClTFBERof379xd4iIwffvhBv/76q1asWKF///1XMTExqlChgh544AH179/f4rb3W9WhQwcZDAbt2LFDR48e1YkTJ5Seni5vb289+OCDevbZZwscXhSFzMzMHK8/BoPBYrlpNvX8qFmzplauXKk///xTf//9t44ePaqTJ08qIyND7u7uCgkJ0f3336+nnnpK/v7+2bYvVaqUpk6dqt9++02LFy/W/v37dejQIZUtW1ZBQUFq3bq1eViOrPtcvny5Zs2apY0bN+rUqVOys7NTjRo11K5dO/Xp0+eWQopOnTqpUaNGmjt3rrZt26aoqCilpqbKw8NDwcHBatGihdq1a2dV3ZcuXTKPcerk5CQ3NzdVqlRJtWvXVpMmTdS5c+cCt93NzU3//e9/tX37du3du1fnz5/XlStX5O7uriZNmuiRRx5Rjx49sg1tYOqduGzZMp06dcr8RVSzZs2sOrabcXJy0g8//KBp06ZpzZo1+vfff+Xm5qa2bdtq2LBhuY5xOXHiRH377bdatmyZzpw5o7S0NLVo0UIvvfRSjtc8k3r16unrr7/WrFmzdOjQIf3zzz8F6hFr63Msv3bu3Gke1z6/vR+7deumyMhIrVq1Sm+99ZZKlSpl1fssP6y5Fjdu3Fjvvfeetm/friNHjpjfX15eXmrTpo169Ohh7gUtWX9+34zpi5S0tLRcJ0qUrvf2LYhvv/3WPOTSvn37FBMTo6ioKDk7O6tKlSoKDg7WY489dtPPj9a+VwrKzs5OM2fO1BdffKHt27fr8OHD5ufl2rVrKleuXKE/7wBwIzvjzaYQBgDcFXbs2KFevXrJ19fXPO4bUBxlZmaqWbNmSkpK0rJly5ihGACAEmjKlCn66quv1LVrV3366adF3RwAuC0YExYAAJQYa9asUVJSkjw9PVWjRo2ibg4AAAAA5AshLAAAKFZ+//13rV692mJiDKPRqPXr12vs2LGSpJ49e+Z7fF8AAAAAKGr89QIAAIqV06dP68MPP5SLi4uqVasmFxcXRUVFKS4uTpJ03333FdrMzgAAAABwOxDCAgCAYqV58+Z67rnnFBYWpnPnzikpKUlubm5q2rSp/vOf/+jJJ5/Mc8ZsAAAAAChOmJgLAAAAAAAAAGyIMWEBAAAAAAAAwIYYjqCYaNKkidLS0uTj41PUTQEAAAAAAACQDxcuXJCzs7PCw8Nvuh4hbDFx7do1ZWZmFnUzAAAAAAAAAORTRkaG8jPaKyFsMVGhQgVJ0saNG4u4JQAAAAAAAADyIzQ0NF/rMSYsAAAAAAAAANgQISwAAAAAAAAA2BAhLAAAAAAAAADYECEsAAAAAAAAANgQISwAAAAAAAAA2BAhLAAAAAAAAADYECEsAAAAAAAAANgQISwAAAAAAAAA2BAhLAAAAAAAAADYECEsAAAAAAAAANiQY1E3IL/S0tI0a9YsLV++XFFRUXJ1dVWTJk00ZMgQBQUFFbg+g8GghQsXatmyZTp69KiSk5Pl7e2t4OBg9e7dW02aNMm2zd69e/X9999r165dunz5slxcXHTvvffqP//5j5555hk5OzsXxqECAAAAAAAAuIOUiBA2LS1N/fr1U1hYmMqXL682bdrowoULWr9+vbZs2aJp06apZcuW+a4vKSlJgwYNUnh4uLy8vNSwYUO5uLgoJiZGmzdvVp06dbKFsKtXr9bIkSNlMBgUGBioxo0bKzExUTt37tTevXu1fv16zZ49W05OToV9+AAAAAAAAABKsBIRws6cOVNhYWEKCQnR7Nmz5e7uLklauXKlRo4cqddff10bNmwwL8/LyJEjFR4err59+2rEiBEWPVjj4+MVFxdnsX5aWprGjh0rg8Ggjz76SN27dzeXxcbG6plnnlF4eLgWLVqkp59+uhCOGAAAAAAAAMCdotiPCZuRkaE5c+ZIksaMGWMRtHbu3FmtWrVSXFycFi1alK/6NmzYoC1btig0NFSjRo3KNoSAp6enAgICLJYdOXJE8fHx8vX1tQhgJaly5crm4HXPnj0FPj4AAAAAAAAAd7ZiH8JGREQoPj5efn5+CgkJyVbeqVMnSdLGjRvzVd/8+fMlSS+88EK+25DfsV69vLzyXScAAAAAAACAu0OxH47g4MGDkpTr5Ft169aVJB0+fDjPujIyMhQeHi4HBwc1aNBAx44d05o1a3T+/Hl5eXmpRYsWatasWbbtAgIC5OvrqzNnzmjBggXZhiP4+eef5ejoqK5du1pziAAAAMiH48ePKz4+vqibYbW4uLgS+6W9p6enqlevXtTNAAAAKLGKfQgbExMjSapUqVKO5abl8fHxunr1qtzc3HKtKyoqSqmpqfL29tbcuXM1ceJEZWZmmsunT5+u1q1ba9KkSRb1ODk5acKECXrxxRf1zjvvaO7cuapevbp5Yq5KlSppxowZqlmzZmEcMgAAAG5w8eJF1axZUwaDoaibcldycHDQ2bNn5e3tXdRNAQAAKJGKfQibnJwsSSpdunSO5a6uruaf8wphExISJF0PbCdMmKAuXbpo0KBB8vHxUXh4uMaMGaMtW7bo/fff12effWaxbZMmTfTTTz9p6NChOnz4sLnnrb29vZo0aSJ/f/9bOk6TrKEwAAAArvPy8tKhQ4dKbE/YQ4cOqVevXpozZ45q165d1M0pME9PT3l5efFZFQAAwErFPoQtTKaeExkZGWrWrJnGjx9vLmvTpo28vb3VvXt3rVixQsOGDVPVqlXN5atWrdLo0aNVt25djRs3TrVr11ZCQoJWrVqlKVOmaOPGjfrhhx9Up06dW2rflStXrD9AAACAO5iPj498fHyKuhlWMXUsqFq1aom9e4rPqQAAANkZDAbZ2+c97VaxD2FNPV1TUlJyLDd9oJV0016wWeuSpB49emQrDwkJUVBQkCIjIxUWFmYOYU+ePKlRo0bJ09NT3377rdzd3SVJ7u7uGjhwoCRp4sSJGjdunH788ccCHJ0le3t7eXh4WL09AAAAiifT51BXV1c+7wEAANxB8hPASiUghK1SpYok6ezZszmWm5Z7enrmGcL6+vqaf/bz88txHT8/P0VGRurixYvmZatWrVJ6erpatWplDmCz6ty5syZOnKhdu3YpLS1Nzs7ONz+om3BwcLB6WwAAABRPps94Dg4OfN4DAAC4C+Uvqi1Cptv79+/fn2P5gQMHJEmBgYF51uXh4WHu3WoaH/ZGpnHGsvaaPXfunHn7nJQpU0bS9e7HiYmJebYDAAAAAAAAwN2j2IewjRo1kqenp6Kjo7Vv375s5atXr5YkhYaG5qs+03rbt2/PVpaYmGgOdYOCgszLTWOP7d27N8c69+zZI+l6cOvl5ZWvdgAAAAAAAAC4OxT7ENbR0VG9evWSJI0dO1ZJSUnmspUrV2rr1q3y8vJSt27dzMv37t2rjh07qmPHjtnq6927t0qVKqV58+ZZBLFpaWkaO3asEhMTVbt2bTVq1Mhc9vDDD8vOzk67du3S7NmzZTQazWXR0dEaN26cJKlDhw7cXgYAAAAAAADAQrEfE1aSBgwYoO3btyssLEzt27dX06ZNdfHiRYWHh8vJyUkTJkywGKs1JSVFJ06cyLGuypUra9y4cXrjjTfUp08f1a9fX97e3tq3b5/Onj0rb29vTZo0SXZ2duZtateurSFDhmjq1Kn65JNPNH/+fAUGBio+Pl579uxRamqqqlWrptdff93mzwUAAAAAAACAkqVEhLDOzs767rvv9P3332v58uXatGmTXF1dFRoaqpdeesli6ID86Ny5s/z9/TVjxgxFREQoMjJSFSpU0LPPPqtBgwapYsWK2bZ55ZVX1KhRI82bN0/79u3Txo0b5ezsrICAALVr104vvPBCjpN2AQAAAAAAALi72Rmz3luPImMaq3bjxo1F3BIAAAAUtoiICDVu3Fi7du2yGPYKAAAAJVt+M71iPyYsAAAAAAAAAJRkhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDjkXdgPxKS0vTrFmztHz5ckVFRcnV1VVNmjTRkCFDFBQUVOD6DAaDFi5cqGXLluno0aNKTk6Wt7e3goOD1bt3bzVp0iTH7c6fP6/vvvtOW7Zs0dmzZ+Xk5KTKlSuradOmGjlypNzc3G71UAEAAAAAAADcQUpECJuWlqZ+/fopLCxM5cuXV5s2bXThwgWtX79eW7Zs0bRp09SyZct815eUlKRBgwYpPDxcXl5eatiwoVxcXBQTE6PNmzerTp06OYawYWFhevHFF3XlyhXVqlVLbdu21dWrV3XixAn99NNPGjhwICEsAAAAAAAAAAslIoSdOXOmwsLCFBISotmzZ8vd3V2StHLlSo0cOVKvv/66NmzYYF6el5EjRyo8PFx9+/bViBEj5OzsbC6Lj49XXFxctm2ioqI0aNAgOTk5adasWXrggQcsyg8dOqSyZcvewlECAAAAAAAAuBMV+zFhMzIyNGfOHEnSmDFjLILWzp07q1WrVoqLi9OiRYvyVd+GDRu0ZcsWhYaGatSoURYBrCR5enoqICAg23affvqpkpOT9fHHH2cLYCWpdu3aKl26dEEODQAAAAAAAMBdoNiHsBEREYqPj5efn59CQkKylXfq1EmStHHjxnzVN3/+fEnSCy+8kO82nDt3Tps3b5avr6/atWuX7+0AAAAAAAAAoNgPR3Dw4EFJynXyrbp160qSDh8+nGddGRkZCg8Pl4ODgxo0aKBjx45pzZo1On/+vLy8vNSiRQs1a9Ys23Y7duxQZmamGjduLIPBoE2bNmnnzp26du2aqlWrpg4dOqhy5cq3cJQAAAAAAAAA7lTFPoSNiYmRJFWqVCnHctPy+Ph4Xb169aYTY0VFRSk1NVXe3t6aO3euJk6cqMzMTHP59OnT1bp1a02aNMminqNHj0qS3Nzc9Pzzzys8PNyi3okTJ+rtt9/W008/bd1BAgAAAAAAALhjFfsQNjk5WZJyHW/V1dXV/HNeIWxCQoKk64HthAkT1KVLFw0aNEg+Pj4KDw/XmDFjtGXLFr3//vv67LPPzNvFx8dLkhYuXCgnJyd99NFHCg0NVWpqqhYuXKhp06bp/fffl7+/v1q0aHFLx5s1FAYAAMCdwfQZLzMzk897AAAAd6FiH8IWJoPBIOn6sATNmjXT+PHjzWVt2rSRt7e3unfvrhUrVmjYsGGqWrWqJMloNEqS0tPT9eGHH6pr167m7V5++WUlJiZq7ty5mjp16i2FsAaDQVeuXLF6ewAAABRPpo4FycnJfN4DAAC4gxgMBtnb5z3tVrEPYU09XVNSUnIsN32glXTTXrBZ65KkHj16ZCsPCQlRUFCQIiMjFRYWZg5hTdu5uLjo8ccfz7bdU089pblz5+qff/5RWlqanJ2d8ziqnNnb28vDw8OqbQEAAFB8mT5Purq68nkPAADgDpKfAFYqASFslSpVJElnz57Nsdy03NPTM88Q1tfX1/yzn59fjuv4+fkpMjJSFy9ezLZd5cqVc3xiTXVlZGQoLi5OFStWvGk7bsbBwcHqbQEAAFA8mT7jOTg48HkPAADgLpS/qLYI1alTR5K0f//+HMsPHDggSQoMDMyzLg8PD3PvVtP4sDcyjf+atdds3bp187WNlHdvXAAAAAAAAAB3l2IfwjZq1Eienp6Kjo7Wvn37spWvXr1akhQaGpqv+kzrbd++PVtZYmKiOdQNCgoyL2/YsKHKlSunuLg4HT58ONt2f//9tyTpnnvukbu7e77aAQAAAAAAAODuUOxDWEdHR/Xq1UuSNHbsWCUlJZnLVq5cqa1bt8rLy0vdunUzL9+7d686duyojh07Zquvd+/eKlWqlObNm2cRxKalpWns2LFKTExU7dq11ahRI3OZg4OD+vfvL0kaM2aM4uLizGXHjh3Tl19+KUl65plnCumoAQAAAAAAANwpiv2YsJI0YMAAbd++XWFhYWrfvr2aNm2qixcvKjw8XE5OTpowYYJFD9SUlBSdOHEix7oqV66scePG6Y033lCfPn1Uv359eXt7a9++fTp79qy8vb01adIk2dnZWWz3wgsvaOfOndq8ebM6duyoBg0aKDU1Vf/8849SU1PVrl07c1gMAABQXF24cEGJiYlF3Yy7TlRUlPn/smXLFnFr7j5lypSRj49PUTcDAADcxUpECOvs7KzvvvtO33//vZYvX65NmzbJ1dVVoaGheumllyyGDsiPzp07y9/fXzNmzFBERIQiIyNVoUIFPfvssxo0aFCOE2s5ODho6tSpmjdvnhYvXqwdO3ZIkmrWrKlu3brpqaeeyvdsaAAAAEXhwoUL6j9okK6mpBZ1U+46KcnJkqQJkz5X6SxzD+D2cCtdSt/OmEEQCwAAioyd0Wg0FnUj8P9j1W7cuLGIWwIAAO5Ux44d06Chw9Sl72BVqOxb1M256yTGx6uMp2dRN+Oucz72jJZ+P10zvpqie++9t6ibAwAA7jD5zfRKRE9YAAAAFJ4KlX3lVy2gqJsBAAAA3DW4fx4AAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGzI8VYrSExM1IIFC/T333/r7NmzSk1N1YYNG8zlW7ZsUXx8vDp16iRnZ+db3R0AAAAAAAAAlCi3FMLu3LlTr7zyiuLi4mQ0GiVJdnZ2Fuvs2bNH06dPl6enp1q3bn0ruwMAAAAAAACAEsfq4Qiio6M1ePBgXb58Wa1bt9ann36qmjVrZluvU6dOMhqN2rhx4y01FAAAAAAAAABKIqtD2G+++UZXr17VwIEDNW3aNHXp0kUeHh7Z1qtZs6bKlCmjiIiIW2ooAAAAAAAAAJREVoewf/75p0qXLq1hw4blua6vr69iY2Ot3RUAAAAAAAAAlFhWh7Dnz59XtWrV5OTklOe6zs7OSktLs3ZXAAAAAAAAAFBiWR3Cli5dWgkJCfla9/z58ypbtqy1uwIAAAAAAACAEsvqELZ69eo6d+5cnsMMHD58WLGxsapTp461uwIAAAAAAACAEsvqELZjx47KzMzUxx9/rMzMzBzXuXbtmsaOHSs7Ozt16tTJ6kYCAAAAAAAAQEnlaO2GPXv21IIFC7Rhwwb17NlT3bp1U1JSkiRp586dOnTokH766SedPHlSdevW1WOPPVZojQYAAAAAAACAksLqENbFxUUzZ87UkCFDtHfvXu3bt89c1qtXL0mS0WhUrVq1NG3aNDk6Wr0rAAAAAAAAACixbikZrVKlihYuXKilS5dqzZo1OnTokBITE+Xq6qpatWrpkUceUffu3eXs7FxY7QUAAAAAAACAEsXqEDYmJkaSVKlSJXXv3l3du3cvtEYBAAAAAAAAwJ3C6hC2bdu2Kl++vP7444/CbA8AAAAAAAAA3FHsrd3Q3d1dvr6+sre3ugoAAAAAAAAAuONZnaAGBATo4sWLhdkWAAAAAAAAALjjWB3CdunSRTExMfr7778Lsz0AAAAAAAAAcEexOoR95pln1L59ew0fPlyrV6+WwWAozHYBAAAAAAAAwB3B6om5evfuLaPRqKSkJI0cOVLvvfeeAgICVLp06RzXt7Oz0w8//GB1QwEAAAAAAACgJLI6hA0LC7N4nJSUpH379uW6vp2dnbW7AgAAAAAAAIASy+oQ9pNPPinMdgAAAAAAAADAHcnqELZr166F2Q4AAAAAAAAAuCNZPTEXAAAAAAAAACBvVveEvVFGRobOnDmjq1evys3NTb6+vnJ0LLTqAQAAAAAAAKBEuuWUdO/evZo2bZr+/vtvXbt2zbzcxcVFLVq00ODBgxUSEnKru1FaWppmzZql5cuXKyoqSq6urmrSpImGDBmioKCgAtdnMBi0cOFCLVu2TEePHlVycrK8vb0VHBys3r17q0mTJjfd/tq1a+rSpYuOHz8u6frz4OLiYtWxAQAAAAAAALhz3VII++uvv+qDDz5QZmamjEajRVlqaqo2btyorVu3asyYMerevbvV+0lLS1O/fv0UFham8uXLq02bNrpw4YLWr1+vLVu2aNq0aWrZsmW+60tKStKgQYMUHh4uLy8vNWzYUC4uLoqJidHmzZtVp06dPEPYr776SidOnLD6mAAAAAAAAADcHawOYQ8cOKCxY8cqMzNTTZo0Ud++fVWrVi1VqFBB58+f15EjR/T9998rPDxc77//voKCglS3bl2r9jVz5kyFhYUpJCREs2fPlru7uyRp5cqVGjlypF5//XVt2LDBvDwvI0eOVHh4uPr27asRI0bI2dnZXBYfH6+4uLibbr9//359//33euqpp/Tzzz9bdUwAAAAAAAAA7g5WT8z13XffKTMzU3369NGPP/6otm3bys/PT87OzvLz81Pbtm31448/qm/fvsrMzNSsWbOs2k9GRobmzJkjSRozZoxF0Nq5c2e1atVKcXFxWrRoUb7q27Bhg7Zs2aLQ0FCNGjXKIoCVJE9PTwUEBOS6fXp6ut566y2VK1dOr732mhVHBAAAAAAAAOBuYnUIGx4erjJlyujVV1+96XojRoyQh4eHwsLCrNpPRESE4uPj5efnl+PYsp06dZIkbdy4MV/1zZ8/X5L0wgsvWNWeb775RocOHdKYMWPk4eFhVR0AAAAAAAAA7h5WD0dw6dIl1alTR05OTjddz8nJSdWqVdOhQ4es2s/BgwclKdfJt0xDHBw+fDjPujIyMhQeHi4HBwc1aNBAx44d05o1a3T+/Hl5eXmpRYsWatasWa7bHz16VNOmTVP79u3Vrl07K44GAAAAAAAAwN3G6hDWzc1NFy9ezNe6Fy9elKurq1X7iYmJkSRVqlQpx3LT8vj4eF29elVubm651hUVFaXU1FR5e3tr7ty5mjhxojIzM83l06dPV+vWrTVp0qRs9WRmZuqtt95SqVKl9M4771h1LAAAAAAAAADuPlaHsHXr1tX27du1ceNGhYaG5rrehg0bFBsbq+bNm1u1n+TkZElS6dKlcyzPGu7mFcImJCRIuh7YTpgwQV26dNGgQYPk4+Oj8PBwjRkzRlu2bNH777+vzz77zGLbH374QXv27NHYsWNVsWJFq44lP7KGwgAAAIUpMzNTMkoyGmU0Gou6OcDtYTRKxuvnP5+1AQBAUbE6hO3WrZv+/vtvvfbaa3r55Zf19NNPWwSlKSkpmj9/vqZMmSI7Ozs9+eSThdLgW2EwGCRdH5agWbNmGj9+vLmsTZs28vb2Vvfu3bVixQoNGzZMVatWlSSdOnVKX375pRo3bqynnnrKpu27cuWKzeoHAAB3t6tXryozM1MZmZnKyMgo6uYAt0XG/8LXq1ev8lkbAAAUOoPBIHv7vKfdsjqE7dy5s9atW6d169ZpwoQJ+vLLL+Xr6ytvb29dvHhRZ86c0bVr12Q0GtWhQwf95z//sWo/pp6uKSkpOZabespKumkv2Kx1SVKPHj2ylYeEhCgoKEiRkZEKCwtT1apVZTQa9c4778hgMOjDDz+UnZ2dNYeRL/b29kz2BQAAbMbNzU0ODg5ydHCQo6PVHwOBEsXRwUEODg5yc3PjszYAACh0+QlgpVsIYSXp888/17Rp0zR79mwlJSXp2LFjOnbsmLnc3d1dL7zwgoYMGWL1PqpUqSJJOnv2bI7lpuWenp55hrC+vr7mn/38/HJcx8/PT5GRkebxbq9cuaKwsDCVKVNG77//fq519+3bV/b29nrllVfUpEmTm7bjZhwcHKzeFgAA4GYcHBwkO0l2djb9YhkoVuzsJLvr5z+ftQEAQFG5pRDWwcFBQ4cOVb9+/RQeHq4TJ06Yx2WtXr26GjdunOtYrvlVp04dSdL+/ftzLD9w4IAkKTAwMM+6PDw8VLVqVZ0+fdo8PuyN4uPjJSnbRGKJiYkKCwvLte7w8HCL7QEAAAAAAABAusUQ1qR06dJq2bKlWrZsWRjVWWjUqJE8PT0VHR2tffv2KSQkxKJ89erVknTTycGyCg0N1axZs7R9+3a1bt3aoiwxMdEc6gYFBUmSypQpo8OHD+danyn83bt3r1xcXPLVBgAAAAAAAAB3j/wNWlCEHB0d1atXL0nS2LFjlZSUZC5buXKltm7dKi8vL3Xr1s28fO/everYsaM6duyYrb7evXurVKlSmjdvnrZv325enpaWprFjxyoxMVG1a9dWo0aNbHhUAAAAAAAAAO4WVveEDQ8P1+TJk/XII4+oZ8+eua43b948/fbbbxo+fLjVweaAAQO0fft2hYWFqX379mratKkuXryo8PBwOTk5acKECXJ3dzevn5KSohMnTuRYV+XKlTVu3Di98cYb6tOnj+rXry9vb2/t27dPZ8+elbe3tyZNmsQ4aQAAAAAAAAAKhdU9YZcsWaKdO3eab9vPTXBwsMLCwrR06VJrdyVnZ2d99913GjFihDw9PbVp0yYdPXpUoaGh+uWXX/TQQw8VqL7OnTtr/vz5atOmjU6ePKktW7bIwcFBzz77rBYvXqx7773X6rYCAAAAAAAAQFZW94SNiIiQu7u76tWrd9P16tWrJw8PD0VERFi7K0nXg9jBgwdr8ODBea5733333XQcV0mqX7++pk6dekttkpTnfgAAAAAAAADc3azuCXvu3Dn5+fnla11fX1+dO3fO2l0BAAAAAAAAQIlldQhrNBplMBjyvW56erq1uwIAAAAAAACAEsvqELZy5co6duyYrly5ctP1rly5omPHjqlixYrW7goAAAAAAAAASiyrQ9jmzZsrMzNTX3755U3Xmzx5sjIzM9W8eXNrdwUAAAAAAAAAJZbVIWzv3r3l6Oion376SaNHj9apU6csyk+dOqW33npLc+fOlaOjo1544YVbbSsAAAAAAAAAlDiO1m5YtWpVjR07Vu+8846WLl2qpUuXytPTU2XKlFFiYqLi4+MlSfb29vrggw9UrVq1QmoyAAAAAAAAAJQcVoewkvTEE0+ocuXK+uyzz3TgwAHFxcUpLi7OXB4cHKzXX39d99133y03FAAAALeurIeDUuIP69KZy+ZlLvbO8nQuowxDhi6lxWfbpmIpb0nS5WvxSjdmWJSVcXJXaYdSSs5I0ZWMqxZlzvZO8nIuK4PRoAvXLutG3i7l5GBnr/i0RF0zpFmUuTu6ys3RVamZ15SQbjkHgaOdg8q7eEmSzqVezFZvOWdPOdk7KiH9ilIzr1mUuTqUloeTm9Iy0xSXnmhRZi97+ZQqJ0m6kHpZBllOQuvlVEbODs66kn5VyZkpFmWlHFxU1slD6YYMXb7Jc3jpWpwyjJkWZWWdPFTKwUVXM5KVlJFsUWZ6bTKNBl3M4Tn0cSknezt7xaUlKM1gORGuh6ObXB1LKyUzVYnpSRZlTnaOKufiKSnn57C8s6cc7R1zfG3cHF3l7uiqa5lpir/hOXSws5e3i+k5vCSDjBblXs5l5WzvpCvpSUrOTLUoK+1QSmWc3HN8Du0kVfjfc3jxWpwyC/AcZibEqaxTtkMEAAC4rW4phJWujw27ePFinTlzRkeOHFFSUpLc3d0VGBioKlWqFEYbAQAAUEiqNSqjVVf/kLLkpQ2upOrpc4m66OSgufeUz7bNp0fPS5J+9fPS6VKWadZTZxPUMOma/i5bWst8PCzKaiZfU7+YBKXa2WnSvT7Z6n3n+AW5G4xaVbmsDrq5WJT95+IVtYxP0V43F82rXNairEpqul6Ovv7F/+f3+ijTzs6ifMTpS/JOy9QWHw+Fly1tUdY67qo6XrqqY6WdNNfXy6KsTEam3jp5SZL0TbXySnR0sCgfcCZOVVLSFV7eTVu83CzKmiSk6MkLV3TO2UFzq1o+hw5Go8YduyBJmufnpZgbnsNnYhNU7+o1HfQsrVXels9hnavX1Ds2QUn2dvq8evbn8P1jF1TKaNSyKmX1r6vlc/j4hStqnpCi3e4u+qWS5XNYNTVdL/7vOfxvjQrZ6n3t1CV5p2dqQ8Uy+sejlEVZ6OWrevjyVR1xddbcKp4WZeXTMvT66eth8bQAb111sBz9bEj0ZVVJzdB2b3dt83S1KLs/PlldLibpjIuj5vqXsyhzMRg09vj1sHiOfzmdd7H8M6ZXTLz8ktO0z8tVa8u7W5SFJKXqocq3/GcPAADALbEzGo3GvFeDrYWGhkqSNm7cWMQtAQAAd6pjx47pjdHD9Vivnirn8/+BHj1hr6Mn7P+7k3rCJl2M08IZszXh8ym69957sx0rAADArchvpmeTr4STkpKUlpamcuXK5b0yAAAAbpuEK5kq7Rmo8r4BFstN0Wr2frD/X1ZG9+RYZ4Yk5zy2La+AbGXG/5W7/+9fTts65llv7m1y+9+/nMrs86jX6yb1lv7fv5zK7PKot+xN6nX537/cts3pOTT8759HtpL/39Ypjzbd7Dm82WvjkEe9tnoOPQv4HKakn1BCeg4bAAAA3Eb5DmEzMzN18eJF2dvby8cn+61QkrRu3Tp98cUXOnHihCSpbNmy6tGjh4YOHSpnZ+fCaTEAAAAAAAAAlCD2ea9y3YYNG9S6dWu9/fbbOZYvW7ZMr7zyik6cOCGj0Sij0aj4+HjNnDlTr732WqE1GAAAAAAAAABKknyHsDt37pQkdevWLVtZcnKyPvnkExmNRnl5eemdd97RzJkzNWjQIDk4OGj9+vX6/fffC6/VAAAAAAAAAFBC5Hs4gr1798re3l4tW7bMVrZ+/XrFx8fLwcFB3377rerWrStJatmypdzc3DRp0iStWLFCDz30UOG1HAAAAAAAAABKgHz3hL1w4YL8/Pzk6uqarezvv/+WJDVr1swcwJo888wzcnZ21r59+26xqQAAAAAAAABQ8uQ7hL18+bI8PT1zLNu3b5/s7Oz04IMPZitzd3dX5cqVde7cOasbCQAAAAAAAAAlVb5DWDs7O8XFxWVbnpKSohMnTkiSgoODc9y2bNmySk9Pt7KJAAAAAAAAAFBy5TuErVChgmJjYxUfH2+xfNeuXTIYDLK3t1dQUFCO2yYmJuY4jAEAAAAAAAAA3OnyHcI2bNhQGRkZ+v777y2Wz5s3T5JUr149ubu7Z9suNTVVUVFRqlKlyi02FQAAAAAAAABKHsf8rtizZ08tX75cM2fO1JEjR1SrVi1FREQoPDxcdnZ26t69e47bbd++XZmZmQoJCSm0RgMAAAAAAABASZHvnrANGjTQwIEDZTQatXXrVs2cOVPh4eGSpCZNmqhLly45brd48WLZ2dnpgQceKJQGAwAAAAAAAEBJku+esJI0YsQIBQUFaeHChTp9+rTc3d3VunVr9e/fX/b22fPcS5cuKTY2VkFBQYSwAAAAAAAAAO5KBQphJal9+/Zq3759vtYtX768FixYUOBGAQAAAAAAAMCdIt/DEQAAAAAAAAAACo4QFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGzotoSw586dU0xMzO3YFQAAAAAAAAAUK463YyePP/64EhMTdeDAgduxOwAAAAAAAAAoNm7bcARGo/F27QoAAAAAAAAAig3GhAUAAAAAAAAAG8r3cAQ7d+60eicZGRlWbwsAAAAAAAAAJVm+Q9jnn39ednZ2Vu3EaDRavS0AAAAAAAAAlGQFnpjLycmpwDtJS0sr8DYAAAAAAAAAcCfIdwhboUIFXbhwQfPnz1dQUFCBdnL//fcrISGhwI0DAAAAAAAAgJIu3xNzBQcHS5L2799vs8YAAAAAAAAAwJ2mwCFsZGSkzRoDAAAAAAAAAHeafA9HEBwcLKPRaFUI27BhQyUlJRV4OwAAAAAAAAAo6fIdwrZs2VI7d+6UnZ1dgXcybdq0Am8DAAAAAAAAAHeCfIewdnZ28vDwsGVbAAAAAAAAAOCOk+8xYQEAAAAAAAAABZfvEDYpKUkpKSm2bAsAAAAAAAAA3HHyHcI2adJEAwYMyLFs48aN2rVrV6E1CgAAAAAAAADuFAUajsBoNOa4/KWXXtLnn39eKA0CAAAAAAAAgDtJoY0Jm1tACwAAAAAAAAB3MybmAgAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbcizIyrGxsfrqq68KXGYydOjQguwOAAAAAAAAAEq8AoewX3/9dY5lMTExuZaZEMICAAAAAAAAuNvkO4Rt2rSpLdsBAAAAAAAAAHekfIewc+fOtWU7AAAAAAAAAOCOxMRcAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ/keE7aopaWladasWVq+fLmioqLk6uqqJk2aaMiQIQoKCipwfQaDQQsXLtSyZct09OhRJScny9vbW8HBwerdu7eaNGliXjclJUV//vmnNm/erF27dikmJkZ2dnby8/NTmzZt1LdvX5UrV64wDxcAAAAAAADAHaJEhLBpaWnq16+fwsLCVL58ebVp00YXLlzQ+vXrtWXLFk2bNk0tW7bMd31JSUkaNGiQwsPD5eXlpYYNG8rFxUUxMTHavHmz6tSpYxHCrly5Uu+8844kqVq1amrdurWuXbumf/75RzNnztSyZcs0d+5cVatWrbAPHQAAAAAAAEAJVyJC2JkzZyosLEwhISGaPXu23N3dJV0PR0eOHKnXX39dGzZsMC/Py8iRIxUeHq6+fftqxIgRcnZ2NpfFx8crLi7OYn1HR0d1795dvXr1Uq1atczLr1y5ouHDh2vbtm1688039fPPPxfC0QIAAAAAAAC4kxT7MWEzMjI0Z84cSdKYMWMsgtbOnTurVatWiouL06JFi/JV34YNG7RlyxaFhoZq1KhRFgGsJHl6eiogIMBiWdeuXfXRRx9ZBLCS5OHhoY8//liStHv3bp05c6bAxwcAAAAAAADgzlbsQ9iIiAjFx8fLz89PISEh2co7deokSdq4cWO+6ps/f74k6YUXXiiU9lWsWNE8Huz58+cLpU4AAAAAAAAAd45iPxzBwYMHJSnXybfq1q0rSTp8+HCedWVkZCg8PFwODg5q0KCBjh07pjVr1uj8+fPy8vJSixYt1KxZswK1LyEhQQkJCZIkb2/vAm0LAAAAAAAA4M5X7EPYmJgYSVKlSpVyLDctj4+P19WrV+Xm5pZrXVFRUUpNTZW3t7fmzp2riRMnKjMz01w+ffp0tW7dWpMmTbppPVnNmTNHmZmZqlWrlvz9/fN7WAAAAAAAAADuEsU+hE1OTpYklS5dOsdyV1dX8895hbCmHqvx8fGaMGGCunTpokGDBsnHx0fh4eEaM2aMtmzZovfff1+fffZZnm3bu3evvvnmG0nSqFGj8n1MN5M1FAYAAChMmZmZklGS0Sij0VjUzQFuD6NRMl4///msDQAAikqxD2ELk8FgkHR9WIJmzZpp/Pjx5rI2bdrI29tb3bt314oVKzRs2DBVrVo117rOnj2roUOHKi0tTf369dODDz5YKO27cuXKLdcDAACQk6tXryozM1MZmZnKyMgo6uYAt0XG/8LXq1ev8lkbAAAUOoPBIHv7vKfdKvYhrKmna0pKSo7lpp6ykvIcQiBrr9kePXpkKw8JCVFQUJAiIyMVFhaWawgbFxenvn376ty5c+rSpYtef/31PI8jP+zt7eXh4VEodQEAANzIzc1NDg4OcnRwkKNjsf8YCBQKRwcHOTg4yM3Njc/aAACg0OUngJVKQAhbpUoVSdd7nubEtNzT0zPPENbX19f8s5+fX47r+Pn5KTIyUhcvXsyxPCkpSf369dOxY8fUrl07ffzxx7Kzs8vzOPLLwcGh0OoCAADIysHBQbKTZGdXqJ9fgGLNzk6yu37+81kbAAAUlfxFtUWoTp06kqT9+/fnWH7gwAFJUmBgYJ51eXh4mHu3msaHvVF8fLwky16zJikpKRo0aJD279+vFi1a6PPPP+eDHAAAAAAAAICbKvYhbKNGjeTp6ano6Gjt27cvW/nq1aslSaGhofmqz7Te9u3bs5UlJiaaQ92goCCLsrS0NA0dOlTh4eFq3Lixvv76azk7OxfoWAAAAAAAAADcfYp9COvo6KhevXpJksaOHaukpCRz2cqVK7V161Z5eXmpW7du5uV79+5Vx44d1bFjx2z19e7dW6VKldK8efMsgti0tDSNHTtWiYmJql27tho1amQuy8zM1GuvvaZt27YpODhY33zzjUqXLm2LwwUAAAAAAABwhyn2Y8JK0oABA7R9+3aFhYWpffv2atq0qS5evKjw8HA5OTlpwoQJcnd3N6+fkpKiEydO5FhX5cqVNW7cOL3xxhvq06eP6tevL29vb+3bt09nz56Vt7e3Jk2aZDFO2o8//qi1a9dKknx8fPTRRx/lWPeTTz6pJk2aFOKRAwAAAAAAACjpSkQI6+zsrO+++07ff/+9li9frk2bNsnV1VWhoaF66aWXsg0dkJfOnTvL399fM2bMUEREhCIjI1WhQgU9++yzGjRokCpWrGixfmJiovnnzZs351pvs2bNCGEBAAAAAAAAWCgRIax0PYgdPHiwBg8enOe69913nw4fPnzTderXr6+pU6fma9/Dhg3TsGHD8rUuAAAAAAAAAGRV7MeEBQAAAAAAAICSjBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsiBAWAAAAAAAAAGyIEBYAAAAAAAAAbIgQFgAAAAAAAABsyLGoGwAAAIDb63zsmaJuwl0pMT5eZTw9i7oZdx3OdwAAUBwQwgIAANwlypQpI7fSpbT0++lF3ZS7Tkpysv76faseeKiVSru6FnVz7jpupUupTJkyRd0MAABwFyOEBQAAuEv4+Pjo2xkzlJiYWNRNuetERkaqy+9b9carIxQcHFzUzbnrlClTRj4+PkXdDAAAcBcjhAUAALiL+Pj4EEYVgYSEBEmSv7+/7r333iJuDQAAAG43JuYCAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAG3Is6gbkV1pammbNmqXly5crKipKrq6uatKkiYYMGaKgoKAC12cwGLRw4UItW7ZMR48eVXJysry9vRUcHKzevXurSZMm2bZJSkrS9OnTtXbtWp09e1Zly5ZV8+bN9fLLL8vf378wDhMAAAAAAADAHaZEhLBpaWnq16+fwsLCVL58ebVp00YXLlzQ+vXrtWXLFk2bNk0tW7bMd31JSUkaNGiQwsPD5eXlpYYNG8rFxUUxMTHavHmz6tSpky2ETUxMVM+ePXX06FH5+voqNDRUp0+f1vLly7Vp0yb9+OOPqlOnTmEfOgAAAAAAAIASrkSEsDNnzlRYWJhCQkI0e/Zsubu7S5JWrlypkSNH6vXXX9eGDRvMy/MycuRIhYeHq2/fvhoxYoScnZ3NZfHx8YqLi8u2zaeffqqjR4+qTZs2mjx5snmbGTNmaNKkSXrttde0fPlyOTg4FMIRAwAAAAAAALhTFPsxYTMyMjRnzhxJ0pgxYyyC1s6dO6tVq1aKi4vTokWL8lXfhg0btGXLFoWGhmrUqFEWAawkeXp6KiAgwGLZpUuXtHTpUjk6OuqDDz6w2GbgwIGqVauWjh49qs2bN1t7mAAAAAAAAADuUMU+hI2IiFB8fLz8/PwUEhKSrbxTp06SpI0bN+arvvnz50uSXnjhhXy34ffff1dmZqYaN26sChUqWJTZ2dmpQ4cOBWoDAAAAAAAAgLtHsR+O4ODBg5KU6+RbdevWlSQdPnw4z7oyMjIUHh4uBwcHNWjQQMeOHdOaNWt0/vx5eXl5qUWLFmrWrFmB22Banp82AAAAAAAAALi7FPsQNiYmRpJUqVKlHMtNy+Pj43X16lW5ubnlWldUVJRSU1Pl7e2tuXPnauLEicrMzDSXT58+Xa1bt9akSZMs6slvG86cOVOAIwMAAAAAAABwNyj2IWxycrIkqXTp0jmWu7q6mn/OK4RNSEiQdD2wnTBhgrp06aJBgwbJx8dH4eHhGjNmjLZs2aL3339fn332WbY2ZN1XTm24evVqAY4sZ1lDYQAAANwZTJ/xMjMz+bwHAABwFyr2IWxhMhgMkq4PS9CsWTONHz/eXNamTRt5e3ure/fuWrFihYYNG6aqVave9vZduXLltu4TAAAAtmf6Uj85OZnPewAAAHcQg8Ege/u8p90q9iGsqZdpSkpKjuWmD7SSbtoLNmtdktSjR49s5SEhIQoKClJkZKTCwsLMIaxpu6z7yqkNee0/L/b29vLw8LilOgAAAFD8mD5Purq68nkPAADgDpKfAFYqASFslSpVJElnz57Nsdy03NPTM88Q1NfX1/yzn59fjuv4+fkpMjJSFy9eLHAbstZvLQcHh1uuAwAAAMWL6TOeg4MDn/cAAADuQvmLaotQnTp1JEn79+/PsfzAgQOSpMDAwDzr8vDwMPduNY0Pe6P4+HhJlr1m82qDaXl+2gAAAAAAAADg7lLsQ9hGjRrJ09NT0dHR2rdvX7by1atXS5JCQ0PzVZ9pve3bt2crS0xMNIe6QUFB5uUPPfSQHBwctGvXLp0/f95iG6PRqLVr1xaoDQAAAAAAAADuHsU+hHV0dFSvXr0kSWPHjlVSUpK5bOXKldq6dau8vLzUrVs38/K9e/eqY8eO6tixY7b6evfurVKlSmnevHkWQWxaWprGjh2rxMRE1a5dW40aNTKXlS9fXl26dFFGRobee+89paWlmctmzpypI0eO6N5771WbNm0K9dgBAAAAAAAAlHzFfkxYSRowYIC2b9+usLAwtW/fXk2bNtXFixcVHh4uJycnTZgwQe7u7ub1U1JSdOLEiRzrqly5ssaNG6c33nhDffr0Uf369eXt7a19+/bp7Nmz8vb21qRJk2RnZ2ex3Ztvvqk9e/Zo8+bN6tixo+rXr69Tp05p//79cnNz08SJExnfCwAAAAAAAEA2xb4nrCQ5Ozvru+++04gRI+Tp6alNmzbp6NGjCg0N1S+//KKHHnqoQPV17txZ8+fPV5s2bXTy5Elt2bJFDg4OevbZZ7V48WLde++92bYpU6aMfvnlF/Xv31/29vZav369zp07p0cffVRLly41jxsLAAAAAAAAAFnZGY1GY1E3Av8/nuzGjRuLuCUAAAAobBEREWrcuLF27dplMewVAAAASrb8ZnoloicsAAAAAAAAAJRUhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDhLAAAAAAAAAAYEOEsAAAAAAAAABgQ4SwAAAAAAAAAGBDjkXdAAAAACA/jh8/rvj4+KJuhlUOHjxo8X9J4+npqerVqxd1MwAAAEosQlgAAAAUexcvXlTNmjVlMBiKuim35LnnnivqJljFwcFBZ8+elbe3d1E3BQAAoEQihAUAAECx5+3trX///bfE9oSVpLi4OHl5eRV1M6zi6elJAAsAAHALCGEBAABQInA7PAAAAEoqJuYCAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAGyKEBQAAAAAAAAAbIoQFAAAAAAAAABsihAUAAAAAAAAAG3Is6gbguvPnzyszM1OhoaFF3RQAAAAAAAAA+RAbGysHB4c816MnbDHh4uIiR0cycQAAAAAAAKCkcHR0lIuLS57r2RmNRuNtaA8AAAAAAAAA3JXoCQsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANkQICwAAAAAAAAA2RAgLAAAAAAAAADZECAsAAAAAAAAANuRY1A0AAAAAbmbnzp167rnnzI+feuopffDBBxbrbN++Xb179zY/rlGjhlatWmWxTkpKipo0aaKMjAxJUkhIiBYuXGgu37Ztm3799Vft3btXly5dkr29vcqUKSMvLy/VrFlTderUUadOnVSlShVbHKZ2796tHTt2aO/evdq3b5/Onz9vLmvWrJnmzp170+1TU1M1d+5crVu3TidPntS1a9dUoUIFNW/eXH369FH16tVz3TYhIUGzZs3Spk2bFB0dLYPBoMqVK6tly5bq16+fKlasWGjHCQAAcDcihAUAAECxFhISIicnJ6Wnp0uSdu3alW2diIgIi8fHjh1TQkKCypYta172zz//mANYSWrUqJH5548++ijHkDM1NVXnz5/X4cOHtXLlSvn4+Ojxxx+/5WPKyaRJkxQWFmbVtmfOnFHfvn118uRJi+VRUVGKiorSkiVL9PHHH+uxxx7Ltu3BgwfVr18/Xbp0yWL58ePHdfz4cS1cuFBff/21mjdvblXbAAAAwHAEAAAAKOZKlSqlunXrmh+bAtasbgxhjUaj/vnnn5uuYwphf/vtN4sA1snJSQ0bNlTbtm113333qXz58oVxGAXi6emZ73XT09M1ePBgiwC2bt26evDBB+Xi4mJeZ9SoUdqzZ4/FtgkJCRowYIBFANuwYUPdf//9cnBwkCRdvXpVL774oqKjo60/IAAAgLscPWEBAABQ7DVq1MgcIBqNRkVERKhNmzaSJIPBYA5c7ezsZDQaJV0PXVu1amWu48YQtmHDhpKkpUuXmpe5u7tryZIlqlq1qsW6hw8f1urVq+Xl5VWox5XVY489pp49eyo4OFhVq1ZVYGBgvrZbuHChjhw5Yn78/PPP65133pEk7d27V08//bQyMzNlMBj0ySef6OeffzavO3PmTF24cMH8+M0331SfPn0kSevXr9fQoUMlScnJyfr88881ceLEWz5OAACAuxE9YQEAAFDsNW7c2OJx1kD133//1ZUrVyRJrVu3lp2dXbZ1sga1kuTr62se5zRrD1J/f/9sAawkBQYGasSIEXrooYdu+Vhy0717d3Xq1CnH/d9M1hBZkgYMGGD+uV69err//vvNj3fv3q0TJ07kuK2rq6ueffZZ8+OHH35Y1apVMz9et26dkpKSCtQ2AAAAXEdPWAAAABR7WcdvlSzHhc0atrZt21anT5/WsWPHtHfvXqWnp8vJyUmHDx+2CBCz1ufk5GT++eDBg/r444/1+OOPq3bt2uZb8m9m8uTJOnr0aIGO57777rMIPK117do1RUZGmh/7+Phkm0QrODhYf/75p/lxRESEAgICdPr0aYtesIGBgXJ2ds62rSmkTktLU2RkpEWoCwAAgPwhhAUAAECxV758ed1zzz06deqUJCkyMlJpaWlydna2CGEbNmyoffv26dixY0pNTdXBgwdVr169XMeDNf2c9Xb+H374QT/88INKly6tOnXqqEmTJmrfvr1CQkJybNvOnTsLPKGWq6trgdbPTXR0tMVkYxUqVMi2jo+Pj8VjU0/YrD1iC7ItISwAAEDBMRwBAAAASoSswWnWHqC7d++WJJUtW1Y1atSwWM8UvmbtOXtjXQMHDsxxrNeUlBRFRETom2++0ZNPPql+/frp4sWLhXdAhSAxMdHicenSpbOtU6pUqRy3MQ3hkNt6OS27cRsAAADkDz1hAQAAUCI0btxYS5YsMT+OiIiQv7+/oqKiJEkNGjSQnZ2dxfixEREReuGFFyx6wrq7u6tWrVrmx76+vvrll180btw4/f777+aJvW60bds2DR061GJiK0maO3duoRxfYcit7dZueyv1AQAA4P8RwgIAAKBEyGlcWH9//2zlVatWlY+Pjy5cuKCIiAjFxMQoNjbWvF6DBg1kb295Q9g999yjb775RmfOnNGff/6piIgI7dy5U9HR0Rbr7d69W/v27ct1aILbzcPDw+JxampqtnVuXFamTJl8b3vt2rWb7g8AAAD5QwgLAACAEqF69ery9PRUfHy8pOuBaE4hrOnntWvX6sKFC1q2bJlFPQ0bNsx1H76+vurRo4d69OghSdqzZ49efvllnT171rzOiRMnLELYopyYy9/fX46OjuZxYc+fP59tnRuXBQQEWPyf23o32xYAAAAFQwgLAACAEsHOzk4NGzbU5s2bJUlxcXFatWqVJMnJyUn16tUzr2sKYSXpxx9/tKjnxh6158+fz3FSKkmqX7++HnnkEc2aNcu8zNHR8iN0UU7M5eLioqCgIO3Zs0eSdOHCBZ07d04VK1Y0r7Nv3z6LbXLqMSxJR44cMU92ZmIad1eSnJ2dFRwcXCjtBgAAuNswMRcAAABKjBsDVNNEWXXq1LGYRCrrelkn03JwcFD9+vUt6hg1apQGDhyojRs3Ki0tzaLsypUr+uuvvyyWVa9e/dYOopB16dLF4vE333xj/nnPnj0WAXHDhg0terM+/vjj5p+Tk5MtAuvffvtNp06dMj9u37693N3dC7PpAAAAdw07I6PtAwAAoIQIDw/P8Tb+F154QaNHjzY/zsjIUJMmTZSSkmKxXt26dS0m9zJt+/fff0u63rM0MDBQ5cuXV2pqqvbu3aurV6+a1w0KCtLixYsL85DMvv76a23dutX82NS7VZLc3NxUo0YN8+MxY8YoKChIkpSenq4nnnhCR44cMZfXrVtX5cqVU1hYmDlYtre3188//2wRQickJOg///mPuTesdD2odXFx0c6dO5WZmSnpes/dFStWyM/Pr5CPGgAA4O7AcAQAAAAoMUJCQuTk5KT09HSL5Tf2kHV0dFS9evW0Y8eOm64nXR/mwOTatWvau3dvjvuuUKGCPvvsM2ubnqeoqCiL4DWrq1evWpQlJSWZf3ZyctL06dPVp08fc8/VAwcOWGzv5OSkcePGZesFXLZsWX3zzTfq37+/Ll26JOn6WLtZubq6aurUqQSwAAAAt4AQFgAAACWGi4uLgoODswWFOU221bhx42whbE7rTZ48Wb///rvCwsJ08OBBnTlzRomJiTIYDHJ3d1f16tXVqlUrPfPMMypTpkzhHlAh8fX11fLlyzVnzhytXbtWJ0+eVFpamipUqKDmzZurb9++uQ6jULduXa1evVqzZs3Spk2bFB0dLYPBoMqVK6tly5bq37+/xRizAAAAKDiGIwAAAAAAAAAAG2JiLgAAAAAAAACwIUJYAAAAAAAAALAhQlgAAAAAAAAAsCFCWAAAAAAAAACwIUJYAAAAAAAAALAhQlgAAAAAAAAAsCHHom4AAAAAsouNjdWcOXP0559/KioqSmlpafLw8FDZsmXl5+enunXrqnnz5nrggQc0ZcoUffXVVwXeh6+vrzZt2iRJCgwMzHEdR0dHubu765577lGLFi30/PPPq1y5crd0bDdz5MgRbd++XTt37tTJkyd16dIlJSQkyM3NTdWrV1e7du3Us2dPubm5Zdt28eLFGj16dJ776Nq1qz799FNbNP+2ePPNN7VkyRLz440bN8rPz68IW1Qy3Pi8STk/d3fLeQQAAG4vQlgAAIBiZvv27XrxxRd19epVi+VxcXGKi4vTyZMntW3bNu3bt08PPPCATduSkZGh+Ph4xcfHa8+ePZo3b57mzp2rWrVq2WR/r776qv79999syxMSErR7927t3r1b8+fP16xZs1S1alWbtAF3nq1bt2YLYAEAAG4nQlgAAIBiJDk5WSNHjrQIYKtVq6Z77rlHBoNBZ86c0cmTJ2UwGMzl9957rzp06GBRz5kzZxQZGWl+7Ovrq+DgYIt1btajtUOHDjIajbpw4YL27Nlj3l98fLzGjRunH3744ZaOMz9q1KghPz8/xcbG6vDhw+bl0dHRGj58uBYvXpzrtqVLl9ZDDz2UY1lISEihtxXFV1JSksaMGWPVtpxHAACgsBDCAgAAFCN//PGHLl68aH78xhtvqF+/fhbrxMXFafPmzTp58uT/tXffQVFe79vAr6V3kKYIRhAUowQiGrAh9oIaFUuiEXuJBY0msSUav0ZjN/begho7drHGgg1EFKygCCpILyq97fsHL8+Ph13KIgSj12fGmT39PLubGXN79j4AADc3N7i5uYn6FP9JtZOTk0I/nV69erXw+sqVKxgzZoxQ9vf3R2ZmJjQ0NMo9X3mpqKhg0KBBGDZsGOrWrSvUHz9+HD///LNQfvjwIR49eoRGjRrJncfQ0FD0DPTpWrJkCaKjowEU/GNEVFRUucfye0RERESVhUFYIiIiog/IixcvROVWrVrJ9KlRowbc3d3/rS3B1dUVurq6ePfuHQAgPz8f7969q5Ig7ObNm2FqaipT//XXX2Pfvn24c+eOUBcREVFiEFZRRXPiOjk5YfPmzdi2bRtOnDiBqKgo6OnpoW3btvjpp5+EE8TR0dFo3769cEq4X79+WLBggWjevLw8uLi4IDExEQDQqFEj4Wfxhw8fRkBAAEJCQpCQkIA3b94gLy8PBgYGsLW1Rbdu3dCnTx8oKyuX+zmK5wf28vKCs7NzudufPHmCnTt3IjAwELGxscjLy4O+vj5MTEzQuHFjfPnll+jfv7/MuiEhIfj777/h7++PmJgY5ObmwtTUFE5OThg6dCgaNmxY5nu+fv16rFu3DufPn0dsbCzatGmD9evXAwAuXLiAQ4cO4dGjR0hKSoKSkhJq1KiB2rVr44svvkCbNm3QunVrmTVu3ryJ/fv3Ayj4DikrKzMtAREREVULBmGJiIiIPiCqqqqi8uzZszFu3Dg4OTlBR0enmnYFSKVS4bWKiopMKoOnT59izZo1Cs87d+5c0VzyArCFjI2NReXS3o+0tDQsXrwYcXFxUFNTQ506ddC6dWvY29uXuafk5GQMGDAAoaGhQl1iYiIOHz6M+/fv4/Dhw1BTU4OZmRlatmyJa9euAQDOnTuH3377DWpqasK4GzduCAFYAOjbt6/weuPGjXj58qXM+vHx8YiPj8e1a9dw/PhxbN26VTRnVbl58yZGjx6NnJwcUX1CQgISEhLw+PFjHDlyRCYIu3nzZqxcuRJ5eXmi+sjISERGRuL48eOYNWsWvvvuuxLXfvv2Lb799ls8e/ZMpm3Dhg1YuXKlTH1MTAxiYmIQGBiI0NBQmSBseno6fv31VwAF351ffvlF4Yu03ud7RERERFQUg7BEREREHxBHR0dROTg4GOPGjYNEIoGlpSWaNGkCFxcXdOjQAerq6v/Knv755x+kpqYK5Xbt2smczkxKSsLZs2cVnnvatGml5qYtlJGRAX9/f6GsoaEBBweHEvunpKRg+/btorpVq1bBxcUFS5YsKXXNwovBLC0tYWpqirt37wqBydDQUJw6dQp9+vQBUBBULQzCvn37FlevXkXHjh2FuU6ePCm8VlNTQ8+ePUVraWlpwcrKCvr6+tDU1MS7d+/w+PFj4dSxn58fdu/ejREjRpT85lSSzZs3C8+prKwMe3t7GBgYIDExEdHR0YiPj5cZc+TIESxfvlwoa2ho4Msvv4SKigru3buH1NRU5Obm4vfff0edOnVKzK/65MkTAAWnvBs1aoS0tDSoqKggOzsbmzdvFvppaWnhyy+/hJqaGmJiYhAZGSn6bha1YsUKREZGAigI9hsYGCj8nrzP94iIiIioKAZhiYiIiD4gDg4O6NKli0xAUyqVIjw8HOHh4fD29oaJiQkWLFgAV1fXKtnHpEmTRBdzFapduzamT59eJWuWZt68eUhOThbKgwYNgr6+vsLz+Pr6YuzYsdi3b1+pP/P38PAQTlEWz0fr5+cnBGE7duwIAwMDpKSkACgIuhYGYbOysnDhwgVhXKdOnUR7Xrt2LaytraGiIv4reVpaGnr16oVXr14BAHx8fP6VIOzr16+F156enhg3bpyoPSwsDNevXxfKeXl5WLFihVC2sLDA3r17hdPMSUlJcHd3R3R0NKRSKf78888Sg7AA0LZtW/z555/Q0tICAGRnZyM5ORnp6elCn61bt6Jp06aiPQQFBYn2DgABAQHYvXs3gIKcyZ06dSr3+1Ae5f0eERERERViEJaIiIjoA7N8+XJYW1vDy8urxFN+8fHxmDBhAry9vdGgQYNK34O8U62dO3fG77//LvdEobOzM0JCQip9H/n5+fjf//4Hb29v0VpTp06V6aurq4tvvvkGrq6usLGxQa1atRAfH48jR45g/fr1Qu7W4OBg+Pj4oEePHnLX1NTUxA8//CCUiwcOi54IVVNTQ48ePYSA36VLl5CWlgZtbW1cunRJ9PkVTUUAFAS0t2/fjitXriA8PBxv376VSQUAAM+fPy/p7alUZmZmwmVvx48fh66uLurVqwdLS0uYmZnB2toa1tbWQv+HDx8iLi5OKCsrK2P+/PmiOYumsXj06BHi4uLkppxQUVHB3LlzhQAsUPDe1qhRA5qamsjIyABQkJrAzc0NdevWhZWVFQwNDeHo6Cg6QZ6VlYVffvkFUqkUhoaGmD17tkLvQ2V9j4iIiIiKYhCWiIiI6AOjqqqKyZMnY8yYMbh16xZu376NwMBABAcHi/Ju5uTkYN++fZgzZ86/sq9z586hXr16mDJlyr+yXnZ2Nn766SdRQNjJyQkbNmyQyZ0LFJw0LX7i0cLCAp6ennj37h3++usvof7atWslBs/q1Kkjyjerq6srs6+i+vbtKwRhMzMzcf78efTu3VuUiqB27dpo0aKFUI6NjcXAgQMRFRVV4vMXKikQX9nGjh2LgIAA5OTk4Pnz5/j999+FNgMDA7Ro0QIjRowQ8qEW/tS/0IsXL2QulisuKipKbhDW3NwcZmZmMvVqamoYM2YMVq1aBaDgBKqvr69oXPv27TF69GjUrFkTAPDXX38JweQ5c+YonDKgsr5HREREREUxCEtERET0gdLU1ES7du3Qrl07AAUXRs2fP18U3AsPD6+StUNCQpCeno5Lly5h1qxZyMzMBFBwmdTnn3+Orl27ivpX1sVchVJTUzFx4kTcvHlTqGvXrh1WrVpVoVy4zZs3FwXP5OU3LVT8pG9ZPzdv1KgRPv/8czx+/BgAcOrUKXTo0AFXrlwR+ri7u0NJSUkob9iwQRSA1dTUFHKwAoC/v78o/UJFFZ7aLJSQkFBi3xYtWuDo0aPYs2cP/Pz88OLFC+Tm5gIoyI3q4+ODixcv4sCBA/j8888rtJ/CE63FmZiYlDhm/PjxsLOzw5EjRxAYGIiYmBihLSoqCrt27cLVq1fh7e0NHR0d0Wc7d+5czJ07VygXTW0AFHwuEokEc+bMQffu3cvcvyLfIyIiIqKiGIQlIiIi+oAkJSVBT09PJk8oUHBp0ZgxY0RB2KrMR6mlpYXu3bvj7du3okDWkiVL0L59e6ipqYn2XVkXcyUlJWHUqFF4+PChUOfu7o758+eX+ry5ubly3zcAMidOtbW1Fd5rafr27Sv8FP/GjRvYv3+/cGJWIpHA3d1d1P/OnTvCazU1NZw5cwa1atUS6rp27VqhIGzxE8Jv3rwRle/du1fqeBsbG/z2228ACk5av379Gnfv3sW8efOQlpaG7Oxs7Nu3D//73/9gYWEhGjtgwADR6VlFFA1Qy9OmTRshLURGRgZevHiB8+fPY+3atQAKTuFevXoVbm5uonGFuXpLUvj+ZGVlCXXV+T0iIiKij1fpf9shIiIion+Vr68vOnfujB07dojybRY6d+6cqFw0R2dVGTBgACwtLYVyVFQUjhw5UiVrRUVFYeDAgaIA7IgRI7Bw4cIyA87dunXDgQMHkJaWJqp/8uQJNmzYIKormkO0MvTs2VMISufm5go/nwcKTk+am5uL+hdNK6GkpCQKaO/bt6/CJ5yNjY1F5SNHjghrbdu2DU+ePClx7OHDh3H9+nUhL62qqirq1q0LNzc3GBkZCf0KA5GNGzcWrXf8+HHRyeVC0dHR2LFjB9atW1ehZ9q4cSMePXoklDU1NdGwYUN8/fXXon7F0yNUVHV+j4iIiOjjxZOwRERERB+YqKgoLFq0CIsWLYKlpSUsLCygoqKC58+f4+XLl0I/iUSC3r17V/l+lJWVMXbsWMycOVOo27x5M/r27SucGKysi7nGjRsn5PMECvKxRkVFYdKkSTJ9u3btKjr5+PLlS8yePRvz5s3D559/DiMjI8TFxeHx48ein+WbmJigX79+773XogwMDNChQwf4+PgAEOeNlbeWvb09wsLCABTkkXVzc4ODgwNev36N0NBQSCQS0aVW5eXs7Cwae/nyZbRo0QISiaTMU6Hnzp3D5cuXoa2tDRsbGxgaGkIqleLx48eIjY0V+hUG5JWVlTF58mTh4qvMzEwMGzYMDRo0gLm5OXJychARESEER/v06aPw8wDApk2b8Oeff8LY2Bj16tWDrq4uMjIyZE71WllZAQB++eUX/PLLL3LnmjFjhugfEC5evChzorc6v0dERET08WIQloiIiOgDIpFIROWIiAhRULKQkpISpk+fXuHcnIr6+uuvsW7dOiGgFhkZiePHj8v8zP59Fb+E6t27dyWmOahfv77c+pycHAQHB8ttq127NjZs2CC6eKuy9O3bVwjCFtLX15e55AkoCDZfvHgRb9++BVCQ7/fy5csAAFdXV6SmpopSFpRXnTp14O7ujsOHDwt1hT+5NzQ0RLNmzWROUxeXlpaGoKAguW01a9bEqFGjhPKAAQMQFxeHdevWCQHK0NBQhIaGyox939QZCQkJJea0bdGiBdq3b/9e8xdXXd8jIiIi+jgxCEtERET0AenRowdq166NGzduICgoCC9evEBiYiIyMzOhoaEBMzMzODo6YuDAgWjcuPG/ti8VFRWMGTMGc+bMEeo2bdqEXr16VWleWkWcPn0a586dQ0BAAJ4/f46EhATk5+dDT08P9evXR/v27dG/f/8qy+PZqlUrmJmZITo6Wqjr3r273IvE6tati/3792PlypW4desWMjIyYGFhgV69emHUqFEYPnx4hfcxb9481K5dG0ePHkVMTAwMDAzQtm1bTJo0Cfv37y8xCDthwgTY2dkhMDAQL1++RHJyMjIzM6GtrY26devCxcUFHh4eMjl8J06ciI4dO2Lfvn0ICAjA69evhXEWFhb44osvRDldFbV06VL4+/sjKCgIMTExSElJQW5uLgwMDFC/fn106dIF/fr1q7TvYXV/j4iIiOjjJJFW5HdORERERERERERERFQuvJiLiIiIiIiIiIiIqAoxCEtERERERERERERUhRiEJSIiIiIiIiIiIqpCDMISERERERERERERVSEGYYmIiIiIiIiIiIiqEIOwRERERERERERERFVIpbo3QERERERUWWxtbUXlkJCQCveLjo6Gl5cXrl+/jlevXiE7Oxu6urrQ19eHhYUFGjVqhBYtWqBly5aV9wAl8Pf3x+HDhxEQEICEhAQoKSnByMgINjY2aN68OYYNG1bq+KSkJLi5uSE5OVmoc3Jywq5du6p450REREQEMAhLRERERCTj1q1bGD9+PNLS0kT1ycnJSE5ORkREBK5du4b79+9XaRA2JycHv/76K44ePSrTlp6ejlevXuHq1atlBmHnzZsnCsASERER0b+LQVgiIiIioiLS09Px448/igKwlpaWqFu3LvLz8xEVFYWIiAjk5+dX+V5mzJiBkydPCmU1NTXY2NjA1NQUcXFxCA8PR3Z2dqlzXLhwAT4+PlW9VSIiIiIqBYOwRERERERF+Pr6IiEhQShPmzYNI0eOFPVJTk7GpUuXEBERUWX7uHDhgigA27JlS/zxxx8wMzMT6rKzs3Ht2rUS53jz5g3mzp0LADA3N0dUVFSV7ZeIiIiISsYgLBERERFRES9evBCVW7VqJdOnRo0acHd3r9J9bN++XXhtbGyMlStXQl9fX9RHTU0N7du3L3GOBQsWID4+HgAwf/58DB8+vGo2S0RERESlYhCWiIiIiKgIVVVVUXn27NkYN24cnJycoKOjU+Z4Pz8/7NmzR+F1V69eLbx+9+4dAgMDhbKrqysiIiJw/vx5REdHQ0tLC40bN0b37t2hq6srd74rV67g2LFjAIABAwb8KxeIEREREZF8DMISERER0Udr0qRJCo9xdHQUlYODgzFu3DhIJBJYWlqiSZMmcHFxQYcOHaCuri4zPioqCmfPnq3wngHg8ePHkEqlQtnX1xeHDx+W6bdixQosXboUrq6uovrU1FTMmTMHAFCrVi1Mnz79vfZDRERERO+HQVgiIiIi+mhVJBjq4OCALl26yIyVSqUIDw9HeHg4vL29YWJiggULFsgEQCtDUlKSqBwXFye335s3b+Dp6YkDBw6gYcOGQv2iRYsQExMDAPj999/LdYKXiIiIiKoOg7BERERERMUsX74c1tbW8PLyQmpqqtw+8fHxmDBhAry9vdGgQQOh3t3d/b3zxWZnZ8vU9ejRAzNnzgQALFy4ULi0KysrC+vXrxfSGdy8eRMHDx4U9tKmTZv32gsRERERvT+JtOjvnIiIiIiI/sNsbW1F5ZCQkPfql5GRgVu3buH27dsIDAxEcHAw8vLyRH2+++474af/leWff/7BuHHjhLKSkhL8/Pygp6cHAHj79i2cnZ2Rn58PADAwMICfnx8AoGvXrggPD4epqSlOnToljAHEz+3k5IRdu3ZV6r6JiIiISD6ehCUiIiIiKoGmpibatWuHdu3aAQCSk5Mxf/584RQqAISHh4vGVMbFXHXq1BG1GRkZiYKpenp6MDQ0REJCAoCCoGyhwrqUlBR06tSpxPUCAwPh7Ows7JmIiIiIqg6DsERERERERSQlJUFPTw8qKrJ/Va5RowbGjBkjCsIqKyuL+lTGxVzW1tbQ19fHmzdvABQEWfPz86GkpAQAyM/Px7t374T+hoaGMnNkZ2fLTWtQKDc3FykpKe+1TyIiIiIqH6Xq3gARERER0YfE19cXnTt3xo4dO+ReiHXu3DlR2drautL3oKSkhJ49ewrlrKwsHD9+XCgfO3YMWVlZQrnwRCsRERERfZiYE5aIiIiIPhqVkRP22LFjmDZtmlC2tLSEhYUFVFRU8Pz5c7x8+VJok0gkOHLkCD7//PPK2L5IQkIC3NzchNOwSkpKcHR0BFCQSqAwH6yqqioOHTqEhg0bljknc8ISERERVQ+mIyAiIiIiKkIikYjKERERiIiIkOmnpKSE6dOnV0kAFgCMjY2xceNGjB07VkhHEBAQIOqjoaGBhQsXlisAS0RERETVh0FYIiIiIqIievTogdq1a+PGjRsICgrCixcvkJiYiMzMTGhoaMDMzAyOjo4YOHAgGjduXKV7cXR0hI+PD7Zt24YrV67g9evXyM/Ph5mZGVq1aoWhQ4eibt26VboHIiIiInp/TEdAREREREREREREVIV4MRcRERERERERERFRFWIQloiIiIiIiIiIiKgKMQhLREREREREREREVIUYhCUiIiIiIiIiIiKqQgzCEhEREREREREREVUhBmGJiIiIiIiIiIiIqhCDsERERERERERERERVSKW6N0BERERE/y22traickhISIX6lXeeqvDq1SvcuHED/v7+CAsLQ0JCAlJSUqCuro66devC1dUVHh4eMDQ0lBm7Zs0arF27VqZeIpFAU1MTxsbG+Pzzz9G9e3d07twZEolE7h7CwsLg5eUFf39/REdHIzc3F3p6etDT04OlpSUaNWoEV1dXODg4VPrzAwXvweXLl3H//n0EBwcjIiICUqlUaC/p88jOzoafnx9u376Nu3fvIi4uDgkJCcjKyoKRkRHs7e3Rr18/uLq6lrh2cHAw9u7di3v37iEmJgZZWVnQ1NREnTp10KJFCwwePBjm5uaV/sxERERE1UUiLfo3LSIiIiKiMnwMQdixY8fi8uXLpfapUaMGNm/eDHt7e1F9SUFYebp164aVK1fK1J88eRIzZsxATk5OqeP79euHBQsWlGstRZX1HCV9Hvfv30e/fv3KnN/d3R0LFiyAkpL4x3e7du3CggULUNr/hmhpaWHjxo1wdnYucx0iIiKi/wKmIyAiIiKiT1qdOnXg6uoKe3t70anV5ORkTJo0CZmZmaWOt7a2RpcuXdC5c2fUr19f1Obj44NLly6J6qKjozFr1ixRALZBgwZo164dWrdujTp16lTCUylGXV0dGhoaFRrXtGlTuLi4wMjISNTm7e2NPXv2iOri4uKwePFiUQDW2toabdu2hampqVCXnp6OOXPmKLwfIiIiog8V0xEQERER0SdHIpGgR48eGDVqFD7//HOh/tatWxg1apQQII2Ojoavry86depU4lzdunWDp6enUJ4yZQpOnz4tlP38/NCuXTuh7OPjg6ysLKG8atUqdO3aVTRnbGwsLly4IOpX2RwdHTF//nzY2dmhfv36GD58OPz9/cs11szMDGPHjkWvXr2gpaUFAMjIyMCPP/6IixcvCv0OHjwIDw8PoXz37l1R8Llnz55YtmwZACAzMxO9evVCREQEACAiIgJJSUlyU0IQERER/dcwCEtEREREn5x58+aJTl4Wat68Obp164bjx48LdeHh4QrN3axZM1EQtnggtTDIWKhVq1Yyc9SsWRPfffedQusqSt665WFlZQUfHx9oamqK6jU1NTFjxgxRELb4s6qqqorKX375pfBaQ0MDDRo0EMZIJBIhwEtERET0X8cgLBERERH9ZyQlJWHu3LkKj/P09BSlCpAXgC1kbGwsKuvq6iq0VkBAgKjcqFEjUVlNTU1UnjJlCkaOHIkmTZqUKyXA6dOncebMGYX2ZGhoWKH3TR4dHZ0S28p675o0aQJtbW2kpaUBAA4cOIDmzZujTp06uHPnDq5duyb0bdu2bYVSJBARERF9iBiEJSIiIqL3MmnSpH9trfT0dJw9e1bhceU9VZqfny8KBAKAk5NTqWN8fHzw9OlTSKVSREREIDQ0VGizt7dHz549Rf0dHR2xa9cuoezr6wtfX18oKyvD2toajo6OaNu2LVxcXKCiIvvX9bCwMIXfA3Nzc4X6V1Txy86Kv3c1atTA4sWL8dNPPyEzMxMhISHo3r27zDwuLi5YuHBhVW6ViIiI6F/FICwRERERvZeKBEU/VOvXrxcFUbt06QJra+tSx4SFhSEsLEymvmbNmli0aJHMac7OnTujSZMmuHv3rqg+Ly8PoaGhCA0Nxb59+2BpaYmlS5fC3t7+PZ7o3xMbG4tFixYJZRUVFYwePVqmX6dOnbBr1y6MHz8e8fHxMu2NGjXCuHHjUKNGjSrdLxEREdG/SSItejUpEREREVEZbG1tKzQuJCSk1HmKt//bNmzYgJUrVwrl+vXrY+/evTI/qV+zZg3Wrl1brjk1NDSwfv16mfyraWlpWL58OQ4dOlTq5Vv6+vo4ffq0zM/8q4KHh4foYi5FPo/IyEiMHDlSlAN2/vz56N+/v0zfnTt3YsmSJcjLywMA2NjYwNzcHCEhIYiJiQFQkA92xowZGDZsWMUehoiIiOgDo1TdGyAiIiKi/7aQkBC5f/4rpFIpFixYIBOA3blzZ7nywU6cOFF45lu3bmHmzJlCW2ZmJmbMmIHs7GzRGG1tbcyZMwfXrl3DypUr4eHhgcaNG0MikYj6vXnzBseOHXu/B6xioaGhGDhwoOhCrV9//VVuAPbmzZtYuHChEIAdP348Tp06hc2bN+P8+fNo3bo1gILPZNmyZXj16tW/9hxEREREVYnpCIiIiIjoP6OyLuYqlJOTg5kzZ+LEiRNCnYODAzZv3gwDAwOF16lRowaGDRuGixcvCqdK4+LicO/ePbm5ZfX09NCtWzd069YNABAdHY2ZM2fi5s2bQp/w8HDRmOq+mKuoO3fuYNy4cXjz5g2AghQE8+fPR58+feT2P378uKg8ZMgQ4bWamhoGDBgg5OTNycnBrVu3UKdOnUrfNxEREdG/jUFYIiIiIvrPqMyLuTIyMjBp0iRcvXpVqGvdujXWrFkDLS2t99qnnp6eqFw092lCQgKMjIxkTr0CgJmZGYYMGSIKwiorK4v6fCgXc12+fBmTJ09GZmYmgILUCytXrkS7du1KHBMXF6fQGsnJye+1RyIiIqIPBdMREBEREdEnJyUlBcOHDxcFYHv06IGNGze+dwA2IiICt27dEtWZmJgIrw8cOICvv/4a+/fvF06QFsrPz8eFCxdEdWVdDFYdjh49igkTJggBWD09PWzbtq3UACxQcFlZUXv27BFeZ2dn48CBA6J2CwuLStoxERERUfXixVxEREREpJDyXqhVVr/i7V26dClxzcWLF0NTU1ORbZZq3Lhx+Oeff4Syqqoq2rVrJ/d0qrOzs+gkbfGLuaytrWFjYwOg4OTmvXv3RDlgTU1NcfHiRaipqQEA1q9fj1WrVgEAlJSUYG1tDTMzMwAF71FsbKwwVlNTE+fOnYOpqWllPLbI5cuXsX79eqH87NkzpKWlCWUHBwfhdf/+/YUcrw8fPkTfvn1R9H8jbG1tYWlpKXeduXPnwtDQEABw48YNDB8+XNRev3591K5dW3QxFwAYGBjgwoUL5crLS0RERPShYzoCIiIiIvoglPYT+/nz51dqEDY1NVVUzsnJwblz5+T2LetkbFhYGMLCwuS2aWtrY9myZUIAFoAo0Jufn4+nT5/i6dOnMmPV1NTwxx9/VEkAFijIrxsUFFRie9E2FxcX4XVqaiqKn+Mo7TK2adOmCUHYli1bYsyYMdi8ebPQLu/5tbS0sGzZMgZgiYiI6KPBICwRERERUSVRUVGBjo4O6tatixYtWmDQoEEyP8EfNWoU7O3tcfPmTdy/fx+vXr1CYmIisrOzoaWlBXNzc+H0bUmnS//LfvzxR7Rt2xaHDh3C3bt3ERMTIzz7Z599hhYtWuC7775D7dq1q3urRERERJWG6QiIiIiIiIiIiIiIqhAv5iIiIiIiIiIiIiKqQgzCEhEREREREREREVUhBmGJiIiIiIiIiIiIqhCDsERERERERERERERViEFYIiIiIiIiIiIioirEICwRERERERERERFRFVKp7g0QEREREQGAra2tqBwSElLmGG9vb8ycOVNum4aGBgwNDWFra4vu3buje/fuUFKq+jMIp06dgre3Nx4/foy3b9/CwMAAX375Jb799lu0bt26wvPm5ubi4MGDOHXqFJ4+fYr09HQYGRmhWbNmGDJkCOzt7Uscm5mZiV27duHcuXOIiIhAVlYWTE1N0aJFCwwfPhz16tWTO+7cuXMIDg7G/fv38eDBA6SmpgptEydOhKenZ4Wfh4iIiOhTIpFKpdLq3gQRERERUWUHYYtzdXXFhg0boKysXKH9lSU7OxuTJk3CpUuXSuzz3XffYfbs2ZBIJArN/ebNG4wePRpBQUFy2yUSCaZOnYoxY8bItEVFRWHEiBGIiIiQO1ZVVRV//PEHvv76a5m24p9JUQzCEhEREZUf0xEQERER0UfD3NwcXbp0Qbt27VCnTh1R25UrV3DixIkqW3v+/PmiAGzt2rXRtm1bGBkZCXV79uzB9u3bFZ576tSpogCslZUVXF1doaOjAwCQSqVYvnw5zpw5IxqXk5OD77//XhSAbdSoEVq3bg11dXWhz/Tp00sM8BbS19dXeN9EREREVIBBWCIiIiL6aDg5OWH16tXYuHEjzp07h+7du4vafX19q2TdsLAwHDhwQCjb29vjzJkz2LRpE3x8fGBubi60rVmzBm/fvi333FevXsW1a9eEcseOHXH69Gls3rwZx44dEwKxALBo0SLk5+cL5UOHDiE0NFQoe3h44MiRI9i2bRt2794tnArOz8/HwoULZdb29PTE5s2bcfPmTaxZs6bceyYiIiIiMQZhiYiIiOijpKSkBDc3N1FdSkpKlax19OhRFM3yNWTIEOGkqb6+PgYMGCC0ZWRkwMfHR6G5ixo5cqSQ29bCwkL0jNHR0bhx40aJY0ePHi28tre3R/PmzYXy3bt3ER4eLuo/ceJEuLq6wtDQsNz7JSIiIiJZvJiLiIiIiD5aRU+FAoCpqalMnz179sDPz0+heW1sbDBp0iShfOfOHVG7nZ1dqeXAwEB888035Vqr6NwSiUTu3EVP4QYGBqJ169bIysrCgwcPhHoTExPUrFlTZuz169dFY62srMq1LyIiIiIqPwZhiYiIiOijlJeXh1OnTonqOnbsKNPv/v37OHv2rEJzJycni8rFL70qHuw1MTERlYufOC1JRkYGYmJihLK+vj7U1NTKNXdkZCRyc3NL3NP77IuIiIiIFMMgLBERERF9NPz9/TFp0iRkZ2fj6dOniIyMFNp69+6N9u3bV8m6xXO8ampqlloub07Yd+/elTpPaXOXtScA0NDQqNC+iIiIiEgxDMISERER0UcjKioKUVFRojpVVVXMmTMH/fv3h0QikRmzaNEiLFq0qFL3UTQ/rLxyZc2ryNyVtQciIiIiUhwv5iIiIiKij1pOTg6WL1+OoKCgKltDV1dXVM7IyBCVMzMzRWU9Pb0KzVt8ntLmfp+xRERERFS5eBKWiIiIiD4affr0waJFixAbG4tVq1bh8OHDAICUlBRMnDgRZ86cgY6OjmhMZVzMZWVlhaSkJKEcFxcnWicuLk40vryXX2lqaqJWrVpCXtg3b94gKysL6urqZc5dp04dqKioCHlhi/d7n30RERERkWIYhCUiIiKij07NmjWxYMEChIWF4d69ewCA+Ph4bN68GVOnThX1rYyLuRwdHXHnzh2h/ODBA9SrV0+0RvH+5eXo6IjTp08DKEgp8ODBAzRt2rTMudXV1dG4cWPhBHB8fDxiY2NRs2bNStkXEREREZUf0xEQERER0UdJIpHg559/FtXt2rVLJoBaGXr37i3KN+vl5SX81D8lJQUHDx4U2jQ1NdGtWzfReFtbW+GPh4eHzNxFbd26Ffn5+QCAV69ewcfHR2gzMzNDixYtShy7efNm4XVQUBD8/f2FcpMmTXgSloiIiKiK8CQsEREREX2Qiv7cv7jFixdDU1OzzDmaNWsGJycnIdiYnp6Ov/76Cz/88IPQpzIu5rKxscGAAQOwf/9+AAUnTLt16wZbW1sEBQWJUhV4enoqlHvV1dUVrVu3xrVr1wAA//zzD9zc3FC3bl0EBAQgLS1N6DtjxgwoKysL5f79+2Pv3r0IDQ0FAOzevRuBgYEwNDSEv78/8vLyAABKSkqYOXOmzNoTJkxAfHw8ACA1NVXUdvDgQfj6+grlAwcOlPuZiIiIiD41DMISERER0QeptBQB8+fPL1cQFgDGjx8vOvG5e/dujBw5Uubiqvf166+/Ii4uDpcuXQIAvH79Gq9fvxb1GTRoEEaMGKHw3CtWrMCoUaMQHBwMAAgPD0d4eLjQLpFIMHXqVHTt2lU0TlVVFRs3bsTw4cPx4sULAMCjR49k+ixYsAAODg4y6z5+/BhRUVFy9xQbG4vY2FiFn4WIiIjoU8QgLBERERF91Fq0aIEmTZrg7t27AIB3797By8sLEyZMqNR11NTUsHHjRpw6dQre3t54+PAhUlNTYWBgAAcHB3z77bdwcXGp0Nz6+vrYu3cvDh48iJMnT+Lp06dIT0+HkZERmjVrhqFDh8Le3l7uWHNzcxw/fhxeXl44e/YsIiIikJ2dDVNTU7Ro0QIjRowQ5a8lIiIiosonkUql0ureBBEREREREREREdHHihdzEREREREREREREVUhBmGJiIiIiIiIiIiIqhCDsERERERERERERERViEFYIiIiIiIiIiIioirEICwRERERERERERFRFWIQloiIiIiIiIiIiKgKqVT3BoiIiIiIKsrW1lZUlkgkUFVVhba2NgwNDWFpaQknJyf07t0bBgYGcufw8/PDkCFD5LapqanBwMAA9evXR6dOndC3b1+oqalV9mPI8PX1xf79+3Hv3j2kpKRAT08PjRo1gru7O9zc3Co05549e3D37l2EhIQgKSkJb968gbKyMgwNDWFra4suXbqgZ8+eUFHh/yIQERERVTaJVCqVVvcmiIiIiIgqongQtiQaGhqYMGECRo8eDYlEImorLQhbnJ2dHby8vKCtra3wXstDKpVi7ty52LdvX4l9OnbsiD///FPhYHCTJk2Qnp5eap9mzZph+/btUFdXV2huIiIiIiod0xEQERER0UejS5cu6NixI5o2bQpNTU2hPjMzE8uXL8fMmTPLnKNGjRro0qULOnToABsbG1HbgwcPsHPnzsretmDLli2iAKyRkRHatm2L2rVrC3UXLlzAokWLKjS/iooKrK2t0bp1a7Rq1QqGhoai9oCAAOzatatimyciIiKiEvEkLBERERH9ZxU/CRsSEiK8zsjIwLZt27Bu3Trk5+cL9XPmzMF3330nlIufhHVychIFIpcuXYqtW7cK5SZNmpR6UrWi3rx5gzZt2iAzMxMAYG5uDm9vbxgYGCArKwuDBw9GcHAwgIK0C6dPn0a9evXKPf/p06fRqlUr6OvrC3XZ2dmYNm0afHx8hLp27dph48aNlfRURERERATwJCwRERERfaQ0NTUxceJE/PTTT6L69evXIysrq9zz9OjRQ1ROSUmpjO3J8PHxEQKwADBgwAAhj626urooUCyVSnH06FGF5ndzcxMFYIGCnLfFn+/fyHlLRERE9Klh1n0iIiIi+qgNGTIEO3bsQHx8PAAgISEBfn5+aNOmTbnGFz1FCwCmpqYyfU6fPo0zZ84otC9DQ0PMnTtXKN+5c0fUbmdnV2o5MDBQofXkycnJwcmTJ0V1Li4u7z0vEREREYkxCEtEREREHzVVVVU4OzuLgo1BQUHlDsKeOHFCVO7YsaNMn7CwMJw9e1ahfZmbm4vKERERonLxYK+JiYmoHB4ertB6hX7++WdkZWXh7du3ePLkCZKTk4W2AQMGwN3dvULzEhEREVHJGIQlIiIioo+emZmZqJyYmFhi36dPn2LSpEnIzc3Fixcv8OzZM6GtVatW+Oabb6pkj2/fvhWVi14sJq9cvH95XbhwAenp6TL1o0aNwsSJE6GsrFyheYmIiIioZAzCEhEREdFHr/hdtBKJpMS+ycnJMqdaJRIJJk2ahNGjR0NVVVVmjKenJzw9PStns/9f8T1X9X26W7duxeXLl7F161aZoDURERERvR9ezEVEREREH72YmBhR2cjISKHxUqkUmzZtwtWrVytzWyK6urqickZGhqhc9NIuANDT06vQOnfv3sWTJ09w8+ZNbNu2Dfb29kLbs2fPMH/+/ArNS0REREQl40lYIiIiIvqo5eTkwM/PT1Tn4OBQYn8nJyfs2rULSUlJ+Ouvv7Bx40YABUHQn376CSdOnICFhYVoTGVczGVlZYX79+8L5bi4ONja2orKRVlZWSm0XlESiQSGhoZo3bo17Ozs0LJlS+Tl5QEALl++jNzcXKio8H8ViIiIiCoL/2ZFRERERB81Ly8vxMfHC2UjIyN89dVXZY4zNDTElClT8PLlS5w+fRoAkJ6ejj///BPLly8X9a2Mi7kcHR1x/PhxofzgwQO4uLgI5aIB2sL+lUFPTw8aGhpIS0sDAOTm5iIlJQXGxsaVMj8RERERMR0BEREREX2kMjIysG7dOixbtkxUP3HiRGhoaJR7nqlTp4pOhZ4+fRphYWGVts9C3bp1E+3rwIEDSElJAVBwCtfLy0tok0gk6N27t2h8+/btYWtrC1tbW7Rv317UtmfPHpw5cwZZWVmi+tzcXKxfv14IwAKAjo4OatSoUUlPRUREREQAT8ISERER0Udk0qRJyM/PR3JyMh4+fCiTV7Vv374YNGiQQnPWqVMHPXv2xJEjRwAA+fn52LhxI5YuXSr0qYyLuQwMDDBhwgThlO3r16/RrVs3ODg4ICQkBK9fvxb6Dho0CPXq1Sv33Pfv38eRI0egoaGBBg0awNjYGBkZGXj69CkSEhJEffv37w9lZeX3ehYiIiIiEmMQloiIiIg+GiWlBNDU1ISnpydGjhxZoXnHjh2LY8eOIT8/HwBw6tQpeHp64rPPPqvwXuUZNWoUIiMjsX//fgBAUlISLl26JOrToUMHzJgxo0LzZ2ZmIjg4uMT2bt26YcqUKRWam4iIiIhKxiAsEREREX00JBIJVFRUoK2tDUNDQ9StWxfOzs7o06cPDAwMKjyvlZUV3NzccPLkSQBAXl4eNm7ciD/++KOSdl5ASUkJ8+bNQ8eOHbFv3z4EBwcjJSUFOjo6aNy4Mdzd3dG9e3eF5x08eDBq1qyJu3fvIjIyEsnJycjKyoK2tjbMzc1hb2+Pnj17litXLhEREREpTiKVSqXVvQkiIiIiIiIiIiKijxUv5iIiIiIiIiIiIiKqQgzCEhEREREREREREVUhBmGJiIiIiIiIiIiIqhCDsERERERERERERERViEFYIiIiIiIiIiIioirEICwRERERERERERFRFVKp7g0QERERfUpsbW1F5ZCQkDLHxMbG4saNG7h9+zaePHmCxMREJCYmQlVVFRYWFmjZsiU8PDxgYWFRVdsW7WXbtm3w9fVFdHQ0lJSUYGFhgfbt22PEiBHQ09Or8NxhYWHYsWMHbt26hbi4OKirq8PS0hJdunSBh4cH1NXVSxwbFBSEXbt2ISAgAImJidDS0kL9+vXRo0cP9O/fH8rKyiWO9fX1xf79+3Hv3j2kpKRAT08PjRo1gru7O9zc3OSOCQkJwbVr13D//n0EBwcjKipKaDM3N8c///wjd5y3tzdmzpwplBcuXAh3d/ey3hpkZ2fDz88Pt2/fxt27dxEXF4eEhARkZWXByMgI9vb26NevH1xdXcuc62Pk5+eHIUOGCOWJEyfC09OzytZLS0vDzZs3cfv2bQQHByM+Ph6JiYnIy8uDiYkJHB0dMXDgQDg6OlbZHoiIiOi/hUFYIiIiog/cpk2bsGfPHpn6nJwchIaGIjQ0FAcOHMCKFSvQrl27KtvH9evXMXHiRKSnp4vqQ0JCEBISgkOHDmHr1q1o2LChwnMfOXIEs2fPRk5OjlCXlZWF4OBgBAcH49ChQ9i5cydq1aolM3bDhg1YtWoVpFKpUJednY3bt2/j9u3bOHr0KLZs2QJdXV3ROKlUirlz52Lfvn2i+sTERPj6+sLX1xenTp3Cn3/+CTU1NVGfHTt24MiRIwo/Z0WFhIRg1KhRcttiYmIQExODc+fOwd3dHQsWLICSEn/wVpV8fX0xefJkuW2RkZGIjIzEiRMnMG7cuBL7ERER0aeFfzsjIiIi+g8xNTWFi4sLHB0doaqqKtSnp6dj6tSpiIuLq5J1X716hQkTJggBWGVlZTg7O6NJkyZCn/j4eIwePRrv3r1TaO7AwEDMmjVLCMCqqamhdevWaNSokdAnPDwc33//PXJzc0VjT506hZUrVwoBWB0dHbi6uqJevXpCn7t37+Knn36SWXfLli2iAKyRkRHatm2L2rVrC3UXLlzAokWLSt2/tra26LOoaurq6mjatClcXFxgZGQkavP29pYbsKeqo62tDWdnZ7Rs2VJ0ElwqlWL9+vUlnoomIiKiTwuDsERERET/AW3atMGePXvg6+uLrVu3Yu/evfD29oa+vr7QJz09HadPn66S9VesWIGMjAyhvHLlSnh5eWHfvn2in9fHxcVh69atCs29aNEi5OfnAygI7u7Zswfbtm3DkSNH4OHhIfR7/PgxvL29hXJeXh4WL14slHV0dHDs2DFs3rwZp06dQseOHYW2y5cv4/r160L5zZs3WLdunVA2NzfH6dOnsWnTJpw5cwb29vZC299//43nz5+L9uzq6orFixfj9OnTCAgIgKmpqULPXBFmZmaYO3cubt26hb///htbt27FxYsX0aFDB1G/gwcPVvleCLCxscGyZctw69YteHl5YceOHTh//jwcHBxE/fh5EBEREcB0BEREREQfvO+//15ukK9BgwYYNGgQNmzYINSFh4dX+vqpqam4cOGCULa0tETnzp2F8qBBg7Bq1SrhlKy3tzemTJlSrrmfP3+OoKAgody8eXNRAHT06NHYtWuXUPb29saAAQMAADdu3EBsbKzQ5ubmJuTFVVJSwsiRI0X79vb2RqtWrQAAPj4+yMzMFNoGDBgAAwMDAAUnTYcMGSKcnpVKpTh69CimTp0q9O/WrVu5nk8RaWlpGDVqFAIDA4U6T09PTJw4EVZWVvDx8YGmpqZojKamJmbMmIGLFy8KdREREXLnT0xMxJ49e3D16lW8ePECGRkZ0NfXxxdffIEBAwagffv2MmM8PDzg7+8vlJ88eYLDhw8LgWk1NTU4Ozvjhx9+gLW1tdx1Hzx4gL///ht37txBbGyskDfVwcEB/fv3R8uWLeWOk0qluHjxIo4fP4779+8jMTERysrKMDY2hr29Pb799lt89dVXJb6f8fHxWLNmDS5duoTk5GSYmZmhV69e+P7776GiIv7foFevXmHHjh3w8/PD69evkZ2dDT09PRgbG6Nhw4bCXgvzEjs5OeHYsWMy8xgYGGDKlCkYNmyYUFfS50FERESfFgZhiYiIiD5wpZ2yNDExEZWL5z1NSkrC3LlzFV7T09MT9evXBwDcv38f2dnZQpudnZ2or5qaGmxtbXH37l0ABadhX716hTp16pS5zp07d0Tl4nPXrFkTJiYmiI+PB1AQ0MvOzoaamlqZYxs3bgyJRCKkKiga3CxrbPFy0bFVISMjA99//71onZ9++gmjR48GUHDKtyTGxsaicvHvAABcu3YNU6dOxZs3b0T1CQkJuHTpEi5duoSvv/4aixYtKvUSs5kzZ4py4WZkZODcuXO4du0aduzYgS+//FLUf9WqVdiwYYMoXy8AREVFISoqCqdPn5abx/bdu3eYNGkSbty4IbOHly9f4uXLlzA0NCwxCPv06VP06tULiYmJonFr1qxBbGwsfv/9d6H+2bNn+Oabb5CamiqaIykpCUlJSQgNDcXx48fRsWNHISexoaFhie9ReT4PIiIi+vQwCEtERET0H3b58mVR2cnJSVROT0/H2bNnFZ73u+++E14XP10rLyhcPBgcHh5eriBs8VOC8uY2NTUVgrA5OTl49eoVrK2tyxyrrq4OfX19pKSkAABev36NzMxMaGholDlW3vNUlaysLIwfP144cSqRSPDLL7+IUjGUpqzvQFhYGCZOnCikk5BIJPjiiy9gaGiIkJAQREdHAwCOHz+OWrVq4ccffyxxrSNHjqBWrVqwsbHB48ePhSBnYU7iM2fOCJeYHThwAOvXrxfGKikpwc7ODurq6ggKChIC+97e3jA1NRWdni4egJVIJKhfvz7Mzc0RFxeHx48fl/qenD17FkpKSnBwcEBubi4ePnwotB08eBBjx44VTk3v3LlTFIBt3LgxatasieTkZMTExAjvT3mV9XkQERHRp4lBWCIiIqL/qCNHjuDq1atC+YsvvoCLi0ulr1P8oi0NDQ2ZPsXryns51/vMXdGxGhoaePv2rahe3s/8iyrev7Lk5ORg4sSJQsBRSUkJ//vf/4SUC2WJjY0VXRymoqIinJ4ttHbtWiEAq6Kigr/++gvNmjUDAOTm5mLixIm4dOkSgIKA5PDhw0s86eni4oL169dDTU0N7969w5AhQ/Do0SMABadbz5w5g6+//hp5eXlYtWqVaOz69evRrl07AAUnmgcNGoSsrCwAwI4dOzBs2DDUqFEDvr6+ogCsjo4ONmzYIApmvn79GmFhYaW+N6tWrRLSZvz6669CblapVAp/f38hCPv69WthTP/+/TF//nzRPFFRUbh69Sq0tbVLXQ8AQkJCROlBdHV1yx1MJyIioo8bg7BERERE/0FHjx7Fr7/+KpRNTU2xevVqSCQSUT8LCwuEhIRU6trFf1peUt2HMHd5xxbvV1nPU5Y///wTycnJAAouJVu4cCF69epVrrGRkZEYOXKkKC/u3Llz0ahRI6Gcn5+PK1euCGVNTU14eXnBy8tLqCs6Pjs7Gzdu3ECPHj3krjlx4kThpKuuri5GjhwpOjl769YtfP3113j48CESEhKEemdnZyEACxSke+jevbtw0VpWVhZu3bqFbt264Z9//hGtOX78eJnTpLVr10bt2rVLfG+aNm0qylvcpk0b0QVZhSergYILzwpdvXoV27Ztg42NDaysrGBhYQFzc3MMHDiwxLUKPXr0CKNGjUJaWhoAQFVVFcuXL0fNmjXLHEtEREQfPwZhiYiIiP5jtm/fjiVLlgiBwpo1a2Lnzp2lBqXeR/F8pEUvtCpUeKKxUHnzYL7P3MXHFp72LM/Y4vsrPrb4PvT09GTmrgyFAVgAGDx4cLkDsKGhoRg5ciTi4uIA/F8Kg/79+8vMXxgUBApOApeVniIqKqrENhsbm1LLMTExAAoCxEU1aNBAZq7CnMPF1y0+tvDUriKKBqIB2c+7aI7jYcOG4cyZM0hNTUVsbCyWLFkitGlpacHZ2RnfffddqafM/fz8MH78eCGtgaqqKlasWAFXV1eF905EREQfJwZhiYiIiP5Dli5diq1btwplS0tLbNu2TfhpdXGVcTFXvXr1RG2Fgb/S6qysrMq1TvF+Zc2tqqoq5Jota2xWVpboIqratWsL6QmsrKxw//590VhbW9v3fp738ffff8PFxaXMlBJ37tzBuHHjhGdTUVHB/Pnz0adPn0rZR3p6eqXMU5309fVF5aKXfhVXv359nDhxArt378b169cRFhaGnJwcAAXvxaVLl3D58mVs2rRJblD1/PnzmDp1qhDY1dLSwpo1a9C6detKfCIiIiL6r2MQloiIiOg/IC8vD7Nnz8bhw4eFusaNG2PLli0wMjIqcVxlXMxlZ2cHVVVVITD14MEDUd/s7GyEhoYKZVNT03JdygUAjo6OonLxuWNiYkQ/HbezsxN+Dl/W2Pv374vSChTt7+joiOPHj4vGFg1+Fg3Qylursnz77bc4fvw40tPThfywW7duxVdffSW3/+XLlzF58mThpK6GhgZWrlwp+ql/UTVq1ICWlpYQWLW0tKzQ96HQs2fP8OWXXwrl58+fi9pr1aoFADA3NxfVF/1+FHr69KmoXHiSu/g/KAQEBMDBwaHCey6P2rVrY9q0aQAK/luLiYnBw4cPMX/+fMTGxkIqlWLXrl0yQdiDBw/it99+Q15eHgDAwMAAW7Zsgb29fZXul4iIiP57Sv4nYSIiIiL6IGRlZcHT01MUgG3evDm8vLxKDcBWFl1dXXTs2FEov3jxQhTI27Vrl+j0ZPETmWvWrIGtra3wx8/PT2iztrYWBaz8/PwQFBQklLds2SKaq+jcrVq1gqmpqVD28fHBq1evABTkQt22bVuJY7t16ya6tOvAgQNISUkBUJCKoGjOVIlEgt69e6MqODg4YPXq1VBVVRXWHjt2rEwQGCjIAzxhwgQhAKunp4dt27aVGIAFCk6AFg0uR0REYPPmzcjPzxf1S01NxYkTJ/DTTz+Vut+1a9cKJz5TU1NFp7KBgu8lUPAPBEW/m35+fqLctI8ePcKpU6eEspqamjC2+POsX78e/v7+orrY2Fj4+vqWutfyOnv2LC5cuCC8r8rKyjA3N0enTp1E/5hQPE3Dpk2b8OuvvwoBWDMzM/z9998MwBIREZFcPAlLREREVI0mTZpUYtvixYuhqamJpUuX4uLFi0K9RCKBpqYmZs2aJTPGxsZGNGdlXcw1depUXL58WcidOmXKFHz11VfIysrC3bt3hX6mpqYYPXq0QnPPmDEDgwcPRn5+PvLy8jB48GA4OTkhKSkJjx49Evo1bNgQffv2FcrKysqYMWMGpk6dCqAgKNi7d280a9YML168QHh4uNC3bdu2op+HGxgYYMKECVi+fDkA4PXr1+jWrRscHBwQEhKC169fC30HDRokk5Lh4MGDooueiqYviIuLw4ABA4Ty+PHj0bZt2xKf38XFBQsXLsTPP/8MqVSKtLQ0jBo1Crt27RJyqT58+BAzZswQnew1MzOTuWSr0Ny5c2FoaAgAmDBhAi5fvizkx12+fDn+/vtvWFtbQ1lZGdHR0Xj+/Dlyc3NlTrAW5+vri06dOsHGxgZPnjwRXb5lbm6Orl27AihIkTBx4kT873//E9rHjRuHL774Ampqarh3754oL+vQoUOF/bZp0wbNmzfHrVu3ABR8rkOGDEH9+vVhbm6OxMREPHr0CIMGDSozdUN5+Pn5Yc+ePdDQ0IC1tTVMTEwgkUgQFhaGly9fCv0sLS2F1+fPn8eKFStE89SqVQurVq2Su8bq1avfe59ERET038YgLBEREVE1Ku2n4fPnz4empqZw2U8hqVSKS5cuyR1T9KKnyvTZZ59h7dq18PT0RHp6OvLy8oQgWSFjY2Ns2bKl3JdyFWratCn++OMPzJ49Gzk5OcjOzsa1a9dEfSwtLbFx40aoqIj/+tq9e3e8ePECq1evhlQqRWpqKi5fvizq06RJEyxbtkxm3VGjRiEyMhL79+8HUJA/t/j72qFDB8yYMUNmbExMjOjEblE5OTmitqSkpJIf/v/r2bMnkpKS8McffwAAUlJSMGLECOzZswd169ZFamqqKAALACEhISUG2KdNmyYENW1tbbFq1SpMmzYNb9++BQBER0cjOjpaZpyysnKp+xw6dCj++usv4QKuQlpaWli2bJmQKgIoCF7HxMRg8+bNkEqlyMvLw71792Tm7N27N6ZMmSKqW716NTw9PYVT01KpFKGhoXLTGlSWzMxMPHz4UG6brq6u6B833r17J9On6D9GEBERERXHICwRERERlUvr1q3h4+ODbdu24erVq4iJiYGSkhIsLCzQoUMHDB8+XOZCpPLq06cP7O3tsWPHDty8eRNxcXFQV1eHpaUlunTpAg8PD1H6gKLGjx+PVq1awcvLCwEBAUhMTISWlhbq16+PHj16YMCAAXKDi0pKSpg3bx46duyIffv2ITg4GCkpKdDR0UHjxo3h7u6O7t27V+h5KmLo0KFISkrCxo0bAQDx8fEYPnw49uzZ895zt2vXDqdPn8a+fftw7do1hIeHIzU1Ferq6qhZsyYaNmyIli1bonPnzqXOM2vWLDRu3BheXl4ICwuDqqoqnJ2dMXnyZOEit6KmTp2Kjh07Yu/evbhz5w5iY2ORn58PIyMjODg4oH///nIvsNLX18fOnTtx/vx5nDhxAg8ePEBiYiKUlZVhbGwMBwcHdOrU6b3fF6AgWFyzZk0EBgYiPDwcycnJSEtLg4aGBurUqYMWLVpgyJAhQs5aIiIiooqQSIv/kzoREREREREADw8PUT7WykhtQURERPQp4sVcRERERERERERERFWIQVgiIiIiIiIiIiKiKsQgLBEREREREREREVEVYk5YIiIiIiIiIiIioirEk7BEREREREREREREVYhBWCIiIiIiIiIiIqIqpFLdGyAiIiIiAoBz587B09MTANC4cWN4e3tX847kCwsLw44dO3Dr1i3ExcVBXV0dlpaW6NKlCzw8PKCurv5e8/v7++Pw4cMICAhAQkIClJSUYGRkBBsbGzRv3hzDhg0T9T9x4gRu3ryJx48fIz4+HikpKZBIJDAwMED9+vXRvn179OvXDxoaGiWumZ+fj+PHj8PHxwcPHz5ESkoKtLW1YWhoCDs7O3Tt2hUdOnQQ+j958gS9evUCAJiYmOD8+fPQ1NR8r+cmIiIi+pgxJywRERERVbvc3Fx0794dERERAIBVq1aha9eu1bspOY4cOYLZs2cjJydHbruVlRV27tyJWrVqKTx3Tk4Ofv31Vxw9erTEPsrKynj06JGorkePHnj69Gmpc1tbW2P37t0wNDSUaYuNjcW4cePw8OHDEse3aNECO3fuFNV9//33uHTpEgBg8uTJGD9+fKl7ICIiIvqUMR0BEREREVW7I0eOCAHYzz77DJ07d67eDckRGBiIWbNmCQFYNTU1tG7dGo0aNRL6hIeH4/vvv0dubq7C88+YMUMUgFVTU0OjRo3Qtm1bNGrUqNSTphKJBJaWlnB2dkabNm1gZmYmag8LC8OaNWtkxqWlpWHIkCGiAKyenh4cHR3h4uICGxsbKCsry11z1KhRwuutW7fizZs35X1UIiIiok8O0xEQERERUbXbvXu38LpHjx5QUvrwzgosWrQI+fn5AApOpO7Zswf29vYAgPnz52PXrl0AgMePH8Pb2xsDBgwo99wXLlzAyZMnhXLLli3xxx9/iIKp2dnZuHbtmszYH374AQ4ODjAxMRHq8vPzsWTJEuzYsUOoCwgIkBm7cuVKIfgNAMOHD8fkyZNFAd83b97InL4FgKZNm8Lc3BxRUVFIS0vDsWPHMGTIkHI/MxEREdGn5MP72y0RERERfVKCgoLw5MkTodyjRw9Ru7e3N2xtbYU/xXPFltVeGZ4/f46goCCh3Lx5cyEACwCjR4+W2ZMitm/fLrw2NjbGypUrZU6zqqmpoX379jJjO3bsKArAAoCSkhJ69+4tM76o1NRUHDp0SCi3atUKM2bMkDlxq6+vjxYtWsisK5FI0K1bN6F88ODBEp6OiIiIiHgSloiIiIiq1cWLF4XXJiYmsLa2rtT5nz59Kven+GWZO3eukEP1zp07ojY7OztRuWbNmjAxMUF8fDwA4MGDB8jOzpYJfMrz7t07BAYGCmVXV1dERETg/PnziI6OhpaWFho3bozu3btDV1e3XHvPz8/HsWPHRHWtW7cWlf38/JCeni6Ue/TogQsXLuDmzZtISUlBjRo14OTkhA4dOpSYkqB58+bYunUrACA0NBSvXr1CnTp1yrVHIiIiok8Jg7BEREREVK1u374tvHZwcKj0+ZOSknD27FmFx02bNk0Iwhb9yT4AmJqayvQ3NTUVgrA5OTl49epVuQLKjx8/RtG7cn19fXH48GGZfitWrMDSpUvh6uoqd56FCxciOjoaaWlpePr0KWJjY4W29u3bY+zYsTLrFvXnn38iLi5OVLdr1y7Ur18f69atQ926dWXWtLe3h0QiEfbv7+/PICwRERGRHExHQERERETVqmgw0MbGphp3UrJ3796JyhoaGjJ9itcVH1OSpKQkUbl4ILTQmzdv4OnpKUrdUNT169dx9uxZXLt2TRSA7d27NxYtWgQtLa0Krfv06VOMGjUKGRkZMm36+vqiVAjycscSEREREYOwRERERFSN0tPTRcE9AwODSl/D2dkZISEhCv+xsLAocc6iJ1dLqyuP7OxsmboePXrg+vXruH79uihHblZWFtavX6/Q/EePHkXPnj1lgrfy1p01axbu3LmDU6dOwdbWVqh/+fJliXlui35mCQkJCu2NiIiI6FPBICwRERERVZvip0V1dHSqaSelK76vzMxMmT5ZWVmicnnztxafW0lJCb/99huMjY1hbGyM3377DUpK//fXdj8/P7nznDx5EiEhIfD398eePXvg4uIitMXGxmLmzJmiQHHxdRs3boyhQ4dCR0cHNjY2+OGHH0TtJa2rra0tvC7v6V8iIiKiTw1zwhIRERFRtSkeqExNTa30NSrjYi4rKytRm7yf7hetU1VVLXdu1OL9jIyMoKenJ5T19PRgaGgonDJ9+/ZtqfPp6+ujWbNm2LBhA1xdXZGYmAigIFVAVFSUcMK3+LqWlpaicvEcsCkpKXLXS0tLE16XN/BMRERE9KlhEJaIiIiIqo2WlhY0NTWFlAQlBfqKKh6EfPPmTan9K+NiLkdHR1HbgwcPROWYmBjhUi4AsLOzg5qaWrnWsba2hr6+vvAcb9++RX5+vnD6NT8/X3TCtHBPZVFVVUWNGjWEICwAJCYmCkHYJk2aiPoXfx+Lv89GRkZy10lOTlZ4b0RERESfGqYjICIiIqJq1bBhQ+F1WFhYmf3PnDkj5DPNzs7G6dOnRe1FT2ZWFmtra9jb2wtlPz8/BAUFCeUtW7aI+vfp00dUnjFjBmxtbYU/kZGRQpuSkhJ69uwplLOysnD8+HGhfOzYMVGqA2dnZ+H1yZMn4e3tLXOCWCqV4tChQ6L3UyKRwNzcXCg3atQIDRo0EMr+/v4IDw8XygcOHBDNWXTdQikpKaLgc6NGjWT6EBERERFPwhIRERFRNXNycsLdu3cBAMHBwWX2v3v3Lrp06QIbGxs8e/YMr1+/FrVv2rQJAQEBWLVqFYD/u5jrfc2YMQODBw9Gfn4+8vLyMHjwYDg5OSEpKQmPHj0S+jVs2BB9+/ZVaO5x48bhxIkTwmnUmTNn4uDBgwCAwMBAoZ+qqirGjBkjlMPDw7F27VrMmTMHNjY2qFWrFnJycvD8+XOZ96VTp04wNjYW1U2fPh0jR44EUBDQ7tu3L5o0aYLY2Fg8ffpU6Gdubo7evXvL7Lv45/XVV18p9NxEREREnwqehCUiIiKiatWhQwfhdWxsrOg0Zklev36Nq1evCoHGL7/8UmiLj4/H/fv3K32fTZs2xR9//AFVVVUABUHLa9euiQKwlpaW2LhxI1RUFDvrYGxsjI0bNwq5YPPz8xEQEICAgADk5+cDADQ0NLBkyRLRyeFCOTk5ePz4MS5duoRr167JBGCdnJzw+++/y4xr3bo1Zs+eLaQ+SEtLw7Vr10QB2Nq1a2PTpk3Q0NCQGX/z5k3hdb169WTyyhIRERFRAZ6EJSIiIqJq5eDgAFtbW+G06smTJ+Hp6Vlifw8PD9y9exfPnj1DrVq1MG7cOHTq1Am//PILLl++DHV1dbRq1apK9tqnTx/Y29tjx44duHnzJuLi4qCurg5LS0t06dIFHh4ecoOV5eHo6AgfHx9s27YNV65cwevXr5Gfnw8zMzO0atUKQ4cOlbksq3v37lBSUkJgYCBevHiB5ORkZGRkQFNTE2ZmZmjcuDG6du2Kdu3albju4MGD0bRpU+zcuRN+fn5ISEiAmpoarKys0KFDBwwePFh0UVghqVQKHx8fofzNN99U6LmJiIiIPgUSqVQqre5NEBEREdGn7cCBA5g9ezYAoG7dujh79iwkEgkAwNvbGzNnzhT6Lly4EO7u7tWyT/o//v7+8PDwAABoa2vjn3/+gYGBQfVuioiIiOgDxXQERERERFTt3N3dhZ+yv3jxAufOnaveDVGZtm7dKrweOXIkA7BEREREpWAQloiIiIiqnYqKCn788UehvGnTpmrcDZXlyZMnuHLlCgDAxMQEI0aMqOYdEREREX3YmBOWiIiIiD4InTt3FvLC0oetYcOG/KyIiIiIFMCcsERERERERERERERViOkIiIiIiIiIiIiIiKoQg7BEREREREREREREVYhBWCIiIiIiIiIiIqIqxCAsERERERERERERURViEJaIiIiIiIiIiIioCjEIS0RERESfhPz8fOzcuRM9e/aEvb09bG1tYWtrCwCIjIwUlamAh4cHbG1t4e3tXd1bISIiIvpPU6nuDRARERHRf9vLly9x8OBB3Lp1C5GRkXj79i00NDRgbm6OJk2aoEePHvjqq6+qe5tYt24d1q5dC4lEgvr160NHR6e6t1St1qxZAwAYOnQo9PT0qnk3RERERB83BmGJiIiIqELy8vKwbNkyeHl5ITc3FwBgYWEBc3NzpKWlISIiAiEhIdi3bx+++uor7N69u9r2KpVKsWfPHgDAihUr4ObmJmpXVVWFlZVVdWyt2qxduxYA0KdPnxKDsGZmZrCysoKuru6/uTUiIiKij45EKpVKq3sTRERERPTfIpVK4enpifPnz0NVVRWjR4/GoEGDYGJiIvTJyMjA1atXsWnTJjx8+BAhISHVtt/ExES0bNkSAHDv3j1oampW214+FIWpFy5evAgLC4tq3g0RERHRx40nYYmIiIhIYdu3bxcCsFu2bEGLFi1k+mhqaqJLly7o3LkzNmzYUA27/D+ZmZnCawZgiYiIiOjfxou5iIiIiEgh6enp2Lx5MwBg5MiRcgOwRUkkEowfP15UJ5VKcfLkSQwfPhzOzs6ws7NDmzZt8OOPP+Lhw4dy5/H29oatrS08PDyEcv/+/dGkSRM4OjrCw8MD169fF40pvHCrffv2Ql3hBVy2trZCXtSyLubKzs7Gpk2b4Obmhi+++AItW7bE5MmT8fTpU/j5+cmsUXytyMjIcj1TSWODg4MxadIktGrVCp9//rmwb6lUiitXrmDevHno3bs3mjdvDjs7O7i4uGDSpEkICAiQmXfNmjWi5+zQoYPc9wQo+2KusLAwzJw5E+3bt4ednR2++uorDB48GAcPHkReXp7cMUWfKyQkBD/88ANatmwJOzs7dOnSBWvXrkV2drbcsURERET/VTwJS0REREQKuXLlClJSUqCkpIQhQ4YoPD43NxdTp07F2bNnAQC1atWChYUFXrx4gZMnT8LHxwe//fYbvvnmmxLnmDVrFg4fPizkLA0PD4e/vz8CAgKwZs0adOzYEQCgrq4OR0dHZGdn48GDBwAAR0dHYR4zM7My95uZmYlRo0bh9u3bAIDPPvsMurq6uHz5Mq5cuYIJEyYo/B4o4ty5c1i+fDnU1NRgZWUFHR0dSCQSAAUB8TFjxkAikaBGjRowNTVFzZo1ER0djbNnz+LcuXP47bffMHDgQGE+MzMzODo6IjAwEABgZ2cHNTU1UXt5nD59GtOmTUNOTg60tLTQoEEDvHnzBrdv38bt27fh4+OD9evXQ0NDQ+7469evY8GCBVBWVoaVlRWUlZURERGBNWvWIDQ0FKtXr67oW0ZERET0wWEQloiIiIgUcufOHQCAjY0NjIyMFB6/ceNGnD17Fpqamli6dCk6deoEoOC06YoVK7Bjxw7873//Q8OGDeHg4CAz/u7duwgNDcX27dvRqlUrAAXByGnTpuH8+fP4448/0KFDB0gkEpiYmGDv3r2IjIxEhw4dAAB79+5VaL+rV6/G7du3oaenhzVr1qB58+YAgHfv3mHWrFlYtWqVwu+BIpYtW4ahQ4fihx9+gLq6OoD/S6+gqqqKefPmoW3btqhZs6YwJi8vD2fPnsWsWbOwYMECtG3bVgiu9uvXD/369RNOw65atUrhnLBhYWGYMWMGcnJy0L9/f8yaNQtaWloAgBs3bmDSpEm4fv06lixZgjlz5sid4/fff8fQoUMxadIk4blOnDiBn3/+GWfPnsWtW7eE95qIiIjov47pCIiIiIhIIbGxsQCAOnXqKDw2PT0dO3bsAABMnDhRCMACgJqaGmbMmIFmzZohLy+vxDyyOTk5mDVrlhCABQAtLS389ttvUFVVRVRUVKVdApaamioEbX/99VdRUFBXVxfLli2DqalppaxVkhYtWmD69OlCoBKAcLpUTU0N33zzjSgACwDKyspwc3PD0KFDkZOTgxMnTlTqnrZt24asrCw0aNAAv//+uxCABYCWLVti+vTpAIADBw4gLi5O7hzNmjXDzz//LHqunj17om3btgCAS5cuVeqeiYiIiKoTg7BEREREpJDU1FQAEAXeyisgIACpqalQV1cX/US+qBEjRgAoOFEpLzeorq4uvv76a5l6ExMTmJubAwBevnyp8N7kuXPnDtLT06GtrQ03NzeZdnV1dfTq1atS1ipJ3759y+wTHByM5cuXY/z48fDw8MDAgQMxcOBAnDlzBgDw+PHjSt3T1atXAQBDhgwRUiMU1bt3bxgZGSEnJwc3btyQO8d3330nt75JkyYAgBcvXlTSbomIiIiqH9MREBEREZFCdHR0ABScalVUeHg4AMDc3Bza2tpy+zRo0AAAkJWVhaioKFhZWYna69atKzfwBwDGxsaIiIhAWlqawnsrbb82NjZQVVWV26dRo0aVslZJ6tevX2Jbbm4uZs2ahWPHjpU6R0pKSqXt5927d4iPjwfwf59VcaqqqqhXrx4SExPx/PlzuX0sLS3l1hemuKisz5CIiIjoQ8AgLBEREREppPCn769evVJ4bGFgzdjYuMQ+RX/eLy8QV9oJXCWlgh96SaVShfcmT2GguaSAcVltlUFTU7PEtu3bt+PYsWNQV1fH1KlT4eLiAjMzM2hqakIikeDQoUP45ZdfkJubW2n7KfqZlPY5mpiYyPQvqqTnKvwMiYiIiD4m/BsOERERESmkadOmAIBnz54hMTFRobGFAcuEhIQS+xTNIVrVAc6yFAZ8SzuVWZ4TmyUFhTMyMiq2sf/P29sbADB9+nQMGzYM1tbW0NLSEk4KV+YJ2EJFP5PSPsfC07LV/RkSERERfQgYhCUiIiIihbRp0wYGBgbIz8+Hl5eXQmPr1asHAIiKiioxeBkaGgqgIN9qYY7X6lKYCuHZs2fIycmR26e0fKuFQdySgtWF6Q4qKjIyEkDBJVfyBAUFvdf88ujq6gqnXAs/q+Jyc3OFNASFnzkRERHRp4xBWCIiIiJSiLa2NkaNGgUA2LZtG27evFlqf6lUig0bNgAoOEWro6ODrKws7N27V27/HTt2AABatmwJNTW1Sty54po1awYtLS2kpaUJl1wVlZ2dXWo+1rp16wIA7t27J9P29u1bnDp16r32V/iT/sJTp0WFhYXh0qVLZY7NzMxUeF1XV1cAgJeXl9xTvseOHUNiYiJUVVXRqlUrhecnIiIi+tgwCEtEREREChs1ahTat2+PnJwcjB49GqtXr5YJBGZlZeHChQvo378/Vq5cCaDgZOjw4cMBAGvXrsWFCxeE/tnZ2ViyZAlu374NZWVljBs37l97npJoa2tj0KBBAIDff/8d/v7+Qltqaip+/vlnxMbGlji+ffv2AICtW7fiyZMnQn18fDx+/PFHvHv37r3299VXXwEAVqxYIUrj8OTJE4wbN67U/KqfffYZAODGjRsKrztixAioq6sjNDQUc+bMEV3SdvPmTSxevBgA8M033winZomIiIg+ZbyYi4iIiIgUJpFIsGbNGixZsgS7d+/GunXrsH79elhYWKBGjRpIS0tDZGQksrKyAADNmzcXxn7//fcIDQ3F2bNnMWHCBJiZmcHY2BgRERF49+4dlJSU8Ntvv8HBwaG6Hk/E09MTQUFBuH37Njw8PFC3bl3o6uoiLCwMUqkUkyZNwvLly+UGPIcPH47jx4/j1atX6NOnD+rWrQt1dXU8e/YMpqammDBhghCgrojJkyfj5s2bePjwITp06AArKytkZ2cjPDwcZmZmmDBhAlasWCF3bO/evbF48WIsWLAAe/fuhZGRESQSCfr06QN3d/dS17W2tsaiRYswbdo0HDhwACdPnkS9evXw5s0b4cK2Vq1a4eeff67wsxERERF9THgSloiIiIgqREVFBbNmzYKPjw9Gjx4NOzs7pKam4tGjR4iNjYWVlRUGDhyIv//+G3/99Zdo3KpVq7Bs2TI0b94c6enpePLkCTQ1NdGjRw8cPHgQ33zzTTU+mZiGhga2b9+OKVOmwMrKCtHR0Xj9+jVcXFxw4MAB1K9fHwCgo6MjM1ZXVxd79+7FgAEDYGRkhMjISLx9+xbffvstvL29UbNmzffam62tLfbt24cOHTpAQ0MD4eHhyM3NhYeHB44cOVLqKdRhw4Zh+vTpaNiwIaKjo3H79m34+/sjKiqqXGu7ubnh6NGjcHd3h4GBAUJCQpCSkoJmzZph/vz52LJlCzQ0NN7r+YiIiIg+FhJpSVe1EhERERFRmbZu3YqlS5eiU6dOWLt2bXVvh4iIiIg+QDwJS0RERERUQTk5OTh69CiAgku8iIiIiIjkYRCWiIiIiKgMq1atQnh4uKguISEBP/74I54+fQo9PT306tWrmnZHRERERB86piMgIiIiIiqDs7MzUlJSUKtWLdSsWRNpaWkIDw9HXl4e1NTUsHLlSnTo0KG6t0lEREREHygGYYmIiIiIyrB3715cvHgRT58+RUpKCqRSKUxNTeHs7IwRI0bA2tq6urdIRERERB8wBmGJiIiIiIiIiIiIqhBzwhIRERERERERERFVIQZhiYiIiIiIiIiIiKoQg7BEREREREREREREVYhBWCIiIiIiIiIiIqIqxCAsERERERERERERURViEJaIiIiIiIiIiIioCjEIS0RERERERERERFSFGIQlIiIiIiIiIiIiqkIMwhIRERERERERERFVof8Hnn7s+NjzvawAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1400x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_top_configurations_rnn(results, k_splits=param_grid['k'][0], top_n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkklZJpQQSb"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Esv7nvO4QQSb",
        "outputId": "eb85a3c0-176a-4a46-8a5c-a2ca5aa41d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\n",
            "============================================================\n",
            "\n",
            "Best Configuration Found:\n",
            "  l1_lambda: 0\n",
            "  rnn_type: GRU\n",
            "  bidirectional: True\n",
            "  window_size: 100\n",
            "  stride: 25\n",
            "  n_val_users: 45\n",
            "  hidden_size: 64\n",
            "  hidden_layers: 2\n",
            "  batch_size: 128\n",
            "  learning_rate: 0.001\n",
            "  dropout_rate: 0.3\n",
            "  l2_lambda: 0.0001\n",
            "  k: 2\n",
            "  epochs: 2\n",
            "\n",
            "Best CV F1 Score: 0.6358\n",
            "\n",
            "============================================================\n",
            "CREATING FINAL TRAIN/VAL SPLIT\n",
            "============================================================\n",
            "Training samples: 616 (98560 timesteps)\n",
            "Validation samples: 45 (7200 timesteps)\n",
            "Training data: 93.2% of total\n",
            "\n",
            "Building sequences...\n",
            "Built 2464 sequences with 2464 labels\n",
            "Built 180 sequences with 180 labels\n",
            "Training sequences: (2464, 100, 37)\n",
            "Validation sequences: (180, 100, 37)\n",
            "\n",
            "============================================================\n",
            "INITIALIZING FINAL MODEL\n",
            "============================================================\n",
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "rnn (GRU)                 [[-1, 100, 128], [4, -1, 64]] 114,048        \n",
            "classifier (Linear)       [-1, 3]                      387            \n",
            "===============================================================================\n",
            "Total params: 114,435\n",
            "Trainable params: 114,435\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "TRAINING FINAL MODEL\n",
            "============================================================\n",
            "Training for 2 epochs\n",
            "Training 2 epochs...\n",
            "Epoch   1/2 | Train: Loss=0.7364, F1 Score=0.6670 | Val: Loss=0.6052, F1 Score=0.7732\n",
            "Best model restored from epoch 1 with val_f1 0.7732\n",
            "\n",
            "✓ Final model trained!\n",
            "Best Validation F1: 0.7732\n",
            "CV F1 Score: 0.6358\n",
            "Improvement: +0.1374\n",
            "\n",
            "============================================================\n",
            "PREPARING TEST DATA\n",
            "============================================================\n",
            "Built 5296 sequences with 5296 labels\n",
            "Test sequences: (5296, 100, 37)\n",
            "\n",
            "============================================================\n",
            "MAKING PREDICTIONS\n",
            "============================================================\n",
            "Total window predictions: 5296\n",
            "\n",
            "Mapping 5296 windows to 1324 samples...\n",
            "\n",
            "============================================================\n",
            "PREDICTIONS SAVED\n",
            "============================================================\n",
            "Output file: pirate_pain_test_predictions.csv\n",
            "Total samples: 1324\n",
            "\n",
            "📊 Performance Comparison:\n",
            "  CV Mean F1: 0.6358\n",
            "  Final Model Val F1: 0.7732\n",
            "  Improvement: +0.1374 (+21.6%)\n",
            "\n",
            "📈 Prediction Distribution:\n",
            "predicted_label\n",
            "no_pain    1324\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  no_pain: 1324 (100.0%)\n",
            "  low_pain: 0 (0.0%)\n",
            "  high_pain: 0 (0.0%)\n",
            "\n",
            "🔍 First 10 predictions:\n",
            "   sample_index  predicted_label_id predicted_label\n",
            "0             0                   0         no_pain\n",
            "1             1                   0         no_pain\n",
            "2             2                   0         no_pain\n",
            "3             3                   0         no_pain\n",
            "4             4                   0         no_pain\n",
            "5             5                   0         no_pain\n",
            "6             6                   0         no_pain\n",
            "7             7                   0         no_pain\n",
            "8             8                   0         no_pain\n",
            "9             9                   0         no_pain\n"
          ]
        }
      ],
      "source": [
        "# filepath: /home/federico/Desktop/ANN/grid_search.ipynb\n",
        "\n",
        "# ========================================\n",
        "# STEP 1: PREPARE FINAL TRAINING\n",
        "# ========================================\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Merge best config with fixed params\n",
        "final_best_params = {**fixed_params, **best_config}\n",
        "print(\"\\nBest Configuration Found:\")\n",
        "for key, value in final_best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\nBest CV F1 Score: {best_score:.4f}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 2: CREATE FINAL TRAIN/VAL SPLIT\n",
        "# (Use minimal validation set, maximize training data)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING FINAL TRAIN/VAL SPLIT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use MINIMAL validation set (e.g., 10% of n_val_users from grid search)\n",
        "# This maximizes training data while keeping a small val set for early stopping\n",
        "unique_samples = X_train['sample_index'].unique()\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "np.random.shuffle(unique_samples)\n",
        "\n",
        "# Use smaller validation set than CV (e.g., 20-30 samples instead of 45)\n",
        "n_val_users_final = max(20, int(final_best_params['n_val_users']))  # Half of CV val size\n",
        "n_train_samples = len(unique_samples) - n_val_users_final\n",
        "\n",
        "train_samples = unique_samples[:n_train_samples]\n",
        "val_samples = unique_samples[n_train_samples:]\n",
        "\n",
        "df_train_final = X_train[X_train['sample_index'].isin(train_samples)].copy()\n",
        "df_val_final = X_train[X_train['sample_index'].isin(val_samples)].copy()\n",
        "\n",
        "print(f\"Training samples: {len(train_samples)} ({df_train_final.shape[0]} timesteps)\")\n",
        "print(f\"Validation samples: {len(val_samples)} ({df_val_final.shape[0]} timesteps)\")\n",
        "print(f\"Training data: {len(train_samples)/(len(train_samples)+len(val_samples))*100:.1f}% of total\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 3: PREPROCESS DATA\n",
        "# ========================================\n",
        "# Map labels (if needed)\n",
        "if df_train_final['label'].dtype == 'object':\n",
        "    df_train_final['label'] = df_train_final['label'].map(pain_mapping)\n",
        "    df_val_final['label'] = df_val_final['label'].map(pain_mapping)\n",
        "\n",
        "# Normalize features\n",
        "pain_survey_columns = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes']\n",
        "joint_columns = [f'joint_{i:02d}' for i in range(30)]\n",
        "scale_columns = pain_survey_columns + joint_columns\n",
        "\n",
        "train_max = df_train_final[scale_columns].max()\n",
        "train_min = df_train_final[scale_columns].min()\n",
        "\n",
        "for column in scale_columns:\n",
        "    df_train_final[column] = (df_train_final[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "    df_val_final[column] = (df_val_final[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "\n",
        "# Build sequences\n",
        "print(\"\\nBuilding sequences...\")\n",
        "X_train_final, y_train_final = build_sequences(\n",
        "    df_train_final, \n",
        "    window=final_best_params['window_size'], \n",
        "    stride=final_best_params['stride']\n",
        ")\n",
        "X_val_final, y_val_final = build_sequences(\n",
        "    df_val_final, \n",
        "    window=final_best_params['window_size'], \n",
        "    stride=final_best_params['stride']\n",
        ")\n",
        "\n",
        "print(f\"Training sequences: {X_train_final.shape}\")\n",
        "print(f\"Validation sequences: {X_val_final.shape}\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_ds_final = TensorDataset(\n",
        "    torch.from_numpy(X_train_final.astype(np.float32)), \n",
        "    torch.from_numpy(y_train_final.astype(np.int64))\n",
        ")\n",
        "val_ds_final = TensorDataset(\n",
        "    torch.from_numpy(X_val_final.astype(np.float32)), \n",
        "    torch.from_numpy(y_val_final.astype(np.int64))\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader_final = make_loader(\n",
        "    train_ds_final, \n",
        "    batch_size=final_best_params['batch_size'], \n",
        "    shuffle=True, \n",
        "    drop_last=False\n",
        ")\n",
        "val_loader_final = make_loader(\n",
        "    val_ds_final, \n",
        "    batch_size=final_best_params['batch_size'], \n",
        "    shuffle=False, \n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# STEP 4: INITIALIZE FINAL MODEL\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INITIALIZING FINAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_model = RecurrentClassifier(\n",
        "    input_size=X_train_final.shape[2],\n",
        "    hidden_size=final_best_params['hidden_size'],\n",
        "    num_layers=final_best_params['hidden_layers'],\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=final_best_params['dropout_rate'],\n",
        "    bidirectional=final_best_params['bidirectional'],\n",
        "    rnn_type=final_best_params['rnn_type']\n",
        ").to(device)\n",
        "\n",
        "recurrent_summary(final_model, input_size=(final_best_params['window_size'], X_train_final.shape[2]))\n",
        "\n",
        "# ========================================\n",
        "# STEP 5: TRAIN FINAL MODEL\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING FINAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "optimizer_final = torch.optim.AdamW(\n",
        "    final_model.parameters(), \n",
        "    lr=final_best_params['learning_rate'], \n",
        "    weight_decay=final_best_params['l2_lambda']\n",
        ")\n",
        "scaler_final = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "# Create directory for final model\n",
        "os.makedirs(\"models/final_model\", exist_ok=True)\n",
        "\n",
        "# Train with MORE epochs than CV (since we have more data)\n",
        "final_epochs = int(final_best_params['epochs'])  # 1.5x more epochs\n",
        "final_patience = PATIENCE\n",
        "print(f\"Training for {final_epochs} epochs\")\n",
        "\n",
        "final_model, training_history = fit(\n",
        "    model=final_model,\n",
        "    train_loader=train_loader_final,\n",
        "    val_loader=val_loader_final,\n",
        "    epochs=final_epochs,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_final,\n",
        "    scaler=scaler_final,\n",
        "    device=device,\n",
        "    l1_lambda=final_best_params['l1_lambda'],\n",
        "    l2_lambda=0,  # Already in optimizer weight_decay\n",
        "    patience=final_patience,\n",
        "    evaluation_metric=\"val_f1\",\n",
        "    mode='max',\n",
        "    restore_best_weights=True,\n",
        "    writer=None,\n",
        "    verbose=VERBOSE,\n",
        "    experiment_name=\"final_model/best\"\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Final model trained!\")\n",
        "print(f\"Best Validation F1: {max(training_history['val_f1']):.4f}\")\n",
        "print(f\"CV F1 Score: {best_score:.4f}\")\n",
        "print(f\"Improvement: {max(training_history['val_f1']) - best_score:+.4f}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 6: LOAD TEST DATA\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPARING TEST DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_test = pd.read_csv('pirate_pain_test.csv')\n",
        "\n",
        "# Drop joint_30\n",
        "if 'joint_30' in X_test.columns:\n",
        "    X_test.drop('joint_30', axis=1, inplace=True)\n",
        "\n",
        "# Convert categorical to binary\n",
        "binary_cols = ['n_hands', 'n_eyes', 'n_legs']\n",
        "for col in binary_cols:\n",
        "    X_test[col] = X_test[col].map(lambda x: 1 if str(x).lower().strip() == 'two' else 0)\n",
        "\n",
        "df_test_original = X_test.copy()\n",
        "df_test_processed = X_test.copy()\n",
        "\n",
        "# Normalize test data using TRAINING statistics (from final training set)\n",
        "for column in scale_columns:\n",
        "    df_test_processed[column] = (df_test_processed[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "\n",
        "# Build test sequences\n",
        "df_test_processed['label'] = 0  # Dummy label\n",
        "X_test_sequences, _ = build_sequences(\n",
        "    df_test_processed,\n",
        "    window=final_best_params['window_size'],\n",
        "    stride=final_best_params['stride']\n",
        ")\n",
        "\n",
        "print(f\"Test sequences: {X_test_sequences.shape}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 7: MAKE PREDICTIONS\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MAKING PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load the best model weights\n",
        "final_model.load_state_dict(torch.load(\"models/final_model/best_model.pt\", map_location=device))\n",
        "final_model.eval()\n",
        "\n",
        "# Create test loader\n",
        "test_ds = TensorDataset(torch.from_numpy(X_test_sequences.astype(np.float32)))\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=final_best_params['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Get predictions\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for (xb,) in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = final_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_predictions.append(preds)\n",
        "\n",
        "all_predictions = np.concatenate(all_predictions)\n",
        "print(f\"Total window predictions: {len(all_predictions)}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 8: MAP PREDICTIONS TO SAMPLES  (coerente con build_sequences)\n",
        "# ========================================\n",
        "sample_predictions = []\n",
        "pred_idx = 0\n",
        "\n",
        "unique_samples = df_test_processed['sample_index'].unique()\n",
        "window = final_best_params['window_size']\n",
        "stride = final_best_params['stride']\n",
        "\n",
        "def n_windows_like_training(n_timestamps: int, window: int, stride: int) -> int:\n",
        "    \"\"\"Conta le finestre esattamente come in build_sequences:\n",
        "       - se L < window: 1 finestra (padding solo fino a window)\n",
        "       - altrimenti sliding con stride\n",
        "       - ancora l’ultima finestra alla fine se non già inclusa\n",
        "    \"\"\"\n",
        "    L = n_timestamps\n",
        "    if L <= 0:\n",
        "        return 0\n",
        "    if L < window:\n",
        "        return 1\n",
        "\n",
        "    starts = list(range(0, max(L - window, 0) + 1, stride))\n",
        "    last_start = L - window\n",
        "    if last_start >= 0 and (len(starts) == 0 or starts[-1] != last_start):\n",
        "        starts.append(last_start)\n",
        "    return len(starts)\n",
        "\n",
        "print(f\"\\nMapping {len(all_predictions)} windows to {len(unique_samples)} samples...\")\n",
        "\n",
        "for sample_id in unique_samples:\n",
        "    L = (df_test_processed['sample_index'] == sample_id).sum()\n",
        "    n_windows = n_windows_like_training(L, window, stride)\n",
        "\n",
        "    end_idx = min(pred_idx + n_windows, len(all_predictions))\n",
        "    sample_window_predictions = all_predictions[pred_idx:end_idx]\n",
        "\n",
        "    if sample_window_predictions.size > 0:\n",
        "        final_prediction = np.bincount(sample_window_predictions).argmax()\n",
        "    else:\n",
        "        final_prediction = 0  # fallback prudente\n",
        "\n",
        "    sample_predictions.append({\n",
        "        'sample_index': sample_id,\n",
        "        'predicted_label_id': final_prediction,\n",
        "        'predicted_label': label_reverse_mapping[final_prediction]\n",
        "    })\n",
        "\n",
        "    pred_idx = end_idx\n",
        "\n",
        "# sicurezza: deve consumare tutte le finestre\n",
        "assert pred_idx == len(all_predictions), \\\n",
        "    f\"Consumed {pred_idx} preds but have {len(all_predictions)}\"\n",
        "# ========================================\n",
        "# STEP 9: SAVE RESULTS\n",
        "# ========================================\n",
        "predictions_df = pd.DataFrame(sample_predictions)\n",
        "\n",
        "output_df = df_test_original[['sample_index']].drop_duplicates().merge(\n",
        "    predictions_df,\n",
        "    on='sample_index',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Handle missing predictions\n",
        "if output_df['predicted_label'].isna().any():\n",
        "    print(f\"WARNING: {output_df['predicted_label'].isna().sum()} samples missing predictions\")\n",
        "    output_df['predicted_label'].fillna('no_pain', inplace=True)\n",
        "\n",
        "# Save CSV\n",
        "output_filename = 'pirate_pain_test_predictions.csv'\n",
        "output_df[['sample_index', 'predicted_label']].to_csv(output_filename, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTIONS SAVED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Output file: {output_filename}\")\n",
        "print(f\"Total samples: {len(output_df)}\")\n",
        "print(f\"\\n📊 Performance Comparison:\")\n",
        "print(f\"  CV Mean F1: {best_score:.4f}\")\n",
        "print(f\"  Final Model Val F1: {max(training_history['val_f1']):.4f}\")\n",
        "print(f\"  Improvement: {max(training_history['val_f1']) - best_score:+.4f} ({((max(training_history['val_f1'])/best_score - 1)*100):+.1f}%)\")\n",
        "\n",
        "print(\"\\n📈 Prediction Distribution:\")\n",
        "print(output_df['predicted_label'].value_counts())\n",
        "print(\"\\nPercentages:\")\n",
        "for pain_level in ['no_pain', 'low_pain', 'high_pain']:\n",
        "    count = (output_df['predicted_label'] == pain_level).sum()\n",
        "    percentage = (count / len(output_df)) * 100\n",
        "    print(f\"  {pain_level}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\n🔍 First 10 predictions:\")\n",
        "print(output_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-KsexxeUae42",
        "outputId": "2768c8b8-d33c-4d47-dd4c-a70f3bc2d804"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshutil\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[32m      4\u001b[39m zip_path = shutil.make_archive(\u001b[33m'\u001b[39m\u001b[33m/content/models\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mzip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m/content\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m files.download(zip_path)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = shutil.make_archive('/content/models', 'zip', '/content', 'models')\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "30ZrMlvpQQSc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
