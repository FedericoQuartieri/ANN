{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17b3dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add local_lib to sys.path so packages installed there are found\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'local_lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83eadb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['effb0_224_maskmul_f1', 'resnet_only', 'test'])\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import includes.config as cfgmod\n",
    "\n",
    "cfgmod = importlib.reload(cfgmod)\n",
    "from includes.config import TrainingConfig, GRID_SEARCH_SPACES\n",
    "\n",
    "print(cfgmod.GRID_SEARCH_SPACES.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c882e86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Project root: c:\\Users\\danie\\ANN\n",
      "Working dir: c:\\Users\\danie\\ANN\n",
      "Python path contains 'includes'? -> True\n",
=======
      "Project root: /home/federico/Desktop/Shared/Projects/ANN\n",
      "Working dir: /home/federico/Desktop/Shared/Projects/ANN\n",
>>>>>>> 7ee1d96 (added grid search and k-fold)
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'backbone': ['resnet18'],\n",
       " 'img_size': [224],\n",
       " 'batch_size': [16],\n",
       " 'num_workers': [4],\n",
       " 'lr': [0.0001],\n",
       " 'weight_decay': [0.0001],\n",
       " 'epochs': [1],\n",
       " 'use_scheduler': [True],\n",
       " 'use_masks': [False],\n",
       " 'mask_mode': ['crop_bbox'],\n",
       " 'cv_type': ['kfold'],\n",
       " 'n_splits': [5]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 - Setup, imports, scelta esperimento\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = \"/content/drive/MyDrive/[2025-2026] AN2DL/challenge2\"\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "\n",
    "from includes.config import TrainingConfig, GRID_SEARCH_SPACES\n",
    "from includes.data_utils import (\n",
    "    load_labels_and_split,\n",
    "    load_full_labels,\n",
    "    get_transforms,\n",
    "    create_dataloaders,\n",
    ")\n",
    "\n",
    "from includes.model_utils import (\n",
    "    build_model,\n",
    "    create_criterion_optimizer_scheduler,\n",
    "    train_model,\n",
    "    evaluate,\n",
    ")\n",
    "from includes.inference_utils import create_test_loader, run_inference_and_save\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --------- choose which experiment to run ---------\n",
    "# available:\n",
    "#   \"baseline\"\n",
    "#   \"resnet50_big\"\n",
    "#   \"resnet50_big_masks\"\n",
    "#   \"challenge_2-2\"\n",
    "EXP_NAME = \"resnet50_big_masks\"\n",
    "cfg = EXPERIMENTS[EXP_NAME]\n",
    "\n",
    "# base cfg con default (paths, val_size, ecc.)\n",
    "cfg = TrainingConfig(exp_name=EXP_NAME)\n",
    "cfg.project_root = PROJECT_ROOT\n",
    "cfg.out_dir = \"out\"\n",
    "\n",
    "# griglia associata a questo esperimento\n",
    "param_grid = GRID_SEARCH_SPACES[EXP_NAME]\n",
    "param_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9472f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "GRID SEARCH for experiment 'test' with 1 combinations\n",
      "Param keys: ['backbone', 'img_size', 'batch_size', 'num_workers', 'lr', 'weight_decay', 'epochs', 'use_scheduler', 'use_masks', 'mask_mode', 'cv_type', 'n_splits']\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Grid 1/1] params = {'backbone': 'resnet18', 'img_size': 224, 'batch_size': 16, 'num_workers': 4, 'lr': 0.0001, 'weight_decay': 0.0001, 'epochs': 1, 'use_scheduler': True, 'use_masks': False, 'mask_mode': 'crop_bbox', 'cv_type': 'kfold', 'n_splits': 5}\n",
      "  >> Using StratifiedKFold with 5 folds\n",
      "    [Fold 1/5]\n",
      "==============================================================\n",
      "Starting training - experiment: test\n",
      "Backbone: resnet18  |  img_size: 224  |  epochs: 1\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/1]\n",
      "    [Batch  71/71] loss=1.4067  f1=0.2917\n",
      "  Train - loss: 1.4067  |  f1: 0.2917\n",
      "  Val   - loss: 1.3887  |  f1: 0.3168\n",
      "  >> New best model! val_f1 improved to 0.3168\n",
      "\n",
      "==============================================================\n",
      "Training finished for experiment: test\n",
      "Best validation F1 (macro): 0.3168\n",
      "==============================================================\n",
      "    >> Fold best F1: 0.3168\n",
      "    [Fold 2/5]\n",
      "==============================================================\n",
      "Starting training - experiment: test\n",
      "Backbone: resnet18  |  img_size: 224  |  epochs: 1\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/1]\n",
      "    [Batch  71/71] loss=1.4113  f1=0.3083\n",
      "  Train - loss: 1.4113  |  f1: 0.3083\n",
      "  Val   - loss: 1.3613  |  f1: 0.3275\n",
      "  >> New best model! val_f1 improved to 0.3275\n",
      "\n",
      "==============================================================\n",
      "Training finished for experiment: test\n",
      "Best validation F1 (macro): 0.3275\n",
      "==============================================================\n",
      "    >> Fold best F1: 0.3275\n",
      "    [Fold 3/5]\n",
      "==============================================================\n",
      "Starting training - experiment: test\n",
      "Backbone: resnet18  |  img_size: 224  |  epochs: 1\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/1]\n",
      "    [Batch  71/71] loss=1.4182  f1=0.2967\n",
      "  Train - loss: 1.4182  |  f1: 0.2967\n",
      "  Val   - loss: 1.3288  |  f1: 0.3090\n",
      "  >> New best model! val_f1 improved to 0.3090\n",
      "\n",
      "==============================================================\n",
      "Training finished for experiment: test\n",
      "Best validation F1 (macro): 0.3090\n",
      "==============================================================\n",
      "    >> Fold best F1: 0.3090\n",
      "    [Fold 4/5]\n",
      "==============================================================\n",
      "Starting training - experiment: test\n",
      "Backbone: resnet18  |  img_size: 224  |  epochs: 1\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/1]\n",
      "    [Batch  71/71] loss=1.4005  f1=0.3138\n",
      "  Train - loss: 1.4005  |  f1: 0.3138\n",
      "  Val   - loss: 1.3963  |  f1: 0.3028\n",
      "  >> New best model! val_f1 improved to 0.3028\n",
      "\n",
      "==============================================================\n",
      "Training finished for experiment: test\n",
      "Best validation F1 (macro): 0.3028\n",
      "==============================================================\n",
      "    >> Fold best F1: 0.3028\n",
      "    [Fold 5/5]\n",
      "==============================================================\n",
      "Starting training - experiment: test\n",
      "Backbone: resnet18  |  img_size: 224  |  epochs: 1\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/1]\n",
      "    [Batch  71/71] loss=1.3837  f1=0.3050\n",
      "  Train - loss: 1.3837  |  f1: 0.3050\n",
      "  Val   - loss: 1.3644  |  f1: 0.2704\n",
      "  >> New best model! val_f1 improved to 0.2704\n",
      "\n",
      "==============================================================\n",
      "Training finished for experiment: test\n",
      "Best validation F1 (macro): 0.2704\n",
      "==============================================================\n",
      "    >> Fold best F1: 0.2704\n",
      "  >> Mean F1 over 5 folds: 0.3053\n",
      "\n",
      "================ GRID SEARCH SUMMARY ================\n",
      "   run  best_val_f1  backbone  img_size  batch_size  num_workers      lr  \\\n",
      "0    1     0.305308  resnet18       224          16            4  0.0001   \n",
      "\n",
      "   weight_decay  epochs  use_scheduler  use_masks  mask_mode cv_type  n_splits  \n",
      "0        0.0001       1           True      False  crop_bbox   kfold         5  \n",
      "\n",
      "Best config (from grid):\n",
      "{'exp_name': 'test', 'project_root': '/home/federico/Desktop/Shared/Projects/ANN', 'data_dir': 'data', 'out_dir': 'out', 'train_img_dir': 'train_data', 'test_img_dir': 'test_data', 'labels_csv': 'train_labels.csv', 'mask_dir': None, 'val_size': 0.2, 'random_seed': 42, 'backbone': 'resnet18', 'img_size': 224, 'batch_size': 16, 'num_workers': 4, 'lr': 0.0001, 'weight_decay': 0.0001, 'epochs': 1, 'use_scheduler': True, 'use_masks': False, 'mask_mode': 'crop_bbox', 'cv_type': 'kfold', 'n_splits': 5}\n",
      "Best val F1: 0.3053078456004309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 - Grid search with optional StratifiedKFold\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "base_cfg = cfg  # base config for this experiment\n",
    "\n",
    "keys = list(param_grid.keys())\n",
    "values_list = [param_grid[k] for k in keys]\n",
    "combos = list(itertools.product(*values_list))\n",
    "n_combos = len(combos)\n",
    "\n",
    "print(\"==============================================================\")\n",
    "print(f\"GRID SEARCH for experiment '{EXP_NAME}' with {n_combos} combinations\")\n",
    "print(\"Param keys:\", keys)\n",
    "print(\"==============================================================\")\n",
    "\n",
    "results = []\n",
    "best_val_f1 = -1.0\n",
    "best_cfg = None\n",
    "best_state_dict = None\n",
    "best_idx_to_label = None\n",
    "best_val_t = None\n",
    "\n",
    "for i, values in enumerate(combos, start=1):\n",
    "    params = dict(zip(keys, values))\n",
    "\n",
    "    print(\"\\n--------------------------------------------------------------\")\n",
    "    print(f\"[Grid {i}/{n_combos}] params = {params}\")\n",
    "\n",
    "    # Validation strategy from params (fallback to holdout)\n",
    "    cv_type = params.get(\"cv_type\", \"holdout\")\n",
    "    n_splits = int(params.get(\"n_splits\", 5))\n",
    "\n",
    "    # 1) Copy base cfg and attach all params as attributes\n",
    "    cfg_i = copy.deepcopy(base_cfg)\n",
    "    for k, v in params.items():\n",
    "        setattr(cfg_i, k, v)\n",
    "\n",
    "    # 2) Common: full labels + transforms\n",
    "    labels_df, unique_labels, label_to_idx, idx_to_label_i = load_full_labels(cfg_i)\n",
    "    train_t_i, val_t_i = get_transforms(cfg_i)\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    fold_f1s = []\n",
    "    best_state_i = None\n",
    "\n",
    "    if cv_type == \"kfold\":\n",
    "        # ----- Stratified K-Fold -----\n",
    "        print(f\"  >> Using StratifiedKFold with {n_splits} folds\")\n",
    "\n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=True,\n",
    "            random_state=cfg_i.random_seed,\n",
    "        )\n",
    "\n",
    "        best_fold_f1 = -1.0\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            skf.split(labels_df, labels_df[\"label_idx\"]), start=1\n",
    "        ):\n",
    "            print(f\"    [Fold {fold}/{n_splits}]\")\n",
    "\n",
    "            train_df = labels_df.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = labels_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "            train_loader_i, val_loader_i = create_dataloaders(\n",
    "                cfg_i, train_df, val_df, train_t_i, val_t_i\n",
    "            )\n",
    "\n",
    "            model_i = build_model(cfg_i, num_classes=num_classes, device=device)\n",
    "            criterion_i, optimizer_i, scheduler_i = create_criterion_optimizer_scheduler(\n",
    "                cfg_i, model_i, train_df, device\n",
    "            )\n",
    "\n",
    "            best_state_fold, history_fold = train_model(\n",
    "                cfg_i,\n",
    "                model_i,\n",
    "                train_loader_i,\n",
    "                val_loader_i,\n",
    "                criterion_i,\n",
    "                optimizer_i,\n",
    "                scheduler_i,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            val_f1_list = history_fold.get(\"val_f1\", history_fold.get(\"val_acc\", []))\n",
    "            fold_best_f1 = max(val_f1_list) if val_f1_list else 0.0\n",
    "            fold_f1s.append(fold_best_f1)\n",
    "            print(f\"    >> Fold best F1: {fold_best_f1:.4f}\")\n",
    "\n",
    "            if fold_best_f1 > best_fold_f1:\n",
    "                best_fold_f1 = fold_best_f1\n",
    "                best_state_i = copy.deepcopy(best_state_fold)\n",
    "\n",
    "            del model_i\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        run_best_f1 = float(np.mean(fold_f1s))\n",
    "        print(f\"  >> Mean F1 over {n_splits} folds: {run_best_f1:.4f}\")\n",
    "\n",
    "    else:\n",
    "        # ----- Standard hold-out validation (current behaviour) -----\n",
    "        train_df, val_df, unique_labels, label_to_idx, idx_to_label_i = load_labels_and_split(\n",
    "            cfg_i\n",
    "        )\n",
    "        train_t_i, val_t_i = get_transforms(cfg_i)\n",
    "        train_loader_i, val_loader_i = create_dataloaders(\n",
    "            cfg_i, train_df, val_df, train_t_i, val_t_i\n",
    "        )\n",
    "\n",
    "        model_i = build_model(cfg_i, num_classes=num_classes, device=device)\n",
    "        criterion_i, optimizer_i, scheduler_i = create_criterion_optimizer_scheduler(\n",
    "            cfg_i, model_i, train_df, device\n",
    "        )\n",
    "\n",
    "        best_state_i, history_i = train_model(\n",
    "            cfg_i,\n",
    "            model_i,\n",
    "            train_loader_i,\n",
    "            val_loader_i,\n",
    "            criterion_i,\n",
    "            optimizer_i,\n",
    "            scheduler_i,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        val_f1_list = history_i.get(\"val_f1\", history_i.get(\"val_acc\", []))\n",
    "        run_best_f1 = max(val_f1_list) if val_f1_list else 0.0\n",
    "        print(f\"Best val F1 for this run: {run_best_f1:.4f}\")\n",
    "\n",
    "        del model_i\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 3) Save result row\n",
    "    row = {\"run\": i, \"best_val_f1\": run_best_f1}\n",
    "    for k in keys:\n",
    "        row[k] = params[k]\n",
    "    results.append(row)\n",
    "\n",
    "    # 4) Update global best\n",
    "    if run_best_f1 > best_val_f1:\n",
    "        best_val_f1 = run_best_f1\n",
    "        best_cfg = copy.deepcopy(cfg_i)\n",
    "        best_state_dict = copy.deepcopy(best_state_i)\n",
    "        best_idx_to_label = idx_to_label_i\n",
    "        best_val_t = val_t_i\n",
    "\n",
    "# ---------- grid search summary ----------\n",
    "results_df = pd.DataFrame(results).sort_values(\"best_val_f1\", ascending=False)\n",
    "print(\"\\n================ GRID SEARCH SUMMARY ================\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nBest config (from grid):\")\n",
    "print(best_cfg.__dict__)\n",
    "print(\"Best val F1:\", best_val_f1)\n",
    "\n",
    "# ---------- objects for the rest of the notebook ----------\n",
    "cfg = best_cfg\n",
    "idx_to_label = best_idx_to_label\n",
    "val_t = best_val_t\n",
    "\n",
    "num_classes = len(idx_to_label)\n",
    "model = build_model(cfg, num_classes=num_classes, device=device)\n",
    "model.load_state_dict(best_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eb61bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\danie/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Model, criterion, optimizer, scheduler\n",
    "\n",
    "model = build_model(cfg, num_classes=len(unique_labels), device=device)\n",
    "criterion, optimizer, scheduler = create_criterion_optimizer_scheduler(\n",
    "    cfg, model, train_df, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7306a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images: 954\n",
      "First 5 test files: ['img_0000.png', 'img_0001.png', 'img_0002.png', 'img_0003.png', 'img_0004.png']\n",
      "[run_inference_and_save] exp_name=test | filename='submission_test.csv'\n",
      "Saved submission to: /home/federico/Desktop/Shared/Projects/ANN/out/submission_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/federico/Desktop/Shared/Projects/ANN/out/submission_test.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Run augmented training experiment\n",
    "# Imports for augmentation pipeline\n",
    "from includes.augmentation_pipeline import run_augmented_experiment\n",
    "\n",
    "# Configure augmentation parameters (adjust as needed)\n",
    "AUG_PARAMS = {\n",
    "    \"flip_p\": 0.5,                    # Horizontal flip probability\n",
    "    \"rotation_degrees\": 15,           # Max rotation angle\n",
    "    \"use_color_jitter\": True,         # Enable color jitter\n",
    "    \"random_erasing_p\": 0.2,          # Random erasing probability\n",
    "    \"num_workers\": 0,                 # 0 for Windows notebooks\n",
    "    \"save_submission\": True,          # Save submission CSV\n",
    "    \"submission_name\": \"submission_augmented.csv\",\n",
    "}\n",
    "\n",
    "# Run complete augmented training pipeline\n",
    "aug_model, aug_history = run_augmented_experiment(\n",
    "    cfg=cfg,\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    unique_labels=unique_labels,\n",
    "    idx_to_label=idx_to_label,\n",
    "    device=device,\n",
    "    val_t=val_t,\n",
    "    **AUG_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Inference and submission\n",
    "\n",
    "test_loader, test_files = create_test_loader(cfg, val_t)\n",
    "run_inference_and_save(cfg, aug_model, test_loader, idx_to_label, device, output_csv=\"submission_augmented.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
