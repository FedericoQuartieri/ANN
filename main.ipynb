{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0effefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"resnet50_new_preprocessing_new_validation_kfold\"\n",
    "KAGGLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cef98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (KAGGLE): \n",
    "    SRC = \"/kaggle/input/challenge2/ANN\"\n",
    "    DST = \"/kaggle/working/ANN\"\n",
    "    \n",
    "    if not os.path.exists(DST):\n",
    "        shutil.copytree(SRC, DST)\n",
    "    \n",
    "    os.chdir(DST)\n",
    "    \n",
    "    if DST not in sys.path:\n",
    "        sys.path.insert(0, DST)\n",
    "    \n",
    "    print(\"cwd:\", os.getcwd())\n",
    "    print(\"data dir writable:\", os.access(os.path.join(DST, \"data\"), os.W_OK))\n",
    "    \n",
    "    \n",
    "    PROJECT_ROOT = \"/kaggle/working/ANN\"\n",
    "    os.environ[\"TORCH_HOME\"] = os.path.join(PROJECT_ROOT, \"torch\")\n",
    "    print(\"TORCH_HOME =\", os.environ[\"TORCH_HOME\"])\n",
    "    \n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    \n",
    "    print(os.getcwd())\n",
    "    print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Setup, imports, scelta esperimento\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "IN_COLAB = False\n",
    "\n",
    "if (not KAGGLE):\n",
    "    # Add local_lib to path BEFORE importing torch and other packages\n",
    "    BASE_DIR = os.getcwd()\n",
    "    LOCAL_LIB = os.path.join(BASE_DIR, 'local_lib')\n",
    "    if os.path.exists(LOCAL_LIB) and LOCAL_LIB not in sys.path:\n",
    "        sys.path.insert(0, LOCAL_LIB)\n",
    "\n",
    "\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        IN_COLAB = True\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "\n",
    "    if IN_COLAB:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        drive.mount(\"/content/drive\")\n",
    "        PROJECT_ROOT = \"/content/drive/MyDrive/[2025-2026] AN2DL/challenge2\"\n",
    "    else:\n",
    "        PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    if PROJECT_ROOT not in sys.path:\n",
    "        sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "\n",
    "# Force reload of config module to get latest changes\n",
    "import importlib\n",
    "if 'includes.config' in sys.modules:\n",
    "    importlib.reload(sys.modules['includes.config'])\n",
    "\n",
    "from includes.config import TrainingConfig, GRID_SEARCH_SPACES\n",
    "from includes.data_utils import (\n",
    "    load_labels_and_split,\n",
    "    load_full_labels,\n",
    "    get_transforms,\n",
    "    create_dataloaders,\n",
    ")\n",
    "\n",
    "from includes.model_utils import (\n",
    "    build_model,\n",
    "    create_criterion_optimizer_scheduler,\n",
    "    train_model,\n",
    "    evaluate,\n",
    ")\n",
    "from includes.inference_utils import create_test_loader, run_inference_and_save\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Verifica che la chiave esista\n",
    "print(\"\\nAvailable experiments in GRID_SEARCH_SPACES:\")\n",
    "for exp in GRID_SEARCH_SPACES.keys():\n",
    "    print(f\"  - {exp}\")\n",
    "\n",
    "# base cfg con default (paths, val_size, ecc.)\n",
    "cfg = TrainingConfig(exp_name=EXP_NAME)\n",
    "cfg.project_root = PROJECT_ROOT\n",
    "cfg.out_dir = \"out\"\n",
    "\n",
    "# griglia associata a questo esperimento\n",
    "if EXP_NAME not in GRID_SEARCH_SPACES:\n",
    "    raise KeyError(f\"Experiment '{EXP_NAME}' not found in GRID_SEARCH_SPACES. Available: {list(GRID_SEARCH_SPACES.keys())}\")\n",
    "\n",
    "param_grid = GRID_SEARCH_SPACES[EXP_NAME]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a06ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing configuration and optional run\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "from includes.config import PREPROCESSING_KEYS\n",
    "\n",
    "# Flag \"preprocessing\": if False, do not run preprocessing and do not check pp_ params\n",
    "RUN_PREPROCESSING = True  # set to False if you already have pp_* data and want to skip\n",
    "\n",
    "def build_preprocessing_config(param_grid: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extract offline preprocessing config from param_grid.\n",
    "\n",
    "    Logical constraint: for every key in PREPROCESSING_KEYS\n",
    "    that appears in the param_grid, there MUST be exactly one value\n",
    "    (no grid search on preprocessing). If not, raise an error.\n",
    "    \"\"\"\n",
    "    preproc_cfg = {}\n",
    "    for key in PREPROCESSING_KEYS:\n",
    "        if key in param_grid:\n",
    "            values = param_grid[key]\n",
    "            if not isinstance(values, (list, tuple)) or len(values) != 1:\n",
    "                raise ValueError(\n",
    "                    f\"Preprocessing parameter '{key}' must have exactly ONE value in the grid. \"\n",
    "                    f\"Current values: {values}\"\n",
    "                )\n",
    "            preproc_cfg[key] = values[0]\n",
    "    return preproc_cfg\n",
    "\n",
    "\n",
    "if RUN_PREPROCESSING:\n",
    "    # 1) Build preprocessing config from grid search config\n",
    "    preproc_cfg = build_preprocessing_config(param_grid)\n",
    "\n",
    "    print(\"========== PREPROCESSING CONFIG ==========\")\n",
    "    for k, v in preproc_cfg.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"==========================================\")\n",
    "\n",
    "    # 2) Call preprocessing/preprocessing.py passing the config as JSON\n",
    "    preproc_script = os.path.join(PROJECT_ROOT, \"preprocessing\", \"preprocessing.py\")\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,               # current Python interpreter\n",
    "        preproc_script,\n",
    "        json.dumps(preproc_cfg),      # first argument: JSON with pp_* config\n",
    "    ]\n",
    "\n",
    "    print(\"\\n>>> Running offline preprocessing...\")\n",
    "    completed = subprocess.run(cmd, check=True)\n",
    "    print(\">>> Preprocessing finished with returncode:\", completed.returncode)\n",
    "else:\n",
    "    print(\"RUN_PREPROCESSING=False: skipping preprocessing and pp_ parameter checks.\")\n",
    "\n",
    "\n",
    "if param_grid.get(\"execute\", True):\n",
    "    print(\"\\n>>> Proceeding to training phase...\")\n",
    "else:\n",
    "    print(\"\\n>>> 'execute' flag is False: skipping training phase.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# 3) Build a new param_grid for TRAINING ONLY (without preprocessing keys)\n",
    "train_param_grid = {\n",
    "    k: v for k, v in param_grid.items() \n",
    "    if k not in PREPROCESSING_KEYS and k != 'execute'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining param_grid keys:\", list(train_param_grid.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Grid search with optional StratifiedKFold (TRAINING ONLY)\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "base_cfg = cfg  # base config for this experiment (already has exp_name, project_root, etc.)\n",
    "\n",
    "# IMPORTANT: use ONLY training grid (without pp_* keys)\n",
    "keys = list(train_param_grid.keys())\n",
    "values_list = [train_param_grid[k] for k in keys]\n",
    "combos = list(itertools.product(*values_list))\n",
    "n_combos = len(combos)\n",
    "\n",
    "print(\"==============================================================\")\n",
    "print(f\"GRID SEARCH for experiment '{EXP_NAME}' with {n_combos} combinations\")\n",
    "print(\"Param keys (training only):\", keys)\n",
    "print(\"==============================================================\")\n",
    "\n",
    "results = []\n",
    "best_val_f1 = -1.0\n",
    "best_cfg = None\n",
    "best_state_dict = None\n",
    "best_idx_to_label = None\n",
    "best_val_t = None\n",
    "\n",
    "for i, values in enumerate(combos, start=1):\n",
    "    params = dict(zip(keys, values))\n",
    "\n",
    "    print(\"\\n--------------------------------------------------------------\")\n",
    "    print(f\"[Grid {i}/{n_combos}] params = {params}\")\n",
    "\n",
    "    # Validation strategy from params (fallback to holdout)\n",
    "    cv_type = params.get(\"cv_type\", \"holdout\")\n",
    "    n_splits = int(params.get(\"n_splits\", 5))\n",
    "\n",
    "    # 1) Copy base cfg and attach all params as attributes\n",
    "    cfg_i = copy.deepcopy(base_cfg)\n",
    "    for k, v in params.items():\n",
    "        setattr(cfg_i, k, v)\n",
    "\n",
    "    # 2) Common: full labels + transforms\n",
    "    labels_df, unique_labels, label_to_idx, idx_to_label_i = load_full_labels(cfg_i)\n",
    "    train_t_i, val_t_i = get_transforms(cfg_i)\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    fold_f1s = []\n",
    "    best_state_i = None\n",
    "\n",
    "    if cv_type == \"kfold\":\n",
    "        # ----- Stratified K-Fold -----\n",
    "        print(f\"  >> Using StratifiedKFold with {n_splits} folds\")\n",
    "\n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=True,\n",
    "            random_state=cfg_i.random_seed,\n",
    "        )\n",
    "\n",
    "        best_fold_f1 = -1.0\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            skf.split(labels_df, labels_df[\"label_idx\"]), start=1\n",
    "        ):\n",
    "            print(f\"    [Fold {fold}/{n_splits}]\")\n",
    "\n",
    "            train_df = labels_df.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = labels_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "            train_loader_i, val_loader_i = create_dataloaders(\n",
    "                cfg_i, train_df, val_df, train_t_i, val_t_i\n",
    "            )\n",
    "\n",
    "            model_i = build_model(cfg_i, num_classes=num_classes, device=device)\n",
    "            criterion_i, optimizer_i, scheduler_i = create_criterion_optimizer_scheduler(\n",
    "                cfg_i, model_i, train_df, device\n",
    "            )\n",
    "\n",
    "            best_state_fold, history_fold = train_model(\n",
    "                cfg_i,\n",
    "                model_i,\n",
    "                train_loader_i,\n",
    "                val_loader_i,\n",
    "                criterion_i,\n",
    "                optimizer_i,\n",
    "                scheduler_i,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            val_f1_list = history_fold.get(\"val_f1\", history_fold.get(\"val_acc\", []))\n",
    "            fold_best_f1 = max(val_f1_list) if val_f1_list else 0.0\n",
    "            fold_f1s.append(fold_best_f1)\n",
    "            print(f\"    >> Fold best F1: {fold_best_f1:.4f}\")\n",
    "\n",
    "            if fold_best_f1 > best_fold_f1:\n",
    "                best_fold_f1 = fold_best_f1\n",
    "                best_state_i = copy.deepcopy(best_state_fold)\n",
    "\n",
    "            del model_i\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        run_best_f1 = float(np.mean(fold_f1s))\n",
    "        print(f\"  >> Mean F1 over {n_splits} folds: {run_best_f1:.4f}\")\n",
    "\n",
    "    else:\n",
    "        # ----- Standard hold-out validation (current behaviour) -----\n",
    "        train_df, val_df, unique_labels, label_to_idx, idx_to_label_i = load_labels_and_split(\n",
    "            cfg_i\n",
    "        )\n",
    "        train_t_i, val_t_i = get_transforms(cfg_i)\n",
    "        train_loader_i, val_loader_i = create_dataloaders(\n",
    "            cfg_i, train_df, val_df, train_t_i, val_t_i\n",
    "        )\n",
    "\n",
    "        model_i = build_model(cfg_i, num_classes=num_classes, device=device)\n",
    "        criterion_i, optimizer_i, scheduler_i = create_criterion_optimizer_scheduler(\n",
    "            cfg_i, model_i, train_df, device\n",
    "        )\n",
    "\n",
    "        best_state_i, history_i = train_model(\n",
    "            cfg_i,\n",
    "            model_i,\n",
    "            train_loader_i,\n",
    "            val_loader_i,\n",
    "            criterion_i,\n",
    "            optimizer_i,\n",
    "            scheduler_i,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        val_f1_list = history_i.get(\"val_f1\", history_i.get(\"val_acc\", []))\n",
    "        run_best_f1 = max(val_f1_list) if val_f1_list else 0.0\n",
    "        print(f\"Best val F1 for this run: {run_best_f1:.4f}\")\n",
    "\n",
    "        del model_i\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 3) Save result row\n",
    "    row = {\"run\": i, \"best_val_f1\": run_best_f1}\n",
    "    for k in keys:\n",
    "        row[k] = params[k]\n",
    "    results.append(row)\n",
    "\n",
    "    # 4) Update global best\n",
    "    if run_best_f1 > best_val_f1:\n",
    "        best_val_f1 = run_best_f1\n",
    "        best_cfg = copy.deepcopy(cfg_i)\n",
    "        best_state_dict = copy.deepcopy(best_state_i)\n",
    "        best_idx_to_label = idx_to_label_i\n",
    "        best_val_t = val_t_i\n",
    "\n",
    "# ---------- grid search summary ----------\n",
    "results_df = pd.DataFrame(results).sort_values(\"best_val_f1\", ascending=False)\n",
    "print(\"\\n================ GRID SEARCH SUMMARY ================\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nBest config (from grid):\")\n",
    "print(best_cfg.__dict__)\n",
    "print(\"Best val F1:\", best_val_f1)\n",
    "\n",
    "# ---------- objects for the rest of the notebook ----------\n",
    "cfg = best_cfg\n",
    "idx_to_label = best_idx_to_label\n",
    "val_t = best_val_t\n",
    "\n",
    "num_classes = len(idx_to_label)\n",
    "model = build_model(cfg, num_classes=num_classes, device=device)\n",
    "model.load_state_dict(best_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST SPLIT CHECK (robusto: ricava case_id da sample_index)\n",
    "\n",
    "import re\n",
    "\n",
    "def _case_id_from_name(name: str) -> str:\n",
    "    # match \"img_0002\" from \"img_0002_k11_aug0.png\"\n",
    "    m = re.match(r\"^(img_\\d+)\", str(name))\n",
    "    return m.group(1) if m else str(name).split(\"_\")[0]\n",
    "\n",
    "for _df, _tag in [(train_df, \"TRAIN\"), (val_df, \"VAL\")]:\n",
    "    _df[\"case_id_dbg\"] = _df[\"sample_index\"].apply(_case_id_from_name)\n",
    "    _df[\"is_aug_dbg\"]  = _df[\"sample_index\"].astype(str).str.contains(r\"_aug\\d+\", regex=True)\n",
    "\n",
    "print(\"===== FAST SPLIT CHECK =====\")\n",
    "print(f\"Train rows: {len(train_df)}\")\n",
    "print(f\"Val   rows: {len(val_df)}\")\n",
    "\n",
    "overlap_cases = set(train_df[\"case_id_dbg\"]) & set(val_df[\"case_id_dbg\"])\n",
    "print(f\"Case_id overlap (should be 0): {len(overlap_cases)}\")\n",
    "if len(overlap_cases) > 0:\n",
    "    print(\"Overlap examples:\", list(sorted(overlap_cases))[:10])\n",
    "\n",
    "aug_in_val = int(val_df[\"is_aug_dbg\"].sum())\n",
    "print(f\"Augmented images in VAL (should be 0): {aug_in_val}\")\n",
    "if aug_in_val > 0:\n",
    "    print(\"Aug-in-val examples:\", val_df.loc[val_df[\"is_aug_dbg\"], \"sample_index\"].head(10).tolist())\n",
    "\n",
    "print(\"===== END CHECK =====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb61bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Diagnostics & Confusion Matrix for best config\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ricrea train/val split e dataloader per la best cfg\n",
    "train_df, val_df, unique_labels, label_to_idx, idx_to_label = load_labels_and_split(cfg)\n",
    "train_t_diag, val_t_diag = get_transforms(cfg)\n",
    "train_loader_diag, val_loader_diag = create_dataloaders(\n",
    "    cfg, train_df, val_df, train_t_diag, val_t_diag\n",
    ")\n",
    "\n",
    "# ---------- CLASS DISTRIBUTION ----------\n",
    "print(\"=== CLASS DISTRIBUTION ===\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(train_df[\"label\"].value_counts().sort_index())\n",
    "print(f\"Total train samples: {len(train_df)}\")\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "print(val_df[\"label\"].value_counts().sort_index())\n",
    "print(f\"Total val samples: {len(val_df)}\")\n",
    "\n",
    "# ---------- TRAINING CONFIG ----------\n",
    "print(\"\\n=== TRAINING CONFIG ===\")\n",
    "print(f\"Experiment name: {cfg.exp_name}\")\n",
    "print(f\"Backbone     : {cfg.backbone}\")\n",
    "print(f\"Image size   : {cfg.img_size}\")\n",
    "print(f\"Learning rate: {cfg.lr}\")\n",
    "print(f\"Epochs       : {cfg.epochs}\")\n",
    "print(f\"Batch size   : {cfg.batch_size}\")\n",
    "print(f\"Use masks    : {cfg.use_masks}\")\n",
    "print(f\"Mask mode    : {cfg.mask_mode if cfg.use_masks else 'N/A'}\")\n",
    "print(f\"Augmentation : {getattr(cfg, 'augmentation', 'strong')}\")\n",
    "\n",
    "# ---------- CLASS WEIGHTS ----------\n",
    "class_counts = train_df[\"label_idx\"].value_counts().sort_index().values.astype(float)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "\n",
    "print(\"\\n=== CLASS WEIGHTS ===\")\n",
    "for i, (label, weight) in enumerate(zip(unique_labels, class_weights)):\n",
    "    print(f\"{label}: {weight:.4f} (count: {int(class_counts[i])})\")\n",
    "\n",
    "# ---------- Confusion matrix on validation set ----------\n",
    "print(\"\\n=== CONFUSION MATRIX (validation, best model) ===\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_diag:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix - Best model on Validation Set\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# tieni idx_to_label aggiornato per l'inference\n",
    "idx_to_label = idx_to_label\n",
    "val_t = val_t_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell X - Sanity checks for train/val splits and masks\n",
    "\n",
    "import os\n",
    "\n",
    "from includes.data_utils import build_paths\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"SANITY CHECK: splits, image dirs, and masks\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# --- Paths from cfg/build_paths ---\n",
    "train_img_dir, test_img_dir, mask_dir = build_paths(cfg)\n",
    "\n",
    "print(\"\\n[CONFIG PATHS]\")\n",
    "print(f\"cfg.train_img_dir -> {cfg.train_img_dir}\")\n",
    "print(f\"cfg.test_img_dir  -> {cfg.test_img_dir}\")\n",
    "val_img_dir_cfg = getattr(cfg, \"val_img_dir\", None)\n",
    "print(f\"cfg.val_img_dir   -> {val_img_dir_cfg}\")\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir : {test_img_dir}\")\n",
    "print(f\"Resolved mask_dir     : {mask_dir}\")\n",
    "\n",
    "# --- Check that train_df and val_df use disjoint sample_index ---\n",
    "train_names = set(train_df[\"sample_index\"].tolist())\n",
    "val_names = set(val_df[\"sample_index\"].tolist())\n",
    "overlap = train_names & val_names\n",
    "\n",
    "print(\"\\n[SPLIT CHECK]\")\n",
    "print(f\"Unique train samples: {len(train_names)}\")\n",
    "print(f\"Unique val samples  : {len(val_names)}\")\n",
    "print(f\"Overlap (train ∩ val): {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(\"WARNING: these files are in BOTH train and val (showing up to 20):\")\n",
    "    print(sorted(list(overlap))[:20])\n",
    "else:\n",
    "    print(\"OK: train and val image sets are disjoint.\")\n",
    "\n",
    "# --- Inspect dataset dirs actually used by the loaders ---\n",
    "train_ds = train_loader_diag.dataset\n",
    "val_ds = val_loader_diag.dataset\n",
    "\n",
    "print(\"\\n[DATASET DIRS USED BY DATALOADERS]\")\n",
    "print(f\"Train loader img_dir: {getattr(train_ds, 'img_dir', 'N/A')}\")\n",
    "print(f\"Train loader mask_dir: {getattr(train_ds, 'mask_dir', 'N/A')}\")\n",
    "print(f\"Val loader img_dir  : {getattr(val_ds, 'img_dir', 'N/A')}\")\n",
    "print(f\"Val loader mask_dir : {getattr(val_ds, 'mask_dir', 'N/A')}\")\n",
    "\n",
    "# --- Check that masks exist for each split (based on sample_index -> mask_* mapping) ---\n",
    "def check_masks_for_split(df, dataset, split_name: str, max_examples: int = 10):\n",
    "    \"\"\"Check that for each image there is a corresponding mask_* file.\"\"\"\n",
    "    base_mask_dir = getattr(dataset, \"mask_dir\", None)\n",
    "    if base_mask_dir is None:\n",
    "        print(f\"\\n[MASK CHECK - {split_name}] Dataset has no mask_dir attribute, skipping.\")\n",
    "        return\n",
    "\n",
    "    missing = []\n",
    "    for name in df[\"sample_index\"]:\n",
    "        # Derive mask filename from image filename\n",
    "        if name.startswith(\"img_\"):\n",
    "            mask_name = \"mask_\" + name[4:]\n",
    "        else:\n",
    "            mask_name = name.replace(\"img\", \"mask\")\n",
    "\n",
    "        mask_path = os.path.join(base_mask_dir, mask_name)\n",
    "        if not os.path.exists(mask_path):\n",
    "            missing.append(mask_name)\n",
    "\n",
    "    print(f\"\\n[MASK CHECK - {split_name}]\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Missing masks: {len(missing)}\")\n",
    "    if missing:\n",
    "        print(\"First missing mask files:\", missing[:max_examples])\n",
    "    else:\n",
    "        print(\"OK: all expected mask_* files exist for this split.\")\n",
    "\n",
    "check_masks_for_split(train_df, train_ds, \"train\")\n",
    "check_masks_for_split(val_df, val_ds, \"val\")\n",
    "\n",
    "print(\"\\n[SUMMARY]\")\n",
    "print(\"If:\")\n",
    "print(\" - train/val overlap is 0,\")\n",
    "print(\" - train loader img_dir == pp_train_data (se stai usando il pp),\")\n",
    "print(\" - val loader img_dir   == train_data (se hai impostato val_img_dir),\")\n",
    "print(\" - e nessuna mask risulta missing,\")\n",
    "print(\"allora split e mask sono coerenti ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb07622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Inference and submission (best model from grid search)\n",
    "\n",
    "test_loader, test_files = create_test_loader(cfg, val_t)\n",
    "\n",
    "submission_name = f\"submission_{cfg.exp_name}.csv\"\n",
    "\n",
    "run_inference_and_save(\n",
    "    cfg,\n",
    "    model,\n",
    "    test_loader,\n",
    "    idx_to_label,\n",
    "    device,\n",
    "    output_csv=submission_name,\n",
    ")\n",
    "\n",
    "print(\"outout saved in:\", submission_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a099a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM Visualization\n",
    "\n",
    "from includes.cam_utils import visualize_gradcam_for_validation\n",
    "\n",
    "print(\"Generating Grad-CAM visualizations for validation set...\")\n",
    "visualize_gradcam_for_validation(\n",
    "    model=model,\n",
    "    val_loader=val_loader_diag,\n",
    "    idx_to_label=idx_to_label,\n",
    "    device=device,\n",
    "    num_samples=10,\n",
    "    save_dir=os.path.join(cfg.out_dir, \"gradcam\"),\n",
    "    exp_name=cfg.exp_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c910e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (IN_COLAB):\n",
    "    full_submission_path = os.path.join(PROJECT_ROOT, cfg.out_dir, submission_name)\n",
    "    from google.colab import files\n",
    "    files.download(full_submission_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
