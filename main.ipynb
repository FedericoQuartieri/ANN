{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0effefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c882e86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/federico/Desktop/Shared/Projects/ANN\n",
      "Working dir: /home/federico/Desktop/Shared/Projects/ANN\n",
      "Using device: cpu\n",
      "\n",
      "Available experiments in GRID_SEARCH_SPACES:\n",
      "  - preprocessing\n",
      "  - test\n",
      "  - resnet50_img384\n",
      "  - resnet50_img384_preprocessing\n",
      "  - resnet50_img384_preprocessing_augementation_test\n",
      "  - resnet50_img384_preprocessing_augementation_10_copies\n",
      "  - resnet50_kfold_sweep\n",
      "  - resnet50_img384_preprocessing_crop_bbox\n",
      "  - resnet50_img384_pp_crop_bbox_offaug2_light\n",
      "  - resnet50_img384_pp_crop_bbox_offaug4_strong\n",
      "  - resnet50_img384_pp_crop_bbox_offaug2_soft\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_img_dir': ['pp_train_data'],\n",
       " 'test_img_dir': ['pp_test_data'],\n",
       " 'labels_csv': ['pp_train_labels.csv'],\n",
       " 'pp_remove_shrek': [True],\n",
       " 'pp_fix_stained': [True],\n",
       " 'pp_split_doubles': [True],\n",
       " 'pp_remove_black_rect': [True],\n",
       " 'pp_padding_square': [False],\n",
       " 'pp_crop_to_mask': [False],\n",
       " 'pp_resize_and_normalize': [True],\n",
       " 'pp_augmentation_enabled': [True],\n",
       " 'pp_crop_padding': [10],\n",
       " 'pp_target_size': [384],\n",
       " 'pp_apply_clahe': [False],\n",
       " 'pp_clahe_clip_limit': [2.0],\n",
       " 'pp_clahe_tile_grid': [(16, 16)],\n",
       " 'pp_num_aug_copies': [1],\n",
       " 'pp_strong_rotation_degrees': [15],\n",
       " 'pp_strong_zoom_min': [0.8],\n",
       " 'pp_strong_zoom_max': [1.0],\n",
       " 'pp_strong_brightness': [0.2],\n",
       " 'pp_strong_contrast': [0.2],\n",
       " 'pp_strong_saturation': [0.2],\n",
       " 'pp_strong_hue': [0.05],\n",
       " 'pp_strong_random_erasing_p': [0.1],\n",
       " 'execute': [True],\n",
       " 'backbone': ['resnet50'],\n",
       " 'img_size': [384],\n",
       " 'batch_size': [16],\n",
       " 'num_workers': [4],\n",
       " 'lr': [0.0001],\n",
       " 'weight_decay': [0.0001],\n",
       " 'epochs': [1],\n",
       " 'use_scheduler': [True],\n",
       " 'use_masks': [True],\n",
       " 'mask_mode': ['multiply'],\n",
       " 'cv_type': ['holdout'],\n",
       " 'n_splits': [5],\n",
       " 'val_size': [0.2],\n",
       " 'val_img_dir': ['pp_train_data']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 - Setup, imports, scelta esperimento\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add local_lib to path BEFORE importing torch and other packages\n",
    "BASE_DIR = os.getcwd()\n",
    "LOCAL_LIB = os.path.join(BASE_DIR, 'local_lib')\n",
    "if os.path.exists(LOCAL_LIB) and LOCAL_LIB not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_LIB)\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = \"/content/drive/MyDrive/[2025-2026] AN2DL/challenge2\"\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "\n",
    "# Force reload of config module to get latest changes\n",
    "import importlib\n",
    "if 'includes.config' in sys.modules:\n",
    "    importlib.reload(sys.modules['includes.config'])\n",
    "\n",
    "from includes.config import TrainingConfig, GRID_SEARCH_SPACES\n",
    "from includes.data_utils import (\n",
    "    load_labels_and_split,\n",
    "    load_full_labels,\n",
    "    get_transforms,\n",
    "    create_dataloaders,\n",
    ")\n",
    "\n",
    "from includes.model_utils import (\n",
    "    build_model,\n",
    "    create_criterion_optimizer_scheduler,\n",
    "    train_model,\n",
    "    evaluate,\n",
    ")\n",
    "from includes.inference_utils import create_test_loader, run_inference_and_save\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Verifica che la chiave esista\n",
    "print(\"\\nAvailable experiments in GRID_SEARCH_SPACES:\")\n",
    "for exp in GRID_SEARCH_SPACES.keys():\n",
    "    print(f\"  - {exp}\")\n",
    "\n",
    "# base cfg con default (paths, val_size, ecc.)\n",
    "cfg = TrainingConfig(exp_name=EXP_NAME)\n",
    "cfg.project_root = PROJECT_ROOT\n",
    "cfg.out_dir = \"out\"\n",
    "\n",
    "# griglia associata a questo esperimento\n",
    "if EXP_NAME not in GRID_SEARCH_SPACES:\n",
    "    raise KeyError(f\"Experiment '{EXP_NAME}' not found in GRID_SEARCH_SPACES. Available: {list(GRID_SEARCH_SPACES.keys())}\")\n",
    "\n",
    "param_grid = GRID_SEARCH_SPACES[EXP_NAME]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a06ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PREPROCESSING CONFIG ==========\n",
      "pp_remove_shrek: True\n",
      "pp_fix_stained: True\n",
      "pp_split_doubles: True\n",
      "pp_remove_black_rect: True\n",
      "pp_padding_square: False\n",
      "pp_crop_to_mask: False\n",
      "pp_resize_and_normalize: True\n",
      "pp_augmentation_enabled: True\n",
      "pp_crop_padding: 10\n",
      "pp_target_size: 384\n",
      "pp_apply_clahe: False\n",
      "pp_clahe_clip_limit: 2.0\n",
      "pp_clahe_tile_grid: (16, 16)\n",
      "pp_num_aug_copies: 1\n",
      "pp_strong_rotation_degrees: 15\n",
      "pp_strong_zoom_min: 0.8\n",
      "pp_strong_zoom_max: 1.0\n",
      "pp_strong_brightness: 0.2\n",
      "pp_strong_contrast: 0.2\n",
      "pp_strong_saturation: 0.2\n",
      "pp_strong_hue: 0.05\n",
      "pp_strong_random_erasing_p: 0.1\n",
      "==========================================\n",
      "\n",
      ">>> Running offline preprocessing...\n",
      "\n",
      "================================================================================\n",
      " External config from ANN notebook\n",
      "================================================================================\n",
      "  pp_remove_shrek = True\n",
      "  pp_fix_stained = True\n",
      "  pp_split_doubles = True\n",
      "  pp_remove_black_rect = True\n",
      "  pp_padding_square = False\n",
      "  pp_crop_to_mask = False\n",
      "  pp_resize_and_normalize = True\n",
      "  pp_augmentation_enabled = True\n",
      "  pp_crop_padding = 10\n",
      "  pp_target_size = 384\n",
      "  pp_apply_clahe = False\n",
      "  pp_clahe_clip_limit = 2.0\n",
      "  pp_clahe_tile_grid = [16, 16]\n",
      "  pp_num_aug_copies = 1\n",
      "  pp_strong_rotation_degrees = 15\n",
      "  pp_strong_zoom_min = 0.8\n",
      "  pp_strong_zoom_max = 1.0\n",
      "  pp_strong_brightness = 0.2\n",
      "  pp_strong_contrast = 0.2\n",
      "  pp_strong_saturation = 0.2\n",
      "  pp_strong_hue = 0.05\n",
      "  pp_strong_random_erasing_p = 0.1\n",
      "================================================================================\n",
      " PREPROCESSING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "This script will preprocess both training and test datasets.\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# PART 1: TRAINING DATASET\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      " Dataset Preprocessing Started\n",
      "================================================================================\n",
      "Source: /home/federico/Desktop/Shared/Projects/ANN/data/train_data\n",
      "Output: /home/federico/Desktop/Shared/Projects/ANN/data/pp_train_data\n",
      "\n",
      "Loading training labels...\n",
      "Loaded 691 samples\n",
      "\n",
      "================================================================================\n",
      " Building Exclusion List\n",
      "================================================================================\n",
      "✓ Shrek contaminated images: 110\n",
      "✓ Stained images to fix: 50\n",
      "✓ Double images to split: 14\n",
      "✓ Black addition images to crop: 1\n",
      "\n",
      ">>> TOTAL IMAGES TO EXCLUDE: 110\n",
      "\n",
      "================================================================================\n",
      " Filtering Dataset\n",
      "================================================================================\n",
      "Shrek: 110 images will be removed\n",
      "\n",
      "Original dataset: 691 images\n",
      "Filtered dataset: 581 images\n",
      "Removed: 110 images (15.92%)\n",
      "\n",
      "================================================================================\n",
      " Copying Preprocessed Files\n",
      "================================================================================\n",
      "✓ Clearing existing directory: /home/federico/Desktop/Shared/Projects/ANN/data/pp_train_data\n",
      "✓ Created clean directory: /home/federico/Desktop/Shared/Projects/ANN/data/pp_train_data\n",
      "\n",
      "Processing 581 valid image-mask pairs...\n",
      "  Progress: 200/581\n",
      "  Progress: 400/581\n",
      "\n",
      "✓ Successfully processed 581 image-mask pairs\n",
      "✓ Split 11 double images into 22 separate images\n",
      "✓ Removed black rectangles from 1 images\n",
      "\n",
      "================================================================================\n",
      " Resizing and Normalizing Images\n",
      "================================================================================\n",
      "Target size: 384x384\n",
      "CLAHE contrast enhancement: Disabled\n",
      "\n",
      "Processing 581 images for resizing...\n",
      "  Progress: 100/581\n",
      "  Progress: 200/581\n",
      "  Progress: 300/581\n",
      "  Progress: 400/581\n",
      "  Progress: 500/581\n",
      "\n",
      "✓ Resized and normalized 581 image-mask pairs\n",
      "\n",
      "================================================================================\n",
      " Data Augmentation\n",
      "================================================================================\n",
      "Creating 1 augmented copies per image with random transformations\n",
      "Rotation jitter (±deg): 15\n",
      "Zoom scale range      : (0.8, 1.0)\n",
      "Brightness jitter     : 0.2\n",
      "Contrast jitter       : 0.2\n",
      "Saturation jitter     : 0.2\n",
      "Hue jitter            : 0.05\n",
      "Random erasing prob   : 0.1\n",
      "\n",
      "Original dataset size: 581 images\n",
      "Generating augmented images...\n",
      "  Progress: 100/581\n",
      "  Progress: 200/581\n",
      "  Progress: 300/581\n",
      "  Progress: 400/581\n",
      "  Progress: 500/581\n",
      "\n",
      "✓ Generated 581 augmented images\n",
      "✓ Total dataset size: 1162 images (2.0x augmentation)\n",
      "\n",
      "================================================================================\n",
      " Verification\n",
      "================================================================================\n",
      "\n",
      "Verifying Shrek exclusion (checking 5 samples):\n",
      "  img_0424.png: ✓ Not found (correct)\n",
      "  img_0044.png: ✓ Not found (correct)\n",
      "  img_0635.png: ✓ Not found (correct)\n",
      "  img_0521.png: ✓ Not found (correct)\n",
      "  img_0533.png: ✓ Not found (correct)\n",
      "\n",
      "✓ Verification passed - excluded images not in output\n",
      "\n",
      "================================================================================\n",
      " Saving Preprocessed Labels\n",
      "================================================================================\n",
      "✓ Saved to: /home/federico/Desktop/Shared/Projects/ANN/data/pp_train_labels.csv\n",
      "✓ Total samples: 1162\n",
      "✓ Verification: CSV file is correct\n",
      "\n",
      "================================================================================\n",
      " Preprocessing Summary\n",
      "================================================================================\n",
      "Original dataset: 691 samples\n",
      "Preprocessed dataset: 1162 samples\n",
      "Removed: -471 samples\n",
      "\n",
      "Preprocessing steps applied:\n",
      "  - Shrek: 110 images removed\n",
      "  - Doubles: 11 images cropped to valid region (kept non-black masks)\n",
      "  - Black rectangles: 1 images cropped\n",
      "  - Resized: All images normalized to 384x384\n",
      "  - Augmentation: 1 random augmented copies per image\n",
      "\n",
      "Preprocessed data saved to:\n",
      "  - Images: /home/federico/Desktop/Shared/Projects/ANN/data/pp_train_data\n",
      "  - Labels: /home/federico/Desktop/Shared/Projects/ANN/data/pp_train_labels.csv\n",
      "\n",
      "Label distribution in preprocessed dataset:\n",
      "  Luminal B: 408 (35.11%)\n",
      "  Luminal A: 316 (27.19%)\n",
      "  HER2(+): 300 (25.82%)\n",
      "  Triple negative: 138 (11.88%)\n",
      "\n",
      "================================================================================\n",
      " Preprocessing Complete\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# PART 2: TEST DATASET\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      " Test Dataset Preprocessing\n",
      "================================================================================\n",
      "Source: /home/federico/Desktop/Shared/Projects/ANN/data/test_data\n",
      "Output: /home/federico/Desktop/Shared/Projects/ANN/data/pp_test_data\n",
      "\n",
      "================================================================================\n",
      " Loading Configuration\n",
      "================================================================================\n",
      "✓ Double images to split: 7\n",
      "\n",
      "================================================================================\n",
      " Processing Test Files\n",
      "================================================================================\n",
      "✓ Clearing existing directory: /home/federico/Desktop/Shared/Projects/ANN/data/pp_test_data\n",
      "✓ Created clean directory: /home/federico/Desktop/Shared/Projects/ANN/data/pp_test_data\n",
      "\n",
      "Processing 477 test images...\n",
      "  Progress: 200/477\n",
      "  Progress: 400/477\n",
      "\n",
      "✓ Successfully processed 477 image-mask pairs\n",
      "✓ Split 7 double images (kept non-black mask regions)\n",
      "\n",
      "================================================================================\n",
      " Resizing and Normalizing Images\n",
      "================================================================================\n",
      "Target size: 384x384\n",
      "CLAHE contrast enhancement: Disabled\n",
      "\n",
      "Processing 477 images for resizing...\n",
      "  Progress: 100/477\n",
      "  Progress: 200/477\n",
      "  Progress: 300/477\n",
      "  Progress: 400/477\n",
      "\n",
      "✓ Resized and normalized 477 image-mask pairs\n",
      "\n",
      "================================================================================\n",
      " Test Preprocessing Summary\n",
      "================================================================================\n",
      "Total test images processed: 477\n",
      "\n",
      "Preprocessing steps applied:\n",
      "  - Doubles: 7 images cropped to valid region (kept non-black masks)\n",
      "  - Resized: All images normalized to 384x384\n",
      "\n",
      "Preprocessed test data saved to: /home/federico/Desktop/Shared/Projects/ANN/data/pp_test_data\n",
      "\n",
      "================================================================================\n",
      " Test Preprocessing Complete\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      " PREPROCESSING PIPELINE COMPLETE\n",
      "================================================================================\n",
      "Training dataset: ✓ Success\n",
      "Test dataset: ✓ Success\n",
      "================================================================================\n",
      ">>> Preprocessing finished with returncode: 0\n",
      "\n",
      ">>> Proceeding to training phase...\n",
      "\n",
      "Training param_grid keys: ['train_img_dir', 'test_img_dir', 'labels_csv', 'backbone', 'img_size', 'batch_size', 'num_workers', 'lr', 'weight_decay', 'epochs', 'use_scheduler', 'use_masks', 'mask_mode', 'cv_type', 'n_splits', 'val_size', 'val_img_dir']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing configuration and optional run\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "from includes.config import PREPROCESSING_KEYS\n",
    "\n",
    "# Flag \"preprocessing\": if False, do not run preprocessing and do not check pp_ params\n",
    "RUN_PREPROCESSING = True  # set to False if you already have pp_* data and want to skip\n",
    "\n",
    "def build_preprocessing_config(param_grid: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extract offline preprocessing config from param_grid.\n",
    "\n",
    "    Logical constraint: for every key in PREPROCESSING_KEYS\n",
    "    that appears in the param_grid, there MUST be exactly one value\n",
    "    (no grid search on preprocessing). If not, raise an error.\n",
    "    \"\"\"\n",
    "    preproc_cfg = {}\n",
    "    for key in PREPROCESSING_KEYS:\n",
    "        if key in param_grid:\n",
    "            values = param_grid[key]\n",
    "            if not isinstance(values, (list, tuple)) or len(values) != 1:\n",
    "                raise ValueError(\n",
    "                    f\"Preprocessing parameter '{key}' must have exactly ONE value in the grid. \"\n",
    "                    f\"Current values: {values}\"\n",
    "                )\n",
    "            preproc_cfg[key] = values[0]\n",
    "    return preproc_cfg\n",
    "\n",
    "\n",
    "if RUN_PREPROCESSING:\n",
    "    # 1) Build preprocessing config from grid search config\n",
    "    preproc_cfg = build_preprocessing_config(param_grid)\n",
    "\n",
    "    print(\"========== PREPROCESSING CONFIG ==========\")\n",
    "    for k, v in preproc_cfg.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"==========================================\")\n",
    "\n",
    "    # 2) Call preprocessing/preprocessing.py passing the config as JSON\n",
    "    preproc_script = os.path.join(PROJECT_ROOT, \"preprocessing\", \"preprocessing.py\")\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,               # current Python interpreter\n",
    "        preproc_script,\n",
    "        json.dumps(preproc_cfg),      # first argument: JSON with pp_* config\n",
    "    ]\n",
    "\n",
    "    print(\"\\n>>> Running offline preprocessing...\")\n",
    "    completed = subprocess.run(cmd, check=True)\n",
    "    print(\">>> Preprocessing finished with returncode:\", completed.returncode)\n",
    "else:\n",
    "    print(\"RUN_PREPROCESSING=False: skipping preprocessing and pp_ parameter checks.\")\n",
    "\n",
    "\n",
    "if param_grid.get(\"execute\", True):\n",
    "    print(\"\\n>>> Proceeding to training phase...\")\n",
    "else:\n",
    "    print(\"\\n>>> 'execute' flag is False: skipping training phase.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# 3) Build a new param_grid for TRAINING ONLY (without preprocessing keys)\n",
    "train_param_grid = {\n",
    "    k: v for k, v in param_grid.items() \n",
    "    if k not in PREPROCESSING_KEYS and k != 'execute'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining param_grid keys:\", list(train_param_grid.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9472f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "GRID SEARCH for experiment 'test' with 1 combinations\n",
      "Param keys (training only): ['train_img_dir', 'test_img_dir', 'labels_csv', 'backbone', 'img_size', 'batch_size', 'num_workers', 'lr', 'weight_decay', 'epochs', 'use_scheduler', 'use_masks', 'mask_mode', 'cv_type', 'n_splits', 'val_size', 'val_img_dir']\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Grid 1/1] params = {'train_img_dir': 'pp_train_data', 'test_img_dir': 'pp_test_data', 'labels_csv': 'pp_train_labels.csv', 'backbone': 'resnet50', 'img_size': 384, 'batch_size': 16, 'num_workers': 4, 'lr': 0.0001, 'weight_decay': 0.0001, 'epochs': 1, 'use_scheduler': True, 'use_masks': True, 'mask_mode': 'multiply', 'cv_type': 'holdout', 'n_splits': 5, 'val_size': 0.2, 'val_img_dir': 'pp_train_data'}\n",
      "==============================================================\n",
      "Starting training - experiment: test\n",
      "Backbone: resnet50  |  img_size: 384  |  epochs: 1\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/1]\n",
      "    [Batch   1/59] loss=1.4362  f1=0.0714\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    119\u001b[39m model_i = build_model(cfg_i, num_classes=num_classes, device=device)\n\u001b[32m    120\u001b[39m criterion_i, optimizer_i, scheduler_i = create_criterion_optimizer_scheduler(\n\u001b[32m    121\u001b[39m     cfg_i, model_i, train_df, device\n\u001b[32m    122\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m best_state_i, history_i = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m val_f1_list = history_i.get(\u001b[33m\"\u001b[39m\u001b[33mval_f1\u001b[39m\u001b[33m\"\u001b[39m, history_i.get(\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m, []))\n\u001b[32m    136\u001b[39m run_best_f1 = \u001b[38;5;28mmax\u001b[39m(val_f1_list) \u001b[38;5;28;01mif\u001b[39;00m val_f1_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/includes/model_utils.py:255\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(cfg, model, train_loader, val_loader, criterion, optimizer, scheduler, device)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# barra di progresso per il training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m train_loss, train_f1 = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m val_loss, val_f1, val_true, val_pred = evaluate(\n\u001b[32m    265\u001b[39m     model, val_loader, criterion, device\n\u001b[32m    266\u001b[39m )\n\u001b[32m    268\u001b[39m history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/includes/model_utils.py:127\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, epoch, num_epochs)\u001b[39m\n\u001b[32m    123\u001b[39m labels = labels.to(device)\n\u001b[32m    125\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m    129\u001b[39m _, preds = torch.max(outputs, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torchvision/models/resnet.py:276\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    274\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer2(x)\n\u001b[32m    275\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer3(x)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n\u001b[32m    279\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torchvision/models/resnet.py:158\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    155\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn3(out)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     identity = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m out += identity\n\u001b[32m    161\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Shared/Projects/ANN/env/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 2 - Grid search with optional StratifiedKFold (TRAINING ONLY)\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "base_cfg = cfg  # base config for this experiment (already has exp_name, project_root, etc.)\n",
    "\n",
    "# IMPORTANT: use ONLY training grid (without pp_* keys)\n",
    "keys = list(train_param_grid.keys())\n",
    "values_list = [train_param_grid[k] for k in keys]\n",
    "combos = list(itertools.product(*values_list))\n",
    "n_combos = len(combos)\n",
    "\n",
    "print(\"==============================================================\")\n",
    "print(f\"GRID SEARCH for experiment '{EXP_NAME}' with {n_combos} combinations\")\n",
    "print(\"Param keys (training only):\", keys)\n",
    "print(\"==============================================================\")\n",
    "\n",
    "results = []\n",
    "best_val_f1 = -1.0\n",
    "best_cfg = None\n",
    "best_state_dict = None\n",
    "best_idx_to_label = None\n",
    "best_val_t = None\n",
    "\n",
    "for i, values in enumerate(combos, start=1):\n",
    "    params = dict(zip(keys, values))\n",
    "\n",
    "    print(\"\\n--------------------------------------------------------------\")\n",
    "    print(f\"[Grid {i}/{n_combos}] params = {params}\")\n",
    "\n",
    "    # Validation strategy from params (fallback to holdout)\n",
    "    cv_type = params.get(\"cv_type\", \"holdout\")\n",
    "    n_splits = int(params.get(\"n_splits\", 5))\n",
    "\n",
    "    # 1) Copy base cfg and attach all params as attributes\n",
    "    cfg_i = copy.deepcopy(base_cfg)\n",
    "    for k, v in params.items():\n",
    "        setattr(cfg_i, k, v)\n",
    "\n",
    "    # 2) Common: full labels + transforms\n",
    "    labels_df, unique_labels, label_to_idx, idx_to_label_i = load_full_labels(cfg_i)\n",
    "    train_t_i, val_t_i = get_transforms(cfg_i)\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    fold_f1s = []\n",
    "    best_state_i = None\n",
    "\n",
    "    if cv_type == \"kfold\":\n",
    "        # ----- Stratified K-Fold -----\n",
    "        print(f\"  >> Using StratifiedKFold with {n_splits} folds\")\n",
    "\n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=True,\n",
    "            random_state=cfg_i.random_seed,\n",
    "        )\n",
    "\n",
    "        best_fold_f1 = -1.0\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            skf.split(labels_df, labels_df[\"label_idx\"]), start=1\n",
    "        ):\n",
    "            print(f\"    [Fold {fold}/{n_splits}]\")\n",
    "\n",
    "            train_df = labels_df.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = labels_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "            train_loader_i, val_loader_i = create_dataloaders(\n",
    "                cfg_i, train_df, val_df, train_t_i, val_t_i\n",
    "            )\n",
    "\n",
    "            model_i = build_model(cfg_i, num_classes=num_classes, device=device)\n",
    "            criterion_i, optimizer_i, scheduler_i = create_criterion_optimizer_scheduler(\n",
    "                cfg_i, model_i, train_df, device\n",
    "            )\n",
    "\n",
    "            best_state_fold, history_fold = train_model(\n",
    "                cfg_i,\n",
    "                model_i,\n",
    "                train_loader_i,\n",
    "                val_loader_i,\n",
    "                criterion_i,\n",
    "                optimizer_i,\n",
    "                scheduler_i,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            val_f1_list = history_fold.get(\"val_f1\", history_fold.get(\"val_acc\", []))\n",
    "            fold_best_f1 = max(val_f1_list) if val_f1_list else 0.0\n",
    "            fold_f1s.append(fold_best_f1)\n",
    "            print(f\"    >> Fold best F1: {fold_best_f1:.4f}\")\n",
    "\n",
    "            if fold_best_f1 > best_fold_f1:\n",
    "                best_fold_f1 = fold_best_f1\n",
    "                best_state_i = copy.deepcopy(best_state_fold)\n",
    "\n",
    "            del model_i\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        run_best_f1 = float(np.mean(fold_f1s))\n",
    "        print(f\"  >> Mean F1 over {n_splits} folds: {run_best_f1:.4f}\")\n",
    "\n",
    "    else:\n",
    "        # ----- Standard hold-out validation (current behaviour) -----\n",
    "        train_df, val_df, unique_labels, label_to_idx, idx_to_label_i = load_labels_and_split(\n",
    "            cfg_i\n",
    "        )\n",
    "        train_t_i, val_t_i = get_transforms(cfg_i)\n",
    "        train_loader_i, val_loader_i = create_dataloaders(\n",
    "            cfg_i, train_df, val_df, train_t_i, val_t_i\n",
    "        )\n",
    "\n",
    "        model_i = build_model(cfg_i, num_classes=num_classes, device=device)\n",
    "        criterion_i, optimizer_i, scheduler_i = create_criterion_optimizer_scheduler(\n",
    "            cfg_i, model_i, train_df, device\n",
    "        )\n",
    "\n",
    "        best_state_i, history_i = train_model(\n",
    "            cfg_i,\n",
    "            model_i,\n",
    "            train_loader_i,\n",
    "            val_loader_i,\n",
    "            criterion_i,\n",
    "            optimizer_i,\n",
    "            scheduler_i,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        val_f1_list = history_i.get(\"val_f1\", history_i.get(\"val_acc\", []))\n",
    "        run_best_f1 = max(val_f1_list) if val_f1_list else 0.0\n",
    "        print(f\"Best val F1 for this run: {run_best_f1:.4f}\")\n",
    "\n",
    "        del model_i\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 3) Save result row\n",
    "    row = {\"run\": i, \"best_val_f1\": run_best_f1}\n",
    "    for k in keys:\n",
    "        row[k] = params[k]\n",
    "    results.append(row)\n",
    "\n",
    "    # 4) Update global best\n",
    "    if run_best_f1 > best_val_f1:\n",
    "        best_val_f1 = run_best_f1\n",
    "        best_cfg = copy.deepcopy(cfg_i)\n",
    "        best_state_dict = copy.deepcopy(best_state_i)\n",
    "        best_idx_to_label = idx_to_label_i\n",
    "        best_val_t = val_t_i\n",
    "\n",
    "# ---------- grid search summary ----------\n",
    "results_df = pd.DataFrame(results).sort_values(\"best_val_f1\", ascending=False)\n",
    "print(\"\\n================ GRID SEARCH SUMMARY ================\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nBest config (from grid):\")\n",
    "print(best_cfg.__dict__)\n",
    "print(\"Best val F1:\", best_val_f1)\n",
    "\n",
    "# ---------- objects for the rest of the notebook ----------\n",
    "cfg = best_cfg\n",
    "idx_to_label = best_idx_to_label\n",
    "val_t = best_val_t\n",
    "\n",
    "num_classes = len(idx_to_label)\n",
    "model = build_model(cfg, num_classes=num_classes, device=device)\n",
    "model.load_state_dict(best_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb61bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Diagnostics & Confusion Matrix for best config\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ricrea train/val split e dataloader per la best cfg\n",
    "train_df, val_df, unique_labels, label_to_idx, idx_to_label = load_labels_and_split(cfg)\n",
    "train_t_diag, val_t_diag = get_transforms(cfg)\n",
    "train_loader_diag, val_loader_diag = create_dataloaders(\n",
    "    cfg, train_df, val_df, train_t_diag, val_t_diag\n",
    ")\n",
    "\n",
    "# ---------- CLASS DISTRIBUTION ----------\n",
    "print(\"=== CLASS DISTRIBUTION ===\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(train_df[\"label\"].value_counts().sort_index())\n",
    "print(f\"Total train samples: {len(train_df)}\")\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "print(val_df[\"label\"].value_counts().sort_index())\n",
    "print(f\"Total val samples: {len(val_df)}\")\n",
    "\n",
    "# ---------- TRAINING CONFIG ----------\n",
    "print(\"\\n=== TRAINING CONFIG ===\")\n",
    "print(f\"Experiment name: {cfg.exp_name}\")\n",
    "print(f\"Backbone     : {cfg.backbone}\")\n",
    "print(f\"Image size   : {cfg.img_size}\")\n",
    "print(f\"Learning rate: {cfg.lr}\")\n",
    "print(f\"Epochs       : {cfg.epochs}\")\n",
    "print(f\"Batch size   : {cfg.batch_size}\")\n",
    "print(f\"Use masks    : {cfg.use_masks}\")\n",
    "print(f\"Mask mode    : {cfg.mask_mode if cfg.use_masks else 'N/A'}\")\n",
    "print(f\"Augmentation : {getattr(cfg, 'augmentation', 'strong')}\")\n",
    "\n",
    "# ---------- CLASS WEIGHTS ----------\n",
    "class_counts = train_df[\"label_idx\"].value_counts().sort_index().values.astype(float)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "\n",
    "print(\"\\n=== CLASS WEIGHTS ===\")\n",
    "for i, (label, weight) in enumerate(zip(unique_labels, class_weights)):\n",
    "    print(f\"{label}: {weight:.4f} (count: {int(class_counts[i])})\")\n",
    "\n",
    "# ---------- Confusion matrix on validation set ----------\n",
    "print(\"\\n=== CONFUSION MATRIX (validation, best model) ===\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_diag:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix - Best model on Validation Set\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# tieni idx_to_label aggiornato per l'inference\n",
    "idx_to_label = idx_to_label\n",
    "val_t = val_t_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell X - Sanity checks for train/val splits and masks\n",
    "\n",
    "import os\n",
    "\n",
    "from includes.data_utils import build_paths\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"SANITY CHECK: splits, image dirs, and masks\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# --- Paths from cfg/build_paths ---\n",
    "train_img_dir, test_img_dir, mask_dir = build_paths(cfg)\n",
    "\n",
    "print(\"\\n[CONFIG PATHS]\")\n",
    "print(f\"cfg.train_img_dir -> {cfg.train_img_dir}\")\n",
    "print(f\"cfg.test_img_dir  -> {cfg.test_img_dir}\")\n",
    "val_img_dir_cfg = getattr(cfg, \"val_img_dir\", None)\n",
    "print(f\"cfg.val_img_dir   -> {val_img_dir_cfg}\")\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir : {test_img_dir}\")\n",
    "print(f\"Resolved mask_dir     : {mask_dir}\")\n",
    "\n",
    "# --- Check that train_df and val_df use disjoint sample_index ---\n",
    "train_names = set(train_df[\"sample_index\"].tolist())\n",
    "val_names = set(val_df[\"sample_index\"].tolist())\n",
    "overlap = train_names & val_names\n",
    "\n",
    "print(\"\\n[SPLIT CHECK]\")\n",
    "print(f\"Unique train samples: {len(train_names)}\")\n",
    "print(f\"Unique val samples  : {len(val_names)}\")\n",
    "print(f\"Overlap (train ∩ val): {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(\"WARNING: these files are in BOTH train and val (showing up to 20):\")\n",
    "    print(sorted(list(overlap))[:20])\n",
    "else:\n",
    "    print(\"OK: train and val image sets are disjoint.\")\n",
    "\n",
    "# --- Inspect dataset dirs actually used by the loaders ---\n",
    "train_ds = train_loader_diag.dataset\n",
    "val_ds = val_loader_diag.dataset\n",
    "\n",
    "print(\"\\n[DATASET DIRS USED BY DATALOADERS]\")\n",
    "print(f\"Train loader img_dir: {getattr(train_ds, 'img_dir', 'N/A')}\")\n",
    "print(f\"Train loader mask_dir: {getattr(train_ds, 'mask_dir', 'N/A')}\")\n",
    "print(f\"Val loader img_dir  : {getattr(val_ds, 'img_dir', 'N/A')}\")\n",
    "print(f\"Val loader mask_dir : {getattr(val_ds, 'mask_dir', 'N/A')}\")\n",
    "\n",
    "# --- Check that masks exist for each split (based on sample_index -> mask_* mapping) ---\n",
    "def check_masks_for_split(df, dataset, split_name: str, max_examples: int = 10):\n",
    "    \"\"\"Check that for each image there is a corresponding mask_* file.\"\"\"\n",
    "    base_mask_dir = getattr(dataset, \"mask_dir\", None)\n",
    "    if base_mask_dir is None:\n",
    "        print(f\"\\n[MASK CHECK - {split_name}] Dataset has no mask_dir attribute, skipping.\")\n",
    "        return\n",
    "\n",
    "    missing = []\n",
    "    for name in df[\"sample_index\"]:\n",
    "        # Derive mask filename from image filename\n",
    "        if name.startswith(\"img_\"):\n",
    "            mask_name = \"mask_\" + name[4:]\n",
    "        else:\n",
    "            mask_name = name.replace(\"img\", \"mask\")\n",
    "\n",
    "        mask_path = os.path.join(base_mask_dir, mask_name)\n",
    "        if not os.path.exists(mask_path):\n",
    "            missing.append(mask_name)\n",
    "\n",
    "    print(f\"\\n[MASK CHECK - {split_name}]\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Missing masks: {len(missing)}\")\n",
    "    if missing:\n",
    "        print(\"First missing mask files:\", missing[:max_examples])\n",
    "    else:\n",
    "        print(\"OK: all expected mask_* files exist for this split.\")\n",
    "\n",
    "check_masks_for_split(train_df, train_ds, \"train\")\n",
    "check_masks_for_split(val_df, val_ds, \"val\")\n",
    "\n",
    "print(\"\\n[SUMMARY]\")\n",
    "print(\"If:\")\n",
    "print(\" - train/val overlap is 0,\")\n",
    "print(\" - train loader img_dir == pp_train_data (se stai usando il pp),\")\n",
    "print(\" - val loader img_dir   == train_data (se hai impostato val_img_dir),\")\n",
    "print(\" - e nessuna mask risulta missing,\")\n",
    "print(\"allora split e mask sono coerenti ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb07622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Inference and submission (best model from grid search)\n",
    "\n",
    "test_loader, test_files = create_test_loader(cfg, val_t)\n",
    "\n",
    "submission_name = f\"submission_{cfg.exp_name}.csv\"\n",
    "\n",
    "run_inference_and_save(\n",
    "    cfg,\n",
    "    model,\n",
    "    test_loader,\n",
    "    idx_to_label,\n",
    "    device,\n",
    "    output_csv=submission_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c910e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (IN_COLAB):\n",
    "    full_submission_path = os.path.join(PROJECT_ROOT, cfg.out_dir, submission_name)\n",
    "    print(full_submission_path)\n",
    "    from google.colab import files\n",
    "    files.download(full_submission_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
