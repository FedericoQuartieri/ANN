{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17b3dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add local_lib to sys.path so packages installed there are found\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'local_lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c882e86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\danie\\ANN\n",
      "Working dir: c:\\Users\\danie\\ANN\n",
      "Python path contains 'includes'? -> True\n",
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingConfig(exp_name='resnet50_big_masks', project_root='c:\\\\Users\\\\danie\\\\ANN', data_dir='data', out_dir='out', train_img_dir='train_data', test_img_dir='test_data', labels_csv='train_labels.csv', mask_dir=None, backbone='resnet50', img_size=384, batch_size=16, num_workers=4, val_size=0.2, random_seed=42, mask_mode='crop_bbox', lr=0.0001, weight_decay=0.0001, epochs=50, use_scheduler=True, use_masks=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 - Imports, environment setup and choose experiment\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --------- detect if running on Colab ---------\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Path to your project on Google Drive\n",
    "    PROJECT_ROOT = \"/content/drive/MyDrive/[2025-2026] AN2DL/challenge2\"\n",
    "else:\n",
    "    # Local project root (adjust if needed)\n",
    "    PROJECT_ROOT = os.getcwd()  # or a fixed path\n",
    "\n",
    "# Change working directory to project root and make sure we can import \"includes\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "print(\"Python path contains 'includes'? ->\", \"includes\" in os.listdir(\".\"))\n",
    "\n",
    "# --------- now we can import from includes ---------\n",
    "from includes.config import EXPERIMENTS\n",
    "from includes.data_utils import (\n",
    "    load_labels_and_split,\n",
    "    get_transforms,\n",
    "    create_dataloaders,\n",
    ")\n",
    "from includes.model_utils import (\n",
    "    build_model,\n",
    "    create_criterion_optimizer_scheduler,\n",
    "    train_model,\n",
    "    evaluate,\n",
    ")\n",
    "from includes.inference_utils import (\n",
    "    create_test_loader,\n",
    "    run_inference_and_save,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --------- choose which experiment to run ---------\n",
    "# available:\n",
    "#   \"baseline\"\n",
    "#   \"resnet50_big\"\n",
    "#   \"resnet50_big_masks\"\n",
    "#   \"challenge_2-2\"\n",
    "EXP_NAME = \"resnet50_big_masks\"\n",
    "cfg = EXPERIMENTS[EXP_NAME]\n",
    "\n",
    "\n",
    "# Override project_root and out_dir according to environment\n",
    "cfg.project_root = PROJECT_ROOT\n",
    "\n",
    "# Save submissions to:\n",
    "#   local:  <PROJECT_ROOT>/out\n",
    "#   Colab:  /content/drive/.../challenge2/out\n",
    "cfg.out_dir = \"out\"\n",
    "\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9472f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['HER2(+)', 'Luminal A', 'Luminal B', 'Triple negative']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Dataframes and loaders\n",
    "\n",
    "train_df, val_df, unique_labels, label_to_idx, idx_to_label = load_labels_and_split(cfg)\n",
    "print(\"Labels:\", unique_labels)\n",
    "\n",
    "train_t, val_t = get_transforms(cfg)\n",
    "train_loader, val_loader = create_dataloaders(cfg, train_df, val_df, train_t, val_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eb61bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\danie/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Model, criterion, optimizer, scheduler\n",
    "\n",
    "model = build_model(cfg, num_classes=len(unique_labels), device=device)\n",
    "criterion, optimizer, scheduler = create_criterion_optimizer_scheduler(\n",
    "    cfg, model, train_df, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd89a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Starting training - experiment: test\n",
      "Backbone: resnet18  |  img_size: 224  |  epochs: 1\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/1]\n",
      "    [Batch  71/71] loss=1.3981  f1=0.3082\n",
      "  Train - loss: 1.3981  |  f1: 0.3082\n",
      "  Val   - loss: 1.3969  |  f1: 0.3104\n",
      "  >> New best model! val_f1 improved to 0.3104\n",
      "\n",
      "==============================================================\n",
      "Training finished for experiment: test\n",
      "Best validation F1 (macro): 0.3104\n",
      "==============================================================\n",
      "Val acc: 0.3104267413794574\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        HER2(+)       0.46      0.24      0.31        80\n",
      "      Luminal A       0.61      0.23      0.33        83\n",
      "      Luminal B       0.36      0.66      0.46        89\n",
      "Triple negative       0.11      0.16      0.13        31\n",
      "\n",
      "       accuracy                           0.36       283\n",
      "      macro avg       0.39      0.32      0.31       283\n",
      "   weighted avg       0.43      0.36      0.35       283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Training\n",
    "\n",
    "best_state, history = train_model(\n",
    "    cfg, model, train_loader, val_loader, criterion, optimizer, scheduler, device\n",
    ")\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# Final report on validation\n",
    "val_loss, val_acc, y_true, y_pred = evaluate(model, val_loader, criterion, device)\n",
    "print(\"Val acc:\", val_acc)\n",
    "print(classification_report(y_true, y_pred, target_names=unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb07622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Inference and submission\n",
    "\n",
    "test_loader, test_files = create_test_loader(cfg, val_t)\n",
    "run_inference_and_save(cfg, model, test_loader, idx_to_label, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94ff31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: Loading images into memory\n",
      "============================================================\n",
      "Loading 1129 images...\n",
      "  Loaded 100/1129 images\n",
      "  Loaded 100/1129 images\n",
      "  Loaded 200/1129 images\n",
      "  Loaded 200/1129 images\n",
      "  Loaded 300/1129 images\n",
      "  Loaded 300/1129 images\n",
      "  Loaded 400/1129 images\n",
      "  Loaded 400/1129 images\n",
      "  Loaded 500/1129 images\n",
      "  Loaded 500/1129 images\n",
      "  Loaded 600/1129 images\n",
      "  Loaded 600/1129 images\n",
      "  Loaded 700/1129 images\n",
      "  Loaded 700/1129 images\n",
      "  Loaded 800/1129 images\n",
      "  Loaded 800/1129 images\n",
      "  Loaded 900/1129 images\n",
      "  Loaded 900/1129 images\n",
      "  Loaded 1000/1129 images\n",
      "  Loaded 1000/1129 images\n",
      "  Loaded 1100/1129 images\n",
      "  Loaded 1100/1129 images\n",
      "  Loaded 1129/1129 images\n",
      "  Loaded 1129/1129 images\n",
      "Loaded 1129 images with shape (1129, 384, 384, 3)\n",
      "Loading 283 images...\n",
      "Loaded 1129 images with shape (1129, 384, 384, 3)\n",
      "Loading 283 images...\n",
      "  Loaded 100/283 images\n",
      "  Loaded 100/283 images\n",
      "  Loaded 200/283 images\n",
      "  Loaded 200/283 images\n",
      "  Loaded 283/283 images\n",
      "Loaded 283 images with shape (283, 384, 384, 3)\n",
      "\n",
      "Train set: (1129, 384, 384, 3), labels: (1129,)\n",
      "Val set: (283, 384, 384, 3), labels: (283,)\n",
      "\n",
      "============================================================\n",
      "STEP 2: Creating augmentation pipeline\n",
      "============================================================\n",
      "Augmentation transforms:\n",
      "Compose(\n",
      "      RandomHorizontalFlip(p=0.5)\n",
      "      RandomRotation(degrees=[-15.0, 15.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n",
      "      ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1))\n",
      "      RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=[0.0], inplace=False)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "STEP 3: Creating augmented DataLoaders\n",
      "============================================================\n",
      "Train augmented dataset: 1129 samples\n",
      "Val augmented dataset: 283 samples\n",
      "\n",
      "Created DataLoaders:\n",
      "  Train batches: 71\n",
      "  Val batches: 18\n",
      "  Batch size: 16\n",
      "\n",
      "============================================================\n",
      "STEP 4: Training model\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training with Augmented Data\n",
      "============================================================\n",
      "  Loaded 283/283 images\n",
      "Loaded 283 images with shape (283, 384, 384, 3)\n",
      "\n",
      "Train set: (1129, 384, 384, 3), labels: (1129,)\n",
      "Val set: (283, 384, 384, 3), labels: (283,)\n",
      "\n",
      "============================================================\n",
      "STEP 2: Creating augmentation pipeline\n",
      "============================================================\n",
      "Augmentation transforms:\n",
      "Compose(\n",
      "      RandomHorizontalFlip(p=0.5)\n",
      "      RandomRotation(degrees=[-15.0, 15.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n",
      "      ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1))\n",
      "      RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=[0.0], inplace=False)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "STEP 3: Creating augmented DataLoaders\n",
      "============================================================\n",
      "Train augmented dataset: 1129 samples\n",
      "Val augmented dataset: 283 samples\n",
      "\n",
      "Created DataLoaders:\n",
      "  Train batches: 71\n",
      "  Val batches: 18\n",
      "  Batch size: 16\n",
      "\n",
      "============================================================\n",
      "STEP 4: Training model\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training with Augmented Data\n",
      "============================================================\n",
      "Train batches: 71, Val batches: 18\n",
      "==============================================================\n",
      "Starting training - experiment: resnet50_big_masks\n",
      "Backbone: resnet50  |  img_size: 384  |  epochs: 50\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/50]\n",
      "Train batches: 71, Val batches: 18\n",
      "==============================================================\n",
      "Starting training - experiment: resnet50_big_masks\n",
      "Backbone: resnet50  |  img_size: 384  |  epochs: 50\n",
      "==============================================================\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 1/50]\n",
      "    [Batch  71/71] loss=1.3436  f1=0.3102\n",
      "\n",
      "  Train - loss: 1.3436  |  f1: 0.3102\n",
      "  Val   - loss: 1.3116  |  f1: 0.2792\n",
      "  >> New best model! val_f1 improved to 0.2792\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 2/50]\n",
      "  Train - loss: 1.3436  |  f1: 0.3102\n",
      "  Val   - loss: 1.3116  |  f1: 0.2792\n",
      "  >> New best model! val_f1 improved to 0.2792\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 2/50]\n",
      "    [Batch  71/71] loss=1.2801  f1=0.3495\n",
      "\n",
      "  Train - loss: 1.2801  |  f1: 0.3495\n",
      "  Val   - loss: 1.3314  |  f1: 0.2678\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 3/50]\n",
      "  Train - loss: 1.2801  |  f1: 0.3495\n",
      "  Val   - loss: 1.3314  |  f1: 0.2678\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 3/50]\n",
      "    [Batch  71/71] loss=1.2814  f1=0.3524\n",
      "\n",
      "  Train - loss: 1.2814  |  f1: 0.3524\n",
      "  Val   - loss: 1.3635  |  f1: 0.1923\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 4/50]\n",
      "  Train - loss: 1.2814  |  f1: 0.3524\n",
      "  Val   - loss: 1.3635  |  f1: 0.1923\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 4/50]\n",
      "    [Batch  71/71] loss=1.2646  f1=0.3523\n",
      "\n",
      "  Train - loss: 1.2646  |  f1: 0.3523\n",
      "  Val   - loss: 1.3530  |  f1: 0.1987\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 5/50]\n",
      "  Train - loss: 1.2646  |  f1: 0.3523\n",
      "  Val   - loss: 1.3530  |  f1: 0.1987\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 5/50]\n",
      "    [Batch  71/71] loss=1.2238  f1=0.3940\n",
      "\n",
      "  Train - loss: 1.2238  |  f1: 0.3940\n",
      "  Val   - loss: 1.3392  |  f1: 0.2838\n",
      "  >> New best model! val_f1 improved to 0.2838\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 6/50]\n",
      "  Train - loss: 1.2238  |  f1: 0.3940\n",
      "  Val   - loss: 1.3392  |  f1: 0.2838\n",
      "  >> New best model! val_f1 improved to 0.2838\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 6/50]\n",
      "    [Batch  71/71] loss=1.1686  f1=0.4402\n",
      "\n",
      "  Train - loss: 1.1686  |  f1: 0.4402\n",
      "  Val   - loss: 1.4260  |  f1: 0.3102\n",
      "  >> New best model! val_f1 improved to 0.3102\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 7/50]\n",
      "  Train - loss: 1.1686  |  f1: 0.4402\n",
      "  Val   - loss: 1.4260  |  f1: 0.3102\n",
      "  >> New best model! val_f1 improved to 0.3102\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 7/50]\n",
      "    [Batch  71/71] loss=1.1297  f1=0.4625\n",
      "\n",
      "  Train - loss: 1.1297  |  f1: 0.4625\n",
      "  Val   - loss: 1.5135  |  f1: 0.2923\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 8/50]\n",
      "  Train - loss: 1.1297  |  f1: 0.4625\n",
      "  Val   - loss: 1.5135  |  f1: 0.2923\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 8/50]\n",
      "    [Batch  71/71] loss=1.0935  f1=0.4749\n",
      "\n",
      "  Train - loss: 1.0935  |  f1: 0.4749\n",
      "  Val   - loss: 1.4743  |  f1: 0.3438\n",
      "  >> New best model! val_f1 improved to 0.3438\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 9/50]\n",
      "  Train - loss: 1.0935  |  f1: 0.4749\n",
      "  Val   - loss: 1.4743  |  f1: 0.3438\n",
      "  >> New best model! val_f1 improved to 0.3438\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[Epoch 9/50]\n",
      "    [Batch  70/71] loss=1.0082  f1=0.5221\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      6\u001b[39m AUG_PARAMS = {\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mflip_p\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.5\u001b[39m,                    \u001b[38;5;66;03m# Horizontal flip probability\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrotation_degrees\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m15\u001b[39m,           \u001b[38;5;66;03m# Max rotation angle\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msubmission_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msubmission_augmented.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m }\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Run complete augmented training pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m aug_model, aug_history = \u001b[43mrun_augmented_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx_to_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43midx_to_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_t\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mAUG_PARAMS\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\includes\\augmentation_pipeline.py:327\u001b[39m, in \u001b[36mrun_augmented_experiment\u001b[39m\u001b[34m(cfg, train_df, val_df, unique_labels, idx_to_label, device, val_t, flip_p, rotation_degrees, use_color_jitter, random_erasing_p, num_workers, save_submission, submission_name)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSTEP 4: Training model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    326\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m aug_model, aug_history = \u001b[43mtrain_with_augmentation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_aug_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_aug_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# 5. Save submission (optional)\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_submission:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\includes\\augmentation_pipeline.py:216\u001b[39m, in \u001b[36mtrain_with_augmentation\u001b[39m\u001b[34m(cfg, train_aug_loader, val_aug_loader, unique_labels, train_df, device)\u001b[39m\n\u001b[32m    210\u001b[39m aug_criterion, aug_optimizer, aug_scheduler = create_criterion_optimizer_scheduler(\n\u001b[32m    211\u001b[39m     cfg, model=aug_model, train_df=train_df, device=device\n\u001b[32m    212\u001b[39m )\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_aug_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_aug_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m best_aug_state, aug_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43maug_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_aug_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_aug_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43maug_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43maug_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43maug_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m aug_model.load_state_dict(best_aug_state)\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\includes\\model_utils.py:255\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(cfg, model, train_loader, val_loader, criterion, optimizer, scheduler, device)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# barra di progresso per il training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m train_loss, train_f1 = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m val_loss, val_f1, val_true, val_pred = evaluate(\n\u001b[32m    265\u001b[39m     model, val_loader, criterion, device\n\u001b[32m    266\u001b[39m )\n\u001b[32m    268\u001b[39m history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\includes\\model_utils.py:127\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, epoch, num_epochs)\u001b[39m\n\u001b[32m    123\u001b[39m labels = labels.to(device)\n\u001b[32m    125\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m    129\u001b[39m _, preds = torch.max(outputs, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torchvision\\models\\resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torchvision\\models\\resnet.py:274\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    271\u001b[39m x = \u001b[38;5;28mself\u001b[39m.maxpool(x)\n\u001b[32m    273\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer3(x)\n\u001b[32m    276\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer4(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torchvision\\models\\resnet.py:147\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    144\u001b[39m identity = x\n\u001b[32m    146\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m    150\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv2(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\ANN\\local_lib\\torch\\nn\\functional.py:2813\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2811\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Optional: Run augmented training experiment\n",
    "# Imports for augmentation pipeline\n",
    "from includes.augmentation_pipeline import run_augmented_experiment\n",
    "\n",
    "# Configure augmentation parameters (adjust as needed)\n",
    "AUG_PARAMS = {\n",
    "    \"flip_p\": 0.5,                    # Horizontal flip probability\n",
    "    \"rotation_degrees\": 15,           # Max rotation angle\n",
    "    \"use_color_jitter\": True,         # Enable color jitter\n",
    "    \"random_erasing_p\": 0.2,          # Random erasing probability\n",
    "    \"num_workers\": 0,                 # 0 for Windows notebooks\n",
    "    \"save_submission\": True,          # Save submission CSV\n",
    "    \"submission_name\": \"submission_augmented.csv\",\n",
    "}\n",
    "\n",
    "# Run complete augmented training pipeline\n",
    "aug_model, aug_history = run_augmented_experiment(\n",
    "    cfg=cfg,\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    unique_labels=unique_labels,\n",
    "    idx_to_label=idx_to_label,\n",
    "    device=device,\n",
    "    val_t=val_t,\n",
    "    **AUG_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Inference and submission\n",
    "\n",
    "test_loader, test_files = create_test_loader(cfg, val_t)\n",
    "run_inference_and_save(cfg, aug_model, test_loader, idx_to_label, device, output_csv=\"submission_augmented.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
