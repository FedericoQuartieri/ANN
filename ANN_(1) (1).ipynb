{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-in9ArBRwrp"
      },
      "outputs": [],
      "source": [
        "# OPTIMAL HYPERPARAMETERS - Based on empirical results\n",
        "K = 5\n",
        "N_VAL_USERS = 40  # â¬†ï¸ Bilanciato (era 5, troppo poco; 30 funziona ma 20 Ã¨ sufficiente)\n",
        "\n",
        "EPOCHS = 400  # â¬†ï¸ Increased (ma con early stopping)\n",
        "PATIENCE = 40  # â¬†ï¸ Slightly increased\n",
        "VERBOSE = 10\n",
        "\n",
        "LEARNING_RATE = 5e-4  # âœ… Keep (buono)\n",
        "BATCH_SIZE = 256  # âœ… Keep (riduce overfitting)\n",
        "WINDOW_SIZE = 100  # âœ… Keep (BEST from both searches)\n",
        "STRIDE = 25  # âœ… Keep (BEST from both searches)\n",
        "RNN_TYPE = 'GRU'  # âœ… Keep\n",
        "BIDIRECTIONAL = True  # âœ… Keep\n",
        "\n",
        "HIDDEN_LAYERS = 2  # âœ… Keep\n",
        "HIDDEN_SIZE = 96  # ðŸ”§ COMPROMISE (tra 64 e 128)\n",
        "\n",
        "DROPOUT_RATE = 0.3  # ðŸ”§ REDUCED from 0.4 (piÃ¹ bilanciato)\n",
        "L1_LAMBDA = 0  # âœ… Keep\n",
        "L2_LAMBDA = 5e-5  # ðŸ”§ REDUCED from 1e-4 (meno aggressivo)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Reverse mapping from integers to pain level names\n",
        "label_reverse_mapping = {\n",
        "    0: 'no_pain',\n",
        "    1: 'low_pain',\n",
        "    2: 'high_pain'\n",
        "}\n",
        "\n",
        "# Create mapping dictionary for pain levels\n",
        "pain_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2\n",
        "}\n",
        "\n",
        "labels = ['no_pain', 'low_pain', 'high_pain']\n",
        "\n",
        "num_classes = len(labels)\n",
        "\n",
        "# Define parameters to search\n",
        "param_grid = {\n",
        "    'window_size': [50, 100, 160],\n",
        "    'stride': [10, 25, 50, 70],\n",
        "    'n_val_users' : [30, 40, 50, 60],\n",
        "    'hidden_size': [64, 96, 128],\n",
        "    'hidden_layers': [1, 2, 3, 5],\n",
        "    'batch_size': [64, 128, 256, 512],\n",
        "    'k' : [3, 5, 7]\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-95VNzsCBdWV",
        "outputId": "3eba7da4-b08e-4dc1-b160-3d268f46cdb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device=cpu, physical_cores=4, OMP_NUM_THREADS=3, torch_threads=3, interop=1\n",
            "PyTorch version: 2.9.0+cu128\n",
            "Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sysctl: cannot stat /proc/sys/hw/physicalcpu: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "LOCAL = True\n",
        "\n",
        "# Import necessary libraries\n",
        "import os, subprocess, shlex\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "def mac_physical_cores():\n",
        "    try:\n",
        "        out = subprocess.check_output(shlex.split(\"sysctl -n hw.physicalcpu\")).decode().strip()\n",
        "        return int(out)\n",
        "    except Exception:\n",
        "        return max(1, (os.cpu_count() or 2) // 2)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "if (LOCAL):\n",
        "\n",
        "    PHYS = mac_physical_cores()\n",
        "\n",
        "    # Leave one core free for the OS (helps thermals on Airs)\n",
        "    OMP = max(1, PHYS - 1)\n",
        "\n",
        "    # --- OpenMP / BLAS knobs (set BEFORE importing torch) ---\n",
        "    os.environ.setdefault(\"OMP_NUM_THREADS\", str(OMP))            # OpenMP (oneDNN, some kernels)\n",
        "    os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", str(OMP))     # Apple Accelerate / vecLib\n",
        "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", str(OMP))       # If any dep uses OpenBLAS\n",
        "    os.environ.setdefault(\"MKL_NUM_THREADS\", str(OMP))            # Rare on macOS, harmless if unused\n",
        "    os.environ.setdefault(\"MKL_DYNAMIC\", \"FALSE\")                 # Avoid oversubscription if MKL present\n",
        "\n",
        "    device = torch.device(\"cpu\")  # Intel Mac: CPU only\n",
        "\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.set_num_threads(OMP)        # intra-op threads\n",
        "    torch.set_num_interop_threads(1)  # parallel ops; start low on laptops\n",
        "\n",
        "    print(f\"device={device}, physical_cores={PHYS}, \"\n",
        "        f\"OMP_NUM_THREADS={OMP}, \"\n",
        "        f\"torch_threads={torch.get_num_threads()}, \"\n",
        "        f\"interop={torch.get_num_interop_threads()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsV2HL_gD9cl",
        "outputId": "7ae7b0c2-bd22-4b52-df62-3dd18bcf3972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows of X_train with encoded labels:\n",
            "   sample_index  label\n",
            "0             0      0\n",
            "1             0      0\n",
            "2             0      0\n",
            "3             0      0\n",
            "4             0      0\n",
            "5             0      0\n",
            "6             0      0\n",
            "7             0      0\n",
            "8             0      0\n",
            "9             0      0\n",
            "\n",
            "Label value counts:\n",
            "label\n",
            "0    81760\n",
            "1    15040\n",
            "2     8960\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Check for NaN labels:\n",
            "NaN count: 0\n"
          ]
        }
      ],
      "source": [
        "X_train = pd.read_csv('pirate_pain_train.csv')\n",
        "y_train = pd.read_csv('pirate_pain_train_labels.csv')\n",
        "\n",
        "\n",
        "# First map the labels in y_train\n",
        "y_train['label_encoded'] = y_train['label'].map(pain_mapping)\n",
        "\n",
        "# Then merge with X_train based on sample_index\n",
        "X_train = X_train.merge(\n",
        "    y_train[['sample_index', 'label_encoded']],\n",
        "    on='sample_index',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Rename the column\n",
        "X_train.rename(columns={'label_encoded': 'label'}, inplace=True)\n",
        "\n",
        "# Verify the mapping worked correctly\n",
        "print(\"\\nFirst few rows of X_train with encoded labels:\")\n",
        "print(X_train[['sample_index', 'label']].head(10))\n",
        "\n",
        "print(\"\\nLabel value counts:\")\n",
        "print(X_train['label'].value_counts())\n",
        "\n",
        "print(\"\\nCheck for NaN labels:\")\n",
        "print(f\"NaN count: {X_train['label'].isna().sum()}\")\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acRv6ssvQQSM",
        "outputId": "ee2dc98f-6a65-4ac2-9555-de8b57e30e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping joint_30 column (all NaN values)...\n",
            "Dropped joint_30 from (105760, 40)\n",
            "\n",
            "Columns after dropping joint_30:\n",
            "X_train columns: 40\n"
          ]
        }
      ],
      "source": [
        "# Drop joint_30 column (contains only NaN values)\n",
        "print(\"Dropping joint_30 column (all NaN values)...\")\n",
        "for df in [X_train]:\n",
        "    if 'joint_30' in df.columns:\n",
        "        df.drop('joint_30', axis=1, inplace=True)\n",
        "        print(f\"Dropped joint_30 from {df.shape}\")\n",
        "\n",
        "print(\"\\nColumns after dropping joint_30:\")\n",
        "print(f\"X_train columns: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2dx6K_ZQQSN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRr1GEMmE1wW",
        "outputId": "b73077e4-ce24-44fe-ee0f-4421a1491c6d"
      },
      "outputs": [],
      "source": [
        "# print(\"\\n--- Data successfully loaded ---\")\n",
        "# print(f\"X_train shape: {X_train.shape}\")\n",
        "# print(f\"y_train shape: {y_train.shape}\")\n",
        "# print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "# print(\"\\n--- Initial Feature Count Breakdown ---\")\n",
        "# all_features = X_train.columns.drop(['sample_index', 'time'])\n",
        "# joint_features = [col for col in all_features if col.startswith('joint_')]\n",
        "# static_features = [col for col in all_features if col in ['n_legs', 'n_hands', 'n_eyes']]\n",
        "\n",
        "# print(f\"Total Features (excluding IDs): {len(all_features)}\")\n",
        "# print(f\"Core Time-Series (Joints): {len(joint_features)} columns\")\n",
        "# print(f\"Static Subject Characteristics: {len(static_features)} columns\")\n",
        "# print(f\"Pain Survey Features: {len(all_features) - len(joint_features) - len(static_features)} columns\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X-Pan_2kQQSO"
      },
      "outputs": [],
      "source": [
        "# First: Convert categorical variables to binary (two -> 1, others -> 0)\n",
        "binary_cols = ['n_hands', 'n_eyes', 'n_legs']\n",
        "for col in binary_cols:\n",
        "    for df_ in [X_train]:\n",
        "        df_[col] = df_[col].map(lambda x: 1 if str(x).lower().strip() == 'two' else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9NAklF6FP2G",
        "outputId": "b607c324-98e5-452f-cd98-c532813c365d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data structure ---\n",
            "\n",
            "X_train Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 105760 entries, 0 to 105759\n",
            "Data columns (total 40 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   sample_index   105760 non-null  int64  \n",
            " 1   time           105760 non-null  int64  \n",
            " 2   pain_survey_1  105760 non-null  int64  \n",
            " 3   pain_survey_2  105760 non-null  int64  \n",
            " 4   pain_survey_3  105760 non-null  int64  \n",
            " 5   pain_survey_4  105760 non-null  int64  \n",
            " 6   n_legs         105760 non-null  int64  \n",
            " 7   n_hands        105760 non-null  int64  \n",
            " 8   n_eyes         105760 non-null  int64  \n",
            " 9   joint_00       105760 non-null  float64\n",
            " 10  joint_01       105760 non-null  float64\n",
            " 11  joint_02       105760 non-null  float64\n",
            " 12  joint_03       105760 non-null  float64\n",
            " 13  joint_04       105760 non-null  float64\n",
            " 14  joint_05       105760 non-null  float64\n",
            " 15  joint_06       105760 non-null  float64\n",
            " 16  joint_07       105760 non-null  float64\n",
            " 17  joint_08       105760 non-null  float64\n",
            " 18  joint_09       105760 non-null  float64\n",
            " 19  joint_10       105760 non-null  float64\n",
            " 20  joint_11       105760 non-null  float64\n",
            " 21  joint_12       105760 non-null  float64\n",
            " 22  joint_13       105760 non-null  float64\n",
            " 23  joint_14       105760 non-null  float64\n",
            " 24  joint_15       105760 non-null  float64\n",
            " 25  joint_16       105760 non-null  float64\n",
            " 26  joint_17       105760 non-null  float64\n",
            " 27  joint_18       105760 non-null  float64\n",
            " 28  joint_19       105760 non-null  float64\n",
            " 29  joint_20       105760 non-null  float64\n",
            " 30  joint_21       105760 non-null  float64\n",
            " 31  joint_22       105760 non-null  float64\n",
            " 32  joint_23       105760 non-null  float64\n",
            " 33  joint_24       105760 non-null  float64\n",
            " 34  joint_25       105760 non-null  float64\n",
            " 35  joint_26       105760 non-null  float64\n",
            " 36  joint_27       105760 non-null  float64\n",
            " 37  joint_28       105760 non-null  float64\n",
            " 38  joint_29       105760 non-null  float64\n",
            " 39  label          105760 non-null  int64  \n",
            "dtypes: float64(30), int64(10)\n",
            "memory usage: 32.3 MB\n",
            "\n",
            "Missing values in X_train: 0\n",
            "\n",
            "y_train Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 661 entries, 0 to 660\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   sample_index   661 non-null    int64 \n",
            " 1   label          661 non-null    object\n",
            " 2   label_encoded  661 non-null    int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 15.6+ KB\n",
            "\n",
            "Missing values in y_train: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Data structure ---\")\n",
        "print(\"\\nX_train Info:\")\n",
        "X_train.info(verbose=True)\n",
        "print(f\"\\nMissing values in X_train: {X_train.isnull().sum().sum()}\")\n",
        "print(\"\\ny_train Info:\")\n",
        "y_train.info(verbose=True)\n",
        "print(f\"\\nMissing values in y_train: {y_train.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "NBmYSucNGROn",
        "outputId": "7c155e64-c277-4c10-8a99-e14aac17918b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>time</th>\n",
              "      <th>pain_survey_1</th>\n",
              "      <th>pain_survey_2</th>\n",
              "      <th>pain_survey_3</th>\n",
              "      <th>pain_survey_4</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>n_hands</th>\n",
              "      <th>n_eyes</th>\n",
              "      <th>joint_00</th>\n",
              "      <th>...</th>\n",
              "      <th>joint_21</th>\n",
              "      <th>joint_22</th>\n",
              "      <th>joint_23</th>\n",
              "      <th>joint_24</th>\n",
              "      <th>joint_25</th>\n",
              "      <th>joint_26</th>\n",
              "      <th>joint_27</th>\n",
              "      <th>joint_28</th>\n",
              "      <th>joint_29</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>1.057600e+05</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "      <td>105760.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>330.000000</td>\n",
              "      <td>79.500000</td>\n",
              "      <td>1.633746</td>\n",
              "      <td>1.654851</td>\n",
              "      <td>1.653640</td>\n",
              "      <td>1.663134</td>\n",
              "      <td>0.990923</td>\n",
              "      <td>0.990923</td>\n",
              "      <td>0.990923</td>\n",
              "      <td>0.943095</td>\n",
              "      <td>...</td>\n",
              "      <td>3.972126e-05</td>\n",
              "      <td>4.176794e-05</td>\n",
              "      <td>3.561780e-05</td>\n",
              "      <td>3.138109e-05</td>\n",
              "      <td>1.024604e-04</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.058244</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.062273</td>\n",
              "      <td>0.311649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>190.814948</td>\n",
              "      <td>46.187338</td>\n",
              "      <td>0.682423</td>\n",
              "      <td>0.669639</td>\n",
              "      <td>0.666649</td>\n",
              "      <td>0.661994</td>\n",
              "      <td>0.094841</td>\n",
              "      <td>0.094841</td>\n",
              "      <td>0.094841</td>\n",
              "      <td>0.202051</td>\n",
              "      <td>...</td>\n",
              "      <td>4.974496e-03</td>\n",
              "      <td>5.472244e-03</td>\n",
              "      <td>1.235450e-03</td>\n",
              "      <td>4.062914e-04</td>\n",
              "      <td>3.206128e-03</td>\n",
              "      <td>0.060293</td>\n",
              "      <td>0.079819</td>\n",
              "      <td>0.060773</td>\n",
              "      <td>0.072597</td>\n",
              "      <td>0.619651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.510494e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.063144e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>165.000000</td>\n",
              "      <td>39.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.828277</td>\n",
              "      <td>...</td>\n",
              "      <td>6.545878e-08</td>\n",
              "      <td>3.321650e-07</td>\n",
              "      <td>3.275038e-07</td>\n",
              "      <td>2.841805e-07</td>\n",
              "      <td>7.161332e-07</td>\n",
              "      <td>0.009885</td>\n",
              "      <td>0.012652</td>\n",
              "      <td>0.016290</td>\n",
              "      <td>0.019638</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>330.000000</td>\n",
              "      <td>79.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.005126</td>\n",
              "      <td>...</td>\n",
              "      <td>8.302747e-07</td>\n",
              "      <td>1.095971e-06</td>\n",
              "      <td>1.024209e-06</td>\n",
              "      <td>8.746147e-07</td>\n",
              "      <td>3.126723e-06</td>\n",
              "      <td>0.021898</td>\n",
              "      <td>0.031739</td>\n",
              "      <td>0.031843</td>\n",
              "      <td>0.039041</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>495.000000</td>\n",
              "      <td>119.250000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.081039</td>\n",
              "      <td>...</td>\n",
              "      <td>2.800090e-06</td>\n",
              "      <td>3.079465e-06</td>\n",
              "      <td>3.021830e-06</td>\n",
              "      <td>2.507548e-06</td>\n",
              "      <td>9.946107e-06</td>\n",
              "      <td>0.048579</td>\n",
              "      <td>0.071051</td>\n",
              "      <td>0.058741</td>\n",
              "      <td>0.079518</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>660.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.407968</td>\n",
              "      <td>...</td>\n",
              "      <td>1.442198e+00</td>\n",
              "      <td>1.305001e+00</td>\n",
              "      <td>2.742411e-01</td>\n",
              "      <td>3.643074e-02</td>\n",
              "      <td>9.473540e-01</td>\n",
              "      <td>1.223617</td>\n",
              "      <td>1.187419</td>\n",
              "      <td>1.412037</td>\n",
              "      <td>1.370765</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sample_index           time  pain_survey_1  pain_survey_2  \\\n",
              "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
              "mean      330.000000      79.500000       1.633746       1.654851   \n",
              "std       190.814948      46.187338       0.682423       0.669639   \n",
              "min         0.000000       0.000000       0.000000       0.000000   \n",
              "25%       165.000000      39.750000       2.000000       2.000000   \n",
              "50%       330.000000      79.500000       2.000000       2.000000   \n",
              "75%       495.000000     119.250000       2.000000       2.000000   \n",
              "max       660.000000     159.000000       2.000000       2.000000   \n",
              "\n",
              "       pain_survey_3  pain_survey_4         n_legs        n_hands  \\\n",
              "count  105760.000000  105760.000000  105760.000000  105760.000000   \n",
              "mean        1.653640       1.663134       0.990923       0.990923   \n",
              "std         0.666649       0.661994       0.094841       0.094841   \n",
              "min         0.000000       0.000000       0.000000       0.000000   \n",
              "25%         2.000000       2.000000       1.000000       1.000000   \n",
              "50%         2.000000       2.000000       1.000000       1.000000   \n",
              "75%         2.000000       2.000000       1.000000       1.000000   \n",
              "max         2.000000       2.000000       1.000000       1.000000   \n",
              "\n",
              "              n_eyes       joint_00  ...      joint_21      joint_22  \\\n",
              "count  105760.000000  105760.000000  ...  1.057600e+05  1.057600e+05   \n",
              "mean        0.990923       0.943095  ...  3.972126e-05  4.176794e-05   \n",
              "std         0.094841       0.202051  ...  4.974496e-03  5.472244e-03   \n",
              "min         0.000000       0.000000  ...  0.000000e+00  1.510494e-07   \n",
              "25%         1.000000       0.828277  ...  6.545878e-08  3.321650e-07   \n",
              "50%         1.000000       1.005126  ...  8.302747e-07  1.095971e-06   \n",
              "75%         1.000000       1.081039  ...  2.800090e-06  3.079465e-06   \n",
              "max         1.000000       1.407968  ...  1.442198e+00  1.305001e+00   \n",
              "\n",
              "           joint_23      joint_24      joint_25       joint_26       joint_27  \\\n",
              "count  1.057600e+05  1.057600e+05  1.057600e+05  105760.000000  105760.000000   \n",
              "mean   3.561780e-05  3.138109e-05  1.024604e-04       0.041905       0.058244   \n",
              "std    1.235450e-03  4.062914e-04  3.206128e-03       0.060293       0.079819   \n",
              "min    0.000000e+00  1.063144e-08  0.000000e+00       0.000203       0.000000   \n",
              "25%    3.275038e-07  2.841805e-07  7.161332e-07       0.009885       0.012652   \n",
              "50%    1.024209e-06  8.746147e-07  3.126723e-06       0.021898       0.031739   \n",
              "75%    3.021830e-06  2.507548e-06  9.946107e-06       0.048579       0.071051   \n",
              "max    2.742411e-01  3.643074e-02  9.473540e-01       1.223617       1.187419   \n",
              "\n",
              "            joint_28       joint_29          label  \n",
              "count  105760.000000  105760.000000  105760.000000  \n",
              "mean        0.049886       0.062273       0.311649  \n",
              "std         0.060773       0.072597       0.619651  \n",
              "min         0.000000       0.000000       0.000000  \n",
              "25%         0.016290       0.019638       0.000000  \n",
              "50%         0.031843       0.039041       0.000000  \n",
              "75%         0.058741       0.079518       0.000000  \n",
              "max         1.412037       1.370765       2.000000  \n",
              "\n",
              "[8 rows x 40 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_WDYT-EHLy6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHvM2ffAJofM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xWuPR5RQQSQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiFEAIuPQQSR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXyo8SxAQQSR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz6-Mj-WQQSR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnKj2jZdQQSR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W3yOX19MQQSR"
      },
      "outputs": [],
      "source": [
        "def build_sequences(df, window=200, stride=50):\n",
        "    \"\"\"\n",
        "    Build sequences from time-series data\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with the data\n",
        "        window: Window size for sequences\n",
        "        stride: Stride for overlapping windows\n",
        "\n",
        "    Returns:\n",
        "        dataset: numpy array of sequences\n",
        "        labels: numpy array of labels\n",
        "    \"\"\"\n",
        "    # Initialise lists to store sequences and their corresponding labels\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over unique IDs in the DataFrame\n",
        "    for sample_id in df['sample_index'].unique():\n",
        "        # Extract sensor data for the current ID\n",
        "        drop_cols = [c for c in ['sample_index', 'time', 'label', 'labels'] if c in df.columns]\n",
        "        temp = df[df['sample_index'] == sample_id].drop(columns=drop_cols).values.astype('float32')\n",
        "\n",
        "        # Retrieve the activity label for the current ID\n",
        "        label_series = df[df['sample_index'] == sample_id]['label']\n",
        "\n",
        "        # Check if label column exists and has values\n",
        "        if label_series.empty:\n",
        "            print(f\"Warning: No label found for sample_id {sample_id}\")\n",
        "            continue\n",
        "\n",
        "        label_value = label_series.values[0]\n",
        "\n",
        "        # Skip samples with NaN labels\n",
        "        if pd.isna(label_value):\n",
        "            print(f\"Warning: NaN label for sample_id {sample_id}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Convert to int and validate\n",
        "        try:\n",
        "            label = int(label_value)\n",
        "            if label < 0 or label > 2:  # Assuming 3 classes: 0, 1, 2\n",
        "                print(f\"Warning: Invalid label {label} for sample_id {sample_id}, skipping...\")\n",
        "                continue\n",
        "        except (ValueError, TypeError) as e:\n",
        "            print(f\"Warning: Cannot convert label {label_value} to int for sample_id {sample_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Calculate padding length to ensure full windows\n",
        "        padding_len = window - len(temp) % window if len(temp) % window != 0 else 0\n",
        "\n",
        "        # Create zero padding with correct number of features\n",
        "        if padding_len > 0:\n",
        "            padding = np.zeros((padding_len, temp.shape[1]), dtype='float32')\n",
        "            temp = np.concatenate((temp, padding))\n",
        "\n",
        "        # Build feature windows and associate them with labels\n",
        "        idx = 0\n",
        "        while idx + window <= len(temp):\n",
        "            dataset.append(temp[idx:idx + window])\n",
        "            labels.append(label)\n",
        "            idx += stride\n",
        "\n",
        "    # Convert lists to numpy arrays for further processing\n",
        "    dataset = np.array(dataset, dtype='float32')\n",
        "    labels = np.array(labels, dtype='int64')\n",
        "\n",
        "    print(f\"Built {len(dataset)} sequences with {len(labels)} labels\")\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_wA9cyBQQSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1NT16n3QQSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oClBmpOPQQSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h8kYIIgQQSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S7SdOdxjQQSS"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G0ypRlR4QQST"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches ahead\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E0XZKKJQQST"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o4Jvq0cQQST"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GpR5aspoQQST"
      },
      "outputs": [],
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKJ48pWqQQST",
        "outputId": "6fed724d-2930-4ccd-baca-d1b749e2f348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "rnn (RNN)                 [[-1, 128], [2, -1]]         54,912         \n",
            "classifier (Linear)       [-1]                         387            \n",
            "===============================================================================\n",
            "Total params: 55,299\n",
            "Trainable params: 55,299\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic RNN classifier (RNN, LSTM, GRU).\n",
        "    Uses the last hidden state for classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Map string name to PyTorch RNN class\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout is only applied between layers (if num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "\n",
        "        # Create the recurrent layer\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calculate input size for the final classifier\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size = hidden_size\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        rnn_out, hidden = self.rnn(x)\n",
        "\n",
        "        # LSTM returns (h_n, c_n), we only need h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "\n",
        "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
        "            # Final shape: (batch_size, hidden_size * 2)\n",
        "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
        "        else:\n",
        "            # Take the last layer's hidden state\n",
        "            # Final shape: (batch_size, hidden_size)\n",
        "            hidden_to_classify = hidden[-1]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=0.,\n",
        "    rnn_type='RNN'\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri_zHv8uQQSU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ral4ClQCQQSU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "o1HTpkmJQQSU"
      },
      "outputs": [],
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5S4P5W1AQQSU"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZnCsqUqtQQSU"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FLYRdcDrQQSU"
      },
      "outputs": [],
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9ITUJXpAQQSV"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynGd49opQQSV"
      },
      "source": [
        "# KFOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8T-RUCTFQQSV"
      },
      "outputs": [],
      "source": [
        "def k_shuffle_split_cross_validation_round_rnn(df, epochs, criterion, device,\n",
        "                            k, n_val_users, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
        "                            window_size, stride, rnn_type, bidirectional,\n",
        "                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Perform K-fold shuffle split cross-validation with sample-based splitting for Pirate Pain time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['sample_index', 'time', 'label', 'pain_survey_*', 'joint_*', 'n_legs', 'n_hands', 'n_eyes']\n",
        "        epochs: Number of training epochs\n",
        "        criterion: Loss function\n",
        "        device: torch.device for computation\n",
        "        k: Number of cross-validation splits\n",
        "        n_val_users: Number of samples for validation set\n",
        "        n_test_users: Number of samples for test set\n",
        "        batch_size: Batch size for training\n",
        "        hidden_layers: Number of recurrent layers\n",
        "        hidden_size: Hidden state dimensionality\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        dropout_rate: Dropout rate\n",
        "        window_size: Length of sliding windows\n",
        "        stride: Step size for sliding windows\n",
        "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "        bidirectional: Whether to use bidirectional RNN\n",
        "        l1_lambda: L1 regularization coefficient (if used)\n",
        "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
        "        patience: Early stopping patience\n",
        "        evaluation_metric: Metric to monitor for early stopping\n",
        "        mode: 'max' or 'min' for evaluation metric\n",
        "        restore_best_weights: Whether to restore best weights after training\n",
        "        writer: TensorBoard writer\n",
        "        verbose: Verbosity level\n",
        "        seed: Random seed\n",
        "        experiment_name: Name for experiment logging\n",
        "\n",
        "    Returns:\n",
        "        fold_losses: Dict with validation losses for each split\n",
        "        fold_metrics: Dict with validation F1 scores for each split\n",
        "        best_scores: Dict with best F1 score for each split plus mean and std\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise containers for results across all splits\n",
        "    fold_losses = {}\n",
        "    fold_metrics = {}\n",
        "    best_scores = {}\n",
        "\n",
        "    # Define pain level mapping\n",
        "    pain_mapping = {\n",
        "        'no_pain': 0,\n",
        "        'low_pain': 1,\n",
        "        'high_pain': 2\n",
        "    }\n",
        "\n",
        "    # Define columns to normalize\n",
        "    pain_survey_columns = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes']\n",
        "    joint_columns = [f'joint_{i:02d}' for i in range(30)]  # joint_00 through joint_29\n",
        "    scale_columns = pain_survey_columns + joint_columns\n",
        "\n",
        "    # Get model architecture parameters\n",
        "    # Count features (excluding sample_index, time, label)\n",
        "    feature_cols = scale_columns  # All features that will be used\n",
        "    in_features = len(feature_cols)\n",
        "    num_classes = 3  # no_pain, low_pain, high_pain\n",
        "\n",
        "    # Initialise model architecture\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=in_features,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=hidden_layers,\n",
        "        num_classes=num_classes,\n",
        "        dropout_rate=dropout_rate,\n",
        "        bidirectional=bidirectional,\n",
        "        rnn_type=rnn_type\n",
        "    ).to(device)\n",
        "\n",
        "    # Store initial weights to reset model for each split\n",
        "    initial_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Iterate through K random splits\n",
        "    for split_idx in range(k):\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"Split {split_idx+1}/{k}\")\n",
        "\n",
        "        # Get unique sample IDs and shuffle them with split-specific seed\n",
        "        unique_samples = df['sample_index'].unique()\n",
        "        random.seed(seed + split_idx)\n",
        "        random.shuffle(unique_samples)\n",
        "\n",
        "        # Calculate the number of samples for the training set\n",
        "        n_train_samples = len(unique_samples) - n_val_users\n",
        "\n",
        "        # Split the shuffled sample IDs into training, validation, and test sets\n",
        "        train_samples = unique_samples[:n_train_samples]\n",
        "        val_samples = unique_samples[n_train_samples:n_train_samples + n_val_users]\n",
        "\n",
        "        # Split the dataset into training, validation, and test sets based on sample IDs\n",
        "        df_train = df[df['sample_index'].isin(train_samples)].copy()\n",
        "        df_val = df[df['sample_index'].isin(val_samples)].copy()\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training set shape: {df_train.shape}\")\n",
        "            print(f\"  Validation set shape: {df_val.shape}\")\n",
        "\n",
        "        # Map pain labels to integers (if not already mapped)\n",
        "        if df_train['label'].dtype == 'object':\n",
        "            df_train['label'] = df_train['label'].map(pain_mapping)\n",
        "            df_val['label'] = df_val['label'].map(pain_mapping)\n",
        "\n",
        "        # Normalise features using training set statistics\n",
        "        train_max = df_train[scale_columns].max()\n",
        "        train_min = df_train[scale_columns].min()\n",
        "\n",
        "        for column in scale_columns:\n",
        "            df_train[column] = (df_train[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "            df_val[column] = (df_val[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "\n",
        "        # Build sequences using the existing build_sequences function\n",
        "        X_train, y_train = build_sequences(df_train, window=window_size, stride=stride)\n",
        "        X_val, y_val = build_sequences(df_val, window=window_size, stride=stride)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training sequences shape: {X_train.shape}\")\n",
        "            print(f\"  Validation sequences shape: {X_val.shape}\")\n",
        "            print(f\"  Test sequences shape: {X_test.shape}\")\n",
        "\n",
        "        # Create PyTorch datasets\n",
        "        train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "        val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "        # Reset model to initial weights for fair comparison across splits\n",
        "        model.load_state_dict(initial_state)\n",
        "\n",
        "        # Define optimizer with L2 regularization\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "        # Enable mixed precision training for GPU acceleration\n",
        "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # Create directory for model checkpoints\n",
        "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
        "\n",
        "        # Train model on current split\n",
        "        model, training_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=epochs,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scaler=split_scaler,\n",
        "            device=device,\n",
        "            writer=writer,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            l1_lambda=l1_lambda,\n",
        "            evaluation_metric=evaluation_metric,\n",
        "            mode=mode,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n",
        "        )\n",
        "\n",
        "        # Store results for this split\n",
        "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
        "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
        "        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n",
        "\n",
        "    # Compute mean and standard deviation of best scores across splits\n",
        "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Best score: {best_scores['mean']:.4f}Â±{best_scores['std']:.4f}\")\n",
        "\n",
        "    return fold_losses, fold_metrics, best_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4yt6vaquQQSV"
      },
      "outputs": [],
      "source": [
        "def grid_search_cv_rnn(df, param_grid, fixed_params, cv_params, verbose=True):\n",
        "    \"\"\"\n",
        "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
        "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
        "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
        "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
        "        verbose: Print progress for each configuration\n",
        "\n",
        "    Returns:\n",
        "        results: Dict with scores for each configuration\n",
        "        best_config: Dict with best hyperparameter combination\n",
        "        best_score: Best mean F1 score achieved\n",
        "    \"\"\"\n",
        "    # Generate all parameter combinations\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    combinations = list(product(*param_values))\n",
        "\n",
        "    results = {}\n",
        "    best_score = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    total = len(combinations)\n",
        "\n",
        "    for idx, combo in enumerate(combinations, 1):\n",
        "        # Create current configuration dict\n",
        "        current_config = dict(zip(param_names, combo))\n",
        "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nConfiguration {idx}/{total}:\")\n",
        "            for param, value in current_config.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "\n",
        "        # Merge current config with fixed parameters\n",
        "        run_params = {**fixed_params, **current_config}\n",
        "\n",
        "        # Execute cross-validation\n",
        "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "            df=df,\n",
        "            experiment_name=config_str,\n",
        "            **run_params,\n",
        "            **cv_params\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results[config_str] = fold_scores\n",
        "\n",
        "        # Track best configuration\n",
        "        if fold_scores[\"mean\"] > best_score:\n",
        "            best_score = fold_scores[\"mean\"]\n",
        "            best_config = current_config.copy()\n",
        "            if verbose:\n",
        "                print(\"  NEW BEST SCORE!\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  F1 Score: {fold_scores['mean']:.4f}Â±{fold_scores['std']:.4f}\")\n",
        "\n",
        "    return results, best_config, best_score\n",
        "\n",
        "\n",
        "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
        "\n",
        "    Args:\n",
        "        results: Dict of results from grid_search_cv_rnn\n",
        "        k_splits: Number of CV splits used\n",
        "        top_n: Number of top configurations to display\n",
        "        figsize: Figure size tuple\n",
        "    \"\"\"\n",
        "    # Sort by mean score\n",
        "    config_scores = {name: data['mean'] for name, data in results.items()}\n",
        "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top N\n",
        "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
        "\n",
        "    # Prepare boxplot data\n",
        "    boxplot_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
        "    replacements = {\n",
        "        'batch_size_': 'BS=',\n",
        "        'learning_rate_': '\\nLR=',\n",
        "        'hidden_layers_': '\\nHL=',\n",
        "        'hidden_size_': '\\nHS=',\n",
        "        'dropout_rate_': '\\nDR=',\n",
        "        'window_size_': '\\nWS=',\n",
        "        'stride_': '\\nSTR=',\n",
        "        'rnn_type_': '\\nRNN=',\n",
        "        'bidirectional_': '\\nBIDIR=',\n",
        "        'l1_lambda_': '\\nL1=',\n",
        "        'l2_lambda_': '\\nL2='\n",
        "    }\n",
        "\n",
        "    # Replacements for separators\n",
        "    separator_replacements = {\n",
        "        '_learning_rate_': '\\nLR=',\n",
        "        '_hidden_layers_': '\\nHL=',\n",
        "        '_hidden_size_': '\\nHS=',\n",
        "        '_dropout_rate_': '\\nDR=',\n",
        "        '_window_size_': '\\nWS=',\n",
        "        '_stride_': '\\nSTR=',\n",
        "        '_rnn_type_': '\\nRNN=',\n",
        "        '_bidirectional_': '\\nBIDIR=',\n",
        "        '_l1_lambda_': '\\nL1=',\n",
        "        '_l2_lambda_': '\\nL2=',\n",
        "        '_': ''\n",
        "    }\n",
        "\n",
        "    for config_name, mean_score in top_configs:\n",
        "        # Extract best score from each split (auto-detect number of splits)\n",
        "        split_scores = []\n",
        "        for i in range(k_splits):\n",
        "            if f'split_{i}' in results[config_name]:\n",
        "                split_scores.append(results[config_name][f'split_{i}'])\n",
        "        boxplot_data.append(split_scores)\n",
        "\n",
        "        # Verify we have the expected number of splits\n",
        "        if len(split_scores) != k_splits:\n",
        "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
        "\n",
        "        # Create readable label using the replacements dictionary\n",
        "        readable_label = config_name\n",
        "        for old, new in replacements.items():\n",
        "            readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        # Apply separator replacements\n",
        "        for old, new in separator_replacements.items():\n",
        "             readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        labels.append(f\"{readable_label}\\n(Î¼={mean_score:.3f})\")\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
        "                    showmeans=True, meanline=True)\n",
        "\n",
        "    # Styling\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    # Highlight best configuration\n",
        "    ax.get_xticklabels()[0].set_fontweight('bold')\n",
        "\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "    plt.xticks(rotation=0, ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lhgn3M-iQQSW"
      },
      "outputs": [],
      "source": [
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdF2fIryQQSW",
        "outputId": "48fd0bf9-e1b7-4664-a3e1-9a3cdb3abe04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Configuration 1/1:\n",
            "  window_size: 50\n",
            "  stride: 25\n",
            "  n_val_users: 30\n",
            "Built 4417 sequences with 4417 labels\n",
            "Built 210 sequences with 210 labels\n",
            "Training 1 epochs...\n",
            "Best model restored from epoch 1 with val_f1 0.8048\n",
            "Built 4417 sequences with 4417 labels\n",
            "Built 210 sequences with 210 labels\n",
            "Training 1 epochs...\n",
            "Best model restored from epoch 1 with val_f1 0.4912\n",
            "  NEW BEST SCORE!\n",
            "  F1 Score: 0.6480Â±0.1568\n",
            "CPU times: user 51.2 s, sys: 2.64 s, total: 53.8 s\n",
            "Wall time: 24.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Fixed hyperparameters (not being tuned)\n",
        "fixed_params = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'hidden_layers': HIDDEN_LAYERS,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'dropout_rate': DROPOUT_RATE,\n",
        "    'l1_lambda': L1_LAMBDA,\n",
        "    'l2_lambda': L2_LAMBDA,\n",
        "    'rnn_type': RNN_TYPE,\n",
        "    'bidirectional': BIDIRECTIONAL\n",
        "}\n",
        "\n",
        "# Cross-validation settings\n",
        "cv_params = {\n",
        "    'epochs': EPOCHS,\n",
        "    'criterion': criterion,\n",
        "    'device': device,\n",
        "    'k': K,\n",
        "    'patience': PATIENCE,\n",
        "    'verbose': 0,\n",
        "    'seed': SEED\n",
        "}\n",
        "\n",
        "# Execute search\n",
        "results, best_config, best_score = grid_search_cv_rnn(\n",
        "    df=X_train,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    cv_params=cv_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "705XWZjNQQSW",
        "outputId": "0e44a307-72af-45c0-da0d-0455f6236764"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_top_configurations_rnn(results, k_splits=K, top_n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVTner9GQQSb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2F2uGuuQQSb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuSbufzqQQSb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkklZJpQQSb"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Esv7nvO4QQSb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping joint_30 column (all NaN values)...\n",
            "Dropped joint_30 from (211840, 39)\n",
            "Preparing test data for prediction...\n",
            "Built 9268 sequences with 9268 labels\n",
            "Test sequences shape: (9268, 50, 37)\n",
            "Number of test sequences: 9268\n",
            "Using best config: window_size=50, stride=25\n",
            "\n",
            "Loading best model from: models/window_size_50_stride_25_n_val_users_30/split_0_model.pt\n",
            "Model configuration: {'batch_size': 512, 'learning_rate': 0.0005, 'hidden_layers': 2, 'hidden_size': 96, 'dropout_rate': 0.3, 'l1_lambda': 0, 'l2_lambda': 5e-05, 'rnn_type': 'GRU', 'bidirectional': True, 'window_size': 50, 'stride': 25, 'n_val_users': 30}\n",
            "âœ“ Model loaded successfully\n",
            "\n",
            "Making predictions on test set...\n",
            "Total window predictions: 9268\n",
            "\n",
            "Mapping 9268 window predictions to 1324 samples...\n",
            "Processed predictions: used 9268/9268 window predictions\n",
            "\n",
            "============================================================\n",
            "PREDICTIONS SAVED\n",
            "============================================================\n",
            "Output file: pirate_pain_test_predictions.csv\n",
            "Total samples predicted: 1324\n",
            "\n",
            "Best F1 Score (CV): 0.6480\n",
            "\n",
            "Prediction distribution:\n",
            "predicted_label\n",
            "no_pain    1324\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First 10 predictions:\n",
            "   sample_index  predicted_label_id predicted_label\n",
            "0             0                   0         no_pain\n",
            "1             1                   0         no_pain\n",
            "2             2                   0         no_pain\n",
            "3             3                   0         no_pain\n",
            "4             4                   0         no_pain\n",
            "5             5                   0         no_pain\n",
            "6             6                   0         no_pain\n",
            "7             7                   0         no_pain\n",
            "8             8                   0         no_pain\n",
            "9             9                   0         no_pain\n",
            "\n",
            "=== Prediction Summary ===\n",
            "no_pain: 1324 samples (100.0%)\n",
            "low_pain: 0 samples (0.0%)\n",
            "high_pain: 0 samples (0.0%)\n",
            "CPU times: user 22.3 s, sys: 292 ms, total: 22.6 s\n",
            "Wall time: 13.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "final_best_params = {**fixed_params, **best_config}\n",
        "\n",
        "X_test = pd.read_csv('pirate_pain_test.csv')\n",
        "\n",
        "# Drop joint_30 column (contains only NaN values)\n",
        "print(\"Dropping joint_30 column (all NaN values)...\")\n",
        "for df in [X_test]:\n",
        "    if 'joint_30' in df.columns:\n",
        "        df.drop('joint_30', axis=1, inplace=True)\n",
        "        print(f\"Dropped joint_30 from {df.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 1. Prepare test data without labels ---\n",
        "print(\"Preparing test data for prediction...\")\n",
        "\n",
        "\n",
        "num_classes = len(label_reverse_mapping)\n",
        "\n",
        "# Apply initial preprocessing steps as done for the training data\n",
        "# Drop joint_30 if it exists\n",
        "if 'joint_30' in X_test.columns:\n",
        "    X_test.drop('joint_30', axis=1, inplace=True)\n",
        "\n",
        "# Convert categorical variables to binary\n",
        "binary_cols = ['n_hands', 'n_eyes', 'n_legs']\n",
        "for col in binary_cols:\n",
        "    X_test[col] = X_test[col].map(lambda x: 1 if str(x).lower().strip() == 'two' else 0)\n",
        "\n",
        "# Store original test data before further preprocessing (for final CSV)\n",
        "df_test_original = X_test.copy()\n",
        "\n",
        "# --- 2. Get normalization parameters from ORIGINAL training data (X_train, not df_train) ---\n",
        "# Use the full training set before any splits\n",
        "pain_survey_columns = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes']\n",
        "joint_columns = [f'joint_{i:02d}' for i in range(30)]  # joint_00 through joint_29\n",
        "scale_columns = pain_survey_columns + joint_columns\n",
        "\n",
        "# Get normalization parameters from the FULL training data (before CV splits)\n",
        "# X_train is still a DataFrame (checked in kernel state)\n",
        "train_max = X_train[scale_columns].max()\n",
        "train_min = X_train[scale_columns].min()\n",
        "\n",
        "# --- 3. Apply preprocessing to test data ---\n",
        "df_test_processed = X_test.copy() # Use the reloaded, preprocessed test data\n",
        "\n",
        "# Apply same normalization as training\n",
        "for column in scale_columns:\n",
        "    df_test_processed[column] = (df_test_processed[column] - train_min[column]) / (train_max[column] - train_min[column] + 1e-8)\n",
        "\n",
        "# --- 4. Build test sequences using BEST parameters from grid search ---\n",
        "df_test_processed['label'] = 0  # Dummy label (required by build_sequences)\n",
        "\n",
        "# Use the best window and stride from grid search\n",
        "X_test_sequences, _ = build_sequences(\n",
        "    df_test_processed,\n",
        "    window=best_config['window_size'],\n",
        "    stride=best_config['stride']\n",
        ")\n",
        "\n",
        "print(f\"Test sequences shape: {X_test_sequences.shape}\")\n",
        "print(f\"Number of test sequences: {len(X_test_sequences)}\")\n",
        "print(f\"Using best config: window_size={best_config['window_size']}, stride={best_config['stride']}\")\n",
        "\n",
        "\n",
        "# --- 5. Initialize model with BEST parameters from grid search ---\n",
        "model = RecurrentClassifier(\n",
        "    input_size=X_test_sequences.shape[2],  # num_features (37)\n",
        "    hidden_size=final_best_params['hidden_size'],\n",
        "    num_layers=final_best_params['hidden_layers'],\n",
        "    num_classes=num_classes,  # 3 classes\n",
        "    dropout_rate=final_best_params['dropout_rate'],\n",
        "    bidirectional=final_best_params['bidirectional'],\n",
        "    rnn_type=final_best_params['rnn_type']\n",
        ").to(device)\n",
        "\n",
        "# --- 6. Load the BEST model from grid search ---\n",
        "# Use the model from the first split (split_0) of the best configuration\n",
        "best_config_str = \"_\".join([f\"{k}_{v}\" for k, v in best_config.items()])\n",
        "model_path = f\"models/{best_config_str}/split_0_model.pt\"\n",
        "\n",
        "print(f\"\\nLoading best model from: {model_path}\")\n",
        "print(f\"Model configuration: {final_best_params}\")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(f\"âœ“ Model loaded successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Model file not found at {model_path}\")\n",
        "    print(\"Available model files:\")\n",
        "    import glob\n",
        "    for f in glob.glob(\"models/**/*.pt\", recursive=True):\n",
        "        print(f\"  - {f}\")\n",
        "    raise\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# --- 7. Create test DataLoader ---\n",
        "test_ds = TensorDataset(torch.from_numpy(X_test_sequences.astype(np.float32)))\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=final_best_params['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=0 if LOCAL else 2\n",
        ")\n",
        "\n",
        "# --- 8. Make predictions ---\n",
        "print(\"\\nMaking predictions on test set...\")\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for (xb,) in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_predictions.append(preds)\n",
        "\n",
        "all_predictions = np.concatenate(all_predictions)\n",
        "print(f\"Total window predictions: {len(all_predictions)}\")\n",
        "\n",
        "# --- 9. Map predictions back to original test samples using EXACT build_sequences logic ---\n",
        "sample_predictions = []\n",
        "pred_idx = 0\n",
        "\n",
        "# Get unique sample IDs in the same order as they appear in the dataset\n",
        "unique_samples = df_test_processed['sample_index'].unique() # This should now work as df_test_processed is a DataFrame\n",
        "\n",
        "print(f\"\\nMapping {len(all_predictions)} window predictions to {len(unique_samples)} samples...\")\n",
        "\n",
        "window = best_config['window_size']\n",
        "stride = best_config['stride']\n",
        "\n",
        "for sample_id in unique_samples:\n",
        "    sample_data = df_test_processed[df_test_processed['sample_index'] == sample_id]\n",
        "    n_timestamps = len(sample_data)\n",
        "\n",
        "    # EXACT REPLICA of build_sequences padding logic\n",
        "    padding_len = window - n_timestamps % window if n_timestamps % window != 0 else 0\n",
        "    total_len = n_timestamps + padding_len\n",
        "\n",
        "    # EXACT REPLICA of build_sequences window counting logic\n",
        "    n_windows = 0\n",
        "    idx = 0\n",
        "    while idx + window <= total_len:\n",
        "        n_windows += 1\n",
        "        idx += stride\n",
        "\n",
        "    # Safety check: ensure we don't exceed available predictions\n",
        "    if pred_idx + n_windows > len(all_predictions):\n",
        "        print(f\"WARNING: Sample {sample_id} expects {n_windows} windows but only {len(all_predictions) - pred_idx} available\")\n",
        "        n_windows = max(0, len(all_predictions) - pred_idx)\n",
        "\n",
        "    # Get predictions for all windows of this sample\n",
        "    if n_windows > 0:\n",
        "        # Use integer slicing (numpy arrays only accept integers)\n",
        "        end_idx = int(pred_idx + n_windows)\n",
        "        sample_window_predictions = all_predictions[int(pred_idx):end_idx]\n",
        "\n",
        "        # Use majority voting across all windows\n",
        "        if len(sample_window_predictions) > 0:\n",
        "            final_prediction = np.bincount(sample_window_predictions).argmax()\n",
        "        else:\n",
        "            print(f\"WARNING: Empty predictions for sample {sample_id}, using default (0)\")\n",
        "            final_prediction = 0\n",
        "    else:\n",
        "        print(f\"WARNING: No predictions available for sample {sample_id}, using default (0)\")\n",
        "        final_prediction = 0\n",
        "\n",
        "    sample_predictions.append({\n",
        "        'sample_index': sample_id,\n",
        "        'predicted_label_id': final_prediction,\n",
        "        'predicted_label': label_reverse_mapping[final_prediction]\n",
        "    })\n",
        "\n",
        "    pred_idx += n_windows\n",
        "\n",
        "print(f\"Processed predictions: used {pred_idx}/{len(all_predictions)} window predictions\")\n",
        "\n",
        "# --- 10. Create output DataFrame ---\n",
        "predictions_df = pd.DataFrame(sample_predictions)\n",
        "\n",
        "output_df = df_test_original[['sample_index']].drop_duplicates().merge(\n",
        "    predictions_df,\n",
        "    on='sample_index',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Handle any missing predictions (shouldn't happen but safety check)\n",
        "if output_df['predicted_label'].isna().any():\n",
        "    print(f\"WARNING: {output_df['predicted_label'].isna().sum()} samples missing predictions, filling with 'no_pain'\")\n",
        "    output_df['predicted_label'].fillna('no_pain', inplace=True)\n",
        "\n",
        "# --- 11. Save predictions to CSV ---\n",
        "output_filename = 'pirate_pain_test_predictions.csv'\n",
        "output_df[['sample_index', 'predicted_label']].to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"PREDICTIONS SAVED\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Output file: {output_filename}\")\n",
        "print(f\"Total samples predicted: {len(output_df)}\")\n",
        "print(f\"\\nBest F1 Score (CV): {best_score:.4f}\")\n",
        "print(f\"\\nPrediction distribution:\")\n",
        "print(output_df['predicted_label'].value_counts())\n",
        "\n",
        "print(\"\\nFirst 10 predictions:\")\n",
        "print(output_df.head(10))\n",
        "\n",
        "print(\"\\n=== Prediction Summary ===\")\n",
        "for pain_level in ['no_pain', 'low_pain', 'high_pain']:\n",
        "    count = (output_df['predicted_label'] == pain_level).sum()\n",
        "    percentage = (count / len(output_df)) * 100\n",
        "    print(f\"{pain_level}: {count} samples ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-KsexxeUae42"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshutil\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[32m      4\u001b[39m zip_path = shutil.make_archive(\u001b[33m'\u001b[39m\u001b[33m/content/models\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mzip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m/content\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m files.download(zip_path)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = shutil.make_archive('/content/models', 'zip', '/content', 'models')\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ZrMlvpQQSc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
