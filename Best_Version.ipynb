{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")\n",
        "current_dir = \"/gdrive/My\\\\ Drive/Gesù/\"\n",
        "%cd $current_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDFhrqIp_7Ov",
        "outputId": "062d22ac-aff6-4862-fc5c-f6c964d6cd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Gesù\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8mDbjnp9Xtc",
        "outputId": "f66fcc45-2697-4409-d3a9-dbc6bb3a8ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "[build_sequences] Target T=180 -> ok:0  padded:661  truncated:0\n",
            "[build_sequences] Target T=180 -> ok:0  padded:1324  truncated:0\n",
            "Train sequences: 661  Test sequences: 1324\n",
            "Dynamic channels: 35  Static dims: 3  Classes: ['high_pain', 'low_pain', 'no_pain']\n",
            "\n",
            "========== FOLD 1/5 ==========\n",
            "Epoch 01: train loss 1.1696 f1 0.1519 acc 0.1591 | val loss 1.1611 f1 0.1499 acc 0.1353\n",
            "Epoch 02: train loss 1.1611 f1 0.1208 acc 0.1364 | val loss 1.1559 f1 0.1341 acc 0.1203\n",
            "Epoch 03: train loss 1.1510 f1 0.1144 acc 0.1231 | val loss 1.1490 f1 0.1274 acc 0.1128\n",
            "Epoch 04: train loss 1.1509 f1 0.0949 acc 0.1061 | val loss 1.1406 f1 0.1176 acc 0.1053\n",
            "Epoch 05: train loss 1.1403 f1 0.1496 acc 0.1477 | val loss 1.1281 f1 0.1860 acc 0.1729\n",
            "Epoch 06: train loss 1.1253 f1 0.2090 acc 0.2235 | val loss 1.1057 f1 0.3889 acc 0.4361\n",
            "Epoch 07: train loss 1.0964 f1 0.3651 acc 0.4489 | val loss 1.0726 f1 0.3998 acc 0.4662\n",
            "Epoch 08: train loss 1.0429 f1 0.4638 acc 0.5966 | val loss 1.0179 f1 0.4797 acc 0.5865\n",
            "Epoch 09: train loss 0.9809 f1 0.5718 acc 0.7159 | val loss 0.9750 f1 0.5000 acc 0.6090\n",
            "Epoch 10: train loss 0.8587 f1 0.5707 acc 0.6932 | val loss 0.9481 f1 0.5789 acc 0.7143\n",
            "Epoch 11: train loss 0.7986 f1 0.6788 acc 0.7936 | val loss 0.9961 f1 0.6070 acc 0.7519\n",
            "Epoch 12: train loss 0.6976 f1 0.7068 acc 0.7992 | val loss 0.9262 f1 0.6924 acc 0.8346\n",
            "Epoch 13: train loss 0.6795 f1 0.7950 acc 0.8807 | val loss 0.9418 f1 0.6817 acc 0.7820\n",
            "Epoch 14: train loss 0.6549 f1 0.7145 acc 0.8049 | val loss 0.8729 f1 0.7104 acc 0.8346\n",
            "Epoch 15: train loss 0.5982 f1 0.8064 acc 0.8807 | val loss 0.8544 f1 0.6392 acc 0.7368\n",
            "Epoch 16: train loss 0.5489 f1 0.7906 acc 0.8712 | val loss 0.7215 f1 0.7991 acc 0.9098\n",
            "Epoch 17: train loss 0.5710 f1 0.7785 acc 0.8580 | val loss 0.8593 f1 0.6757 acc 0.7744\n",
            "Epoch 18: train loss 0.5358 f1 0.8887 acc 0.9337 | val loss 0.8141 f1 0.7087 acc 0.8195\n",
            "Epoch 19: train loss 0.4760 f1 0.8611 acc 0.9167 | val loss 0.8765 f1 0.7909 acc 0.8872\n",
            "Epoch 20: train loss 0.5252 f1 0.8708 acc 0.9280 | val loss 0.8459 f1 0.7633 acc 0.8797\n",
            "Epoch 21: train loss 0.4487 f1 0.9091 acc 0.9470 | val loss 0.8504 f1 0.7418 acc 0.8496\n",
            "Epoch 22: train loss 0.4897 f1 0.9020 acc 0.9432 | val loss 0.9183 f1 0.7081 acc 0.8421\n",
            "Epoch 23: train loss 0.5817 f1 0.8951 acc 0.9489 | val loss 0.6951 f1 0.8099 acc 0.9173\n",
            "Epoch 24: train loss 0.4335 f1 0.8921 acc 0.9337 | val loss 0.9129 f1 0.7459 acc 0.8722\n",
            "Epoch 25: train loss 0.4960 f1 0.9030 acc 0.9489 | val loss 0.8739 f1 0.6830 acc 0.7970\n",
            "Epoch 26: train loss 0.4661 f1 0.8557 acc 0.9148 | val loss 0.6919 f1 0.7892 acc 0.8872\n",
            "Epoch 27: train loss 0.3980 f1 0.9487 acc 0.9697 | val loss 0.7089 f1 0.8297 acc 0.9323\n",
            "Epoch 28: train loss 0.3837 f1 0.9513 acc 0.9716 | val loss 0.7696 f1 0.8223 acc 0.9248\n",
            "Epoch 29: train loss 0.4470 f1 0.9357 acc 0.9678 | val loss 0.7082 f1 0.8258 acc 0.9248\n",
            "Epoch 30: train loss 0.4338 f1 0.9105 acc 0.9489 | val loss 0.8461 f1 0.7475 acc 0.9023\n",
            "Epoch 31: train loss 0.3708 f1 0.9798 acc 0.9886 | val loss 0.6947 f1 0.7945 acc 0.8872\n",
            "Epoch 32: train loss 0.3731 f1 0.9674 acc 0.9830 | val loss 0.7277 f1 0.8392 acc 0.9248\n",
            "Epoch 33: train loss 0.3834 f1 0.9812 acc 0.9886 | val loss 0.6246 f1 0.8382 acc 0.9173\n",
            "Epoch 34: train loss 0.3811 f1 0.9642 acc 0.9811 | val loss 0.6630 f1 0.8500 acc 0.9398\n",
            "Epoch 35: train loss 0.3663 f1 0.9832 acc 0.9886 | val loss 0.6579 f1 0.8288 acc 0.9098\n",
            "Epoch 36: train loss 0.3635 f1 0.9612 acc 0.9792 | val loss 0.7446 f1 0.8222 acc 0.9248\n",
            "Epoch 37: train loss 0.3733 f1 0.9888 acc 0.9943 | val loss 0.7007 f1 0.8141 acc 0.9248\n",
            "Epoch 38: train loss 0.3468 f1 0.9826 acc 0.9905 | val loss 0.7502 f1 0.7789 acc 0.9173\n",
            "Epoch 39: train loss 0.3518 f1 0.9933 acc 0.9962 | val loss 0.7076 f1 0.8470 acc 0.9398\n",
            "Epoch 40: train loss 0.3478 f1 0.9878 acc 0.9943 | val loss 0.7071 f1 0.8500 acc 0.9398\n",
            "Epoch 41: train loss 0.3528 f1 0.9916 acc 0.9962 | val loss 0.6982 f1 0.8500 acc 0.9398\n",
            "Epoch 42: train loss 0.3416 f1 1.0000 acc 1.0000 | val loss 0.6905 f1 0.8625 acc 0.9398\n",
            "Epoch 43: train loss 0.3533 f1 0.9974 acc 0.9981 | val loss 0.6880 f1 0.8500 acc 0.9398\n",
            "Epoch 44: train loss 0.3409 f1 0.9974 acc 0.9981 | val loss 0.7060 f1 0.8470 acc 0.9398\n",
            "Epoch 45: train loss 0.3423 f1 0.9949 acc 0.9962 | val loss 0.7226 f1 0.8675 acc 0.9474\n",
            "Epoch 46: train loss 0.3466 f1 1.0000 acc 1.0000 | val loss 0.7293 f1 0.8582 acc 0.9398\n",
            "Epoch 47: train loss 0.3401 f1 1.0000 acc 1.0000 | val loss 0.7091 f1 0.8714 acc 0.9474\n",
            "Epoch 48: train loss 0.3367 f1 1.0000 acc 1.0000 | val loss 0.6974 f1 0.8470 acc 0.9398\n",
            "Epoch 49: train loss 0.3440 f1 1.0000 acc 1.0000 | val loss 0.7069 f1 0.8500 acc 0.9398\n",
            "Epoch 50: train loss 0.3515 f1 0.9932 acc 0.9962 | val loss 0.7001 f1 0.8470 acc 0.9398\n",
            "Epoch 51: train loss 0.3522 f1 0.9947 acc 0.9962 | val loss 0.7139 f1 0.8714 acc 0.9474\n",
            "Epoch 52: train loss 0.3430 f1 0.9974 acc 0.9981 | val loss 0.7172 f1 0.8350 acc 0.9323\n",
            "Epoch 53: train loss 0.3384 f1 1.0000 acc 1.0000 | val loss 0.7043 f1 0.8582 acc 0.9398\n",
            "Epoch 54: train loss 0.3419 f1 0.9949 acc 0.9962 | val loss 0.7135 f1 0.8625 acc 0.9398\n",
            "Epoch 55: train loss 0.3397 f1 0.9974 acc 0.9981 | val loss 0.7223 f1 0.8625 acc 0.9398\n",
            "Epoch 56: train loss 0.3442 f1 0.9974 acc 0.9981 | val loss 0.7187 f1 0.8625 acc 0.9398\n",
            "Epoch 57: train loss 0.3546 f1 0.9974 acc 0.9981 | val loss 0.7073 f1 0.8378 acc 0.9323\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 2/5 ==========\n",
            "Epoch 01: train loss 1.1665 f1 0.1544 acc 0.1474 | val loss 1.1686 f1 0.0513 acc 0.0833\n",
            "Epoch 02: train loss 1.1578 f1 0.1655 acc 0.1512 | val loss 1.1668 f1 0.0516 acc 0.0833\n",
            "Epoch 03: train loss 1.1623 f1 0.1247 acc 0.1134 | val loss 1.1635 f1 0.0516 acc 0.0833\n",
            "Epoch 04: train loss 1.1507 f1 0.1376 acc 0.1229 | val loss 1.1585 f1 0.0528 acc 0.0833\n",
            "Epoch 05: train loss 1.1398 f1 0.1569 acc 0.1456 | val loss 1.1475 f1 0.1808 acc 0.1818\n",
            "Epoch 06: train loss 1.1284 f1 0.2136 acc 0.2117 | val loss 1.1227 f1 0.3269 acc 0.3561\n",
            "Epoch 07: train loss 1.0932 f1 0.3753 acc 0.4216 | val loss 1.0952 f1 0.3678 acc 0.5530\n",
            "Epoch 08: train loss 1.0308 f1 0.4647 acc 0.6106 | val loss 1.0705 f1 0.4092 acc 0.5985\n",
            "Epoch 09: train loss 0.9547 f1 0.5526 acc 0.7127 | val loss 0.9932 f1 0.5120 acc 0.7348\n",
            "Epoch 10: train loss 0.8662 f1 0.6107 acc 0.7278 | val loss 0.9849 f1 0.5493 acc 0.7652\n",
            "Epoch 11: train loss 0.7651 f1 0.7335 acc 0.8469 | val loss 1.0386 f1 0.5143 acc 0.7879\n",
            "Epoch 12: train loss 0.6685 f1 0.6850 acc 0.7769 | val loss 1.0520 f1 0.5684 acc 0.8106\n",
            "Epoch 13: train loss 0.6263 f1 0.7536 acc 0.8393 | val loss 0.9901 f1 0.5619 acc 0.7727\n",
            "Epoch 14: train loss 0.5803 f1 0.7691 acc 0.8488 | val loss 0.7959 f1 0.6865 acc 0.8258\n",
            "Epoch 15: train loss 0.6216 f1 0.7793 acc 0.8658 | val loss 0.8732 f1 0.6501 acc 0.7879\n",
            "Epoch 16: train loss 0.4945 f1 0.8421 acc 0.8979 | val loss 0.7994 f1 0.7186 acc 0.8485\n",
            "Epoch 17: train loss 0.4951 f1 0.8376 acc 0.8960 | val loss 0.9100 f1 0.6827 acc 0.8333\n",
            "Epoch 18: train loss 0.4776 f1 0.8699 acc 0.9206 | val loss 0.9834 f1 0.7049 acc 0.8561\n",
            "Epoch 19: train loss 0.5048 f1 0.8737 acc 0.9282 | val loss 0.9802 f1 0.6501 acc 0.7727\n",
            "Epoch 20: train loss 0.4379 f1 0.8803 acc 0.9244 | val loss 0.8675 f1 0.7867 acc 0.9015\n",
            "Epoch 21: train loss 0.4558 f1 0.9378 acc 0.9698 | val loss 0.9651 f1 0.6459 acc 0.7576\n",
            "Epoch 22: train loss 0.4219 f1 0.9303 acc 0.9641 | val loss 0.9425 f1 0.7515 acc 0.9015\n",
            "Epoch 23: train loss 0.4087 f1 0.9189 acc 0.9527 | val loss 0.9784 f1 0.7297 acc 0.8712\n",
            "Epoch 24: train loss 0.4135 f1 0.9373 acc 0.9641 | val loss 0.8886 f1 0.7810 acc 0.9015\n",
            "Epoch 25: train loss 0.4532 f1 0.9104 acc 0.9546 | val loss 1.1429 f1 0.6864 acc 0.8712\n",
            "Epoch 26: train loss 0.4395 f1 0.9524 acc 0.9773 | val loss 0.9350 f1 0.7116 acc 0.8712\n",
            "Epoch 27: train loss 0.3885 f1 0.9656 acc 0.9811 | val loss 0.9984 f1 0.7381 acc 0.8636\n",
            "Epoch 28: train loss 0.3772 f1 0.9697 acc 0.9830 | val loss 1.0413 f1 0.7020 acc 0.8561\n",
            "Epoch 29: train loss 0.3658 f1 0.9790 acc 0.9887 | val loss 0.9128 f1 0.7331 acc 0.8788\n",
            "Epoch 30: train loss 0.3620 f1 0.9760 acc 0.9887 | val loss 0.9570 f1 0.7410 acc 0.9015\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 3/5 ==========\n",
            "Epoch 01: train loss 1.1769 f1 0.2799 acc 0.5066 | val loss 1.1706 f1 0.3192 acc 0.5758\n",
            "Epoch 02: train loss 1.1668 f1 0.2738 acc 0.4310 | val loss 1.1653 f1 0.1422 acc 0.1970\n",
            "Epoch 03: train loss 1.1642 f1 0.2000 acc 0.2382 | val loss 1.1560 f1 0.0714 acc 0.1061\n",
            "Epoch 04: train loss 1.1531 f1 0.1260 acc 0.1493 | val loss 1.1449 f1 0.0714 acc 0.1061\n",
            "Epoch 05: train loss 1.1460 f1 0.1257 acc 0.1267 | val loss 1.1300 f1 0.1743 acc 0.1667\n",
            "Epoch 06: train loss 1.1297 f1 0.1861 acc 0.1739 | val loss 1.1088 f1 0.2936 acc 0.3182\n",
            "Epoch 07: train loss 1.1113 f1 0.3884 acc 0.4877 | val loss 1.0768 f1 0.5246 acc 0.6818\n",
            "Epoch 08: train loss 1.0587 f1 0.5060 acc 0.6767 | val loss 1.0338 f1 0.5407 acc 0.7197\n",
            "Epoch 09: train loss 0.9982 f1 0.5419 acc 0.7032 | val loss 0.9864 f1 0.5524 acc 0.7348\n",
            "Epoch 10: train loss 0.8927 f1 0.6211 acc 0.7750 | val loss 0.9511 f1 0.5652 acc 0.7727\n",
            "Epoch 11: train loss 0.8215 f1 0.6259 acc 0.7505 | val loss 0.9751 f1 0.6128 acc 0.8182\n",
            "Epoch 12: train loss 0.7300 f1 0.7198 acc 0.8299 | val loss 0.9155 f1 0.6610 acc 0.8409\n",
            "Epoch 13: train loss 0.6597 f1 0.7212 acc 0.8204 | val loss 0.7946 f1 0.7390 acc 0.8485\n",
            "Epoch 14: train loss 0.6677 f1 0.7538 acc 0.8469 | val loss 0.9214 f1 0.7427 acc 0.8561\n",
            "Epoch 15: train loss 0.6082 f1 0.7971 acc 0.8885 | val loss 0.8198 f1 0.7818 acc 0.8864\n",
            "Epoch 16: train loss 0.5833 f1 0.7902 acc 0.8790 | val loss 0.8149 f1 0.6608 acc 0.7955\n",
            "Epoch 17: train loss 0.5185 f1 0.8445 acc 0.9130 | val loss 0.8926 f1 0.7723 acc 0.8636\n",
            "Epoch 18: train loss 0.5236 f1 0.8292 acc 0.9055 | val loss 0.8313 f1 0.7641 acc 0.8788\n",
            "Epoch 19: train loss 0.5193 f1 0.8964 acc 0.9509 | val loss 0.8387 f1 0.7614 acc 0.8712\n",
            "Epoch 20: train loss 0.4662 f1 0.8775 acc 0.9301 | val loss 0.9080 f1 0.7566 acc 0.8788\n",
            "Epoch 21: train loss 0.4518 f1 0.9211 acc 0.9565 | val loss 0.8853 f1 0.7638 acc 0.8788\n",
            "Epoch 22: train loss 0.4593 f1 0.9284 acc 0.9603 | val loss 0.9162 f1 0.6722 acc 0.8258\n",
            "Epoch 23: train loss 0.4056 f1 0.9313 acc 0.9660 | val loss 0.8732 f1 0.7891 acc 0.8864\n",
            "Epoch 24: train loss 0.4145 f1 0.9396 acc 0.9679 | val loss 0.8726 f1 0.7880 acc 0.8939\n",
            "Epoch 25: train loss 0.3877 f1 0.9600 acc 0.9792 | val loss 0.9575 f1 0.7720 acc 0.8864\n",
            "Epoch 26: train loss 0.3824 f1 0.9398 acc 0.9698 | val loss 0.9712 f1 0.7539 acc 0.8864\n",
            "Epoch 27: train loss 0.4126 f1 0.9576 acc 0.9792 | val loss 1.0403 f1 0.7664 acc 0.8864\n",
            "Epoch 28: train loss 0.4216 f1 0.9421 acc 0.9716 | val loss 0.8808 f1 0.7693 acc 0.8864\n",
            "Epoch 29: train loss 0.3699 f1 0.9914 acc 0.9962 | val loss 0.9033 f1 0.7799 acc 0.8939\n",
            "Epoch 30: train loss 0.3659 f1 0.9615 acc 0.9811 | val loss 0.8669 f1 0.8228 acc 0.9091\n",
            "Epoch 31: train loss 0.3835 f1 0.9718 acc 0.9868 | val loss 0.7992 f1 0.8210 acc 0.9091\n",
            "Epoch 32: train loss 0.3598 f1 0.9787 acc 0.9887 | val loss 0.8276 f1 0.8124 acc 0.9015\n",
            "Epoch 33: train loss 0.3595 f1 0.9915 acc 0.9962 | val loss 0.8542 f1 0.7916 acc 0.8864\n",
            "Epoch 34: train loss 0.3639 f1 0.9791 acc 0.9868 | val loss 0.8223 f1 0.8050 acc 0.9015\n",
            "Epoch 35: train loss 0.3575 f1 0.9878 acc 0.9943 | val loss 0.8879 f1 0.8077 acc 0.9015\n",
            "Epoch 36: train loss 0.3569 f1 0.9752 acc 0.9868 | val loss 0.8905 f1 0.7803 acc 0.8864\n",
            "Epoch 37: train loss 0.3434 f1 1.0000 acc 1.0000 | val loss 0.8442 f1 0.7925 acc 0.8939\n",
            "Epoch 38: train loss 0.3561 f1 0.9842 acc 0.9924 | val loss 0.8688 f1 0.7596 acc 0.8788\n",
            "Epoch 39: train loss 0.3473 f1 0.9959 acc 0.9981 | val loss 0.8840 f1 0.7803 acc 0.8864\n",
            "Epoch 40: train loss 0.3686 f1 0.9932 acc 0.9962 | val loss 0.8989 f1 0.7752 acc 0.8864\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 4/5 ==========\n",
            "Epoch 01: train loss 1.1784 f1 0.2897 acc 0.7656 | val loss 1.1710 f1 0.2906 acc 0.7727\n",
            "Epoch 02: train loss 1.1661 f1 0.3397 acc 0.7486 | val loss 1.1674 f1 0.2890 acc 0.7652\n",
            "Epoch 03: train loss 1.1626 f1 0.3783 acc 0.6049 | val loss 1.1601 f1 0.3838 acc 0.4924\n",
            "Epoch 04: train loss 1.1565 f1 0.2836 acc 0.3214 | val loss 1.1523 f1 0.2914 acc 0.2955\n",
            "Epoch 05: train loss 1.1369 f1 0.2589 acc 0.2533 | val loss 1.1380 f1 0.2331 acc 0.2348\n",
            "Epoch 06: train loss 1.1104 f1 0.2363 acc 0.2287 | val loss 1.1152 f1 0.2901 acc 0.3030\n",
            "Epoch 07: train loss 1.0618 f1 0.3370 acc 0.3535 | val loss 1.0689 f1 0.3547 acc 0.3864\n",
            "Epoch 08: train loss 0.9973 f1 0.4177 acc 0.4707 | val loss 1.0310 f1 0.4328 acc 0.5758\n",
            "Epoch 09: train loss 0.9448 f1 0.5095 acc 0.6295 | val loss 0.9795 f1 0.4904 acc 0.6439\n",
            "Epoch 10: train loss 0.8782 f1 0.5349 acc 0.6144 | val loss 0.8893 f1 0.5834 acc 0.7576\n",
            "Epoch 11: train loss 0.7964 f1 0.6626 acc 0.7958 | val loss 0.7819 f1 0.6536 acc 0.8333\n",
            "Epoch 12: train loss 0.7284 f1 0.6568 acc 0.7580 | val loss 0.8304 f1 0.6622 acc 0.8333\n",
            "Epoch 13: train loss 0.7091 f1 0.7108 acc 0.8129 | val loss 0.7068 f1 0.7588 acc 0.8712\n",
            "Epoch 14: train loss 0.6178 f1 0.7793 acc 0.8563 | val loss 0.6771 f1 0.7991 acc 0.8864\n",
            "Epoch 15: train loss 0.6523 f1 0.7440 acc 0.8299 | val loss 0.5848 f1 0.8160 acc 0.9015\n",
            "Epoch 16: train loss 0.5326 f1 0.8705 acc 0.9263 | val loss 0.6229 f1 0.7144 acc 0.8106\n",
            "Epoch 17: train loss 0.5243 f1 0.8171 acc 0.8847 | val loss 0.6697 f1 0.8506 acc 0.9242\n",
            "Epoch 18: train loss 0.4929 f1 0.8880 acc 0.9395 | val loss 0.6779 f1 0.7258 acc 0.8333\n",
            "Epoch 19: train loss 0.5069 f1 0.8552 acc 0.9168 | val loss 0.8700 f1 0.7391 acc 0.8939\n",
            "Epoch 20: train loss 0.5288 f1 0.8706 acc 0.9301 | val loss 0.6505 f1 0.7526 acc 0.8712\n",
            "Epoch 21: train loss 0.4718 f1 0.9146 acc 0.9546 | val loss 0.6106 f1 0.7658 acc 0.8788\n",
            "Epoch 22: train loss 0.4193 f1 0.9489 acc 0.9716 | val loss 0.6570 f1 0.7794 acc 0.8788\n",
            "Epoch 23: train loss 0.3997 f1 0.9526 acc 0.9754 | val loss 0.6539 f1 0.7711 acc 0.8788\n",
            "Epoch 24: train loss 0.4202 f1 0.9266 acc 0.9584 | val loss 0.6964 f1 0.7938 acc 0.9015\n",
            "Epoch 25: train loss 0.3967 f1 0.9514 acc 0.9735 | val loss 0.7445 f1 0.7201 acc 0.8030\n",
            "Epoch 26: train loss 0.4138 f1 0.9389 acc 0.9679 | val loss 0.6392 f1 0.8300 acc 0.9167\n",
            "Epoch 27: train loss 0.3921 f1 0.9199 acc 0.9546 | val loss 0.5958 f1 0.8146 acc 0.9091\n",
            "Early stopping.\n",
            "\n",
            "========== FOLD 5/5 ==========\n",
            "Epoch 01: train loss 1.1762 f1 0.3302 acc 0.7372 | val loss 1.1731 f1 0.2906 acc 0.7727\n",
            "Epoch 02: train loss 1.1651 f1 0.3458 acc 0.6919 | val loss 1.1661 f1 0.3404 acc 0.7576\n",
            "Epoch 03: train loss 1.1656 f1 0.3515 acc 0.5558 | val loss 1.1566 f1 0.4920 acc 0.7424\n",
            "Epoch 04: train loss 1.1464 f1 0.3325 acc 0.4008 | val loss 1.1448 f1 0.3152 acc 0.3258\n",
            "Epoch 05: train loss 1.1323 f1 0.2607 acc 0.2609 | val loss 1.1290 f1 0.2357 acc 0.2273\n",
            "Epoch 06: train loss 1.1067 f1 0.2866 acc 0.2892 | val loss 1.1086 f1 0.2770 acc 0.2955\n",
            "Epoch 07: train loss 1.0685 f1 0.3965 acc 0.4669 | val loss 1.0797 f1 0.4330 acc 0.5985\n",
            "Epoch 08: train loss 1.0175 f1 0.5530 acc 0.7089 | val loss 1.0451 f1 0.4634 acc 0.6515\n",
            "Epoch 09: train loss 0.9310 f1 0.5694 acc 0.6881 | val loss 0.9996 f1 0.5045 acc 0.6515\n",
            "Epoch 10: train loss 0.8299 f1 0.6317 acc 0.7580 | val loss 1.0160 f1 0.5823 acc 0.7576\n",
            "Epoch 11: train loss 0.7434 f1 0.6643 acc 0.7656 | val loss 1.0464 f1 0.5982 acc 0.7576\n",
            "Epoch 12: train loss 0.7319 f1 0.7505 acc 0.8563 | val loss 1.0313 f1 0.5884 acc 0.7424\n",
            "Epoch 13: train loss 0.6198 f1 0.7465 acc 0.8242 | val loss 0.9265 f1 0.6468 acc 0.7803\n",
            "Epoch 14: train loss 0.5886 f1 0.8066 acc 0.8828 | val loss 1.0272 f1 0.6639 acc 0.8333\n",
            "Epoch 15: train loss 0.5929 f1 0.7976 acc 0.8733 | val loss 0.9778 f1 0.6380 acc 0.7727\n",
            "Epoch 16: train loss 0.5365 f1 0.8066 acc 0.8790 | val loss 0.9662 f1 0.6947 acc 0.8333\n",
            "Epoch 17: train loss 0.4806 f1 0.8805 acc 0.9301 | val loss 1.0406 f1 0.5937 acc 0.8030\n",
            "Epoch 18: train loss 0.5216 f1 0.8482 acc 0.9130 | val loss 0.9719 f1 0.6318 acc 0.7727\n",
            "Epoch 19: train loss 0.5292 f1 0.8501 acc 0.9130 | val loss 0.9427 f1 0.6357 acc 0.8106\n",
            "Epoch 20: train loss 0.5189 f1 0.8195 acc 0.8809 | val loss 0.9230 f1 0.6856 acc 0.8258\n",
            "Epoch 21: train loss 0.5210 f1 0.9075 acc 0.9546 | val loss 0.9037 f1 0.6337 acc 0.7652\n",
            "Epoch 22: train loss 0.4617 f1 0.8810 acc 0.9319 | val loss 1.0158 f1 0.7133 acc 0.8636\n",
            "Epoch 23: train loss 0.4592 f1 0.9081 acc 0.9490 | val loss 0.9422 f1 0.6276 acc 0.8030\n",
            "Epoch 24: train loss 0.4268 f1 0.9229 acc 0.9603 | val loss 1.0013 f1 0.6618 acc 0.8561\n",
            "Epoch 25: train loss 0.4088 f1 0.9272 acc 0.9584 | val loss 0.8752 f1 0.7368 acc 0.8788\n",
            "Epoch 26: train loss 0.4113 f1 0.9272 acc 0.9584 | val loss 0.9791 f1 0.7353 acc 0.8788\n",
            "Epoch 27: train loss 0.4224 f1 0.9518 acc 0.9735 | val loss 1.0077 f1 0.6984 acc 0.8712\n",
            "Epoch 28: train loss 0.3686 f1 0.9791 acc 0.9868 | val loss 0.8356 f1 0.6965 acc 0.8485\n",
            "Epoch 29: train loss 0.3849 f1 0.9293 acc 0.9622 | val loss 1.0790 f1 0.6816 acc 0.8561\n",
            "Epoch 30: train loss 0.4030 f1 0.9559 acc 0.9773 | val loss 0.9619 f1 0.6602 acc 0.8561\n",
            "Epoch 31: train loss 0.3736 f1 0.9787 acc 0.9887 | val loss 0.9971 f1 0.6712 acc 0.8712\n",
            "Epoch 32: train loss 0.3615 f1 0.9648 acc 0.9792 | val loss 1.0711 f1 0.6536 acc 0.8636\n",
            "Epoch 33: train loss 0.3752 f1 0.9888 acc 0.9943 | val loss 0.9325 f1 0.7028 acc 0.8636\n",
            "Epoch 34: train loss 0.3564 f1 0.9653 acc 0.9811 | val loss 0.8894 f1 0.7621 acc 0.8864\n",
            "Epoch 35: train loss 0.3511 f1 0.9948 acc 0.9962 | val loss 1.0030 f1 0.6769 acc 0.8712\n",
            "Epoch 36: train loss 0.3566 f1 0.9856 acc 0.9905 | val loss 0.8746 f1 0.7144 acc 0.8561\n",
            "Epoch 37: train loss 0.3426 f1 0.9948 acc 0.9962 | val loss 0.8925 f1 0.7268 acc 0.8712\n",
            "Epoch 38: train loss 0.3495 f1 0.9933 acc 0.9962 | val loss 0.9440 f1 0.7118 acc 0.8712\n",
            "Epoch 39: train loss 0.3531 f1 0.9867 acc 0.9924 | val loss 0.9366 f1 0.7335 acc 0.8788\n",
            "Epoch 40: train loss 0.3453 f1 0.9948 acc 0.9962 | val loss 0.9472 f1 0.7335 acc 0.8788\n",
            "Epoch 41: train loss 0.3493 f1 0.9907 acc 0.9943 | val loss 0.9131 f1 0.7047 acc 0.8712\n",
            "Epoch 42: train loss 0.3460 f1 0.9958 acc 0.9981 | val loss 0.8901 f1 0.7288 acc 0.8788\n",
            "Epoch 43: train loss 0.3519 f1 0.9918 acc 0.9962 | val loss 0.8684 f1 0.7543 acc 0.8864\n",
            "Epoch 44: train loss 0.3410 f1 0.9893 acc 0.9943 | val loss 0.8805 f1 0.7543 acc 0.8864\n",
            "Early stopping.\n",
            "\n",
            "OOF macro-F1: 0.8191 | OOF Acc: 0.9138\n",
            "Wrote submission.csv\n"
          ]
        }
      ],
      "source": [
        "# pirate_pain_baseline.py\n",
        "# Train-from-scratch baseline for Pirate Pain (multivariate time-series classification)\n",
        "# Requires: pandas, numpy, scikit-learn, torch, tqdm\n",
        "# Tested on CPU/MPS (Apple Silicon) and CUDA if available.\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def seed_everything(seed=42):\n",
        "    import random, os\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# Data shaping\n",
        "# ---------------------------\n",
        "def infer_columns(df):\n",
        "    id_col = \"sample_index\"\n",
        "    time_col = \"time\" if \"time\" in df.columns else None\n",
        "    static_candidates = [\"n_legs\",\"n_hands\",\"n_eyes\"]\n",
        "    static_cols = [c for c in static_candidates if c in df.columns]\n",
        "    ignore = set([id_col] + ([time_col] if time_col else []) + static_cols)\n",
        "    feature_cols = [c for c in df.columns if c not in ignore]\n",
        "    return id_col, time_col, static_cols, feature_cols\n",
        "\n",
        "def _numericize_features(df, cols):\n",
        "    \"\"\"Return a numeric version of df[cols], mapping common words to numbers and dropping all-NaN cols.\"\"\"\n",
        "    mapping = {\n",
        "        \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
        "        \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
        "        \"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0,\n",
        "        \"none\": None, \"null\": None, \"nan\": None, \"\": None\n",
        "    }\n",
        "    out = df[cols].copy()\n",
        "    for c in out.columns:\n",
        "        if out[c].dtype == object:\n",
        "            s = out[c].astype(str).str.strip().str.lower().replace(mapping)\n",
        "            out[c] = pd.to_numeric(s, errors=\"coerce\")\n",
        "        else:\n",
        "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
        "    keep = out.columns[out.notna().any()].tolist()\n",
        "    dropped = [c for c in out.columns if c not in keep]\n",
        "    if dropped:\n",
        "        print(f\"[build_sequences] Dropping non-numeric/all-NaN features: {dropped[:15]}\" + (\" ...\" if len(dropped) > 15 else \"\"))\n",
        "    return out[keep], keep\n",
        "\n",
        "def build_sequences(X_df, y_df=None, expect_T=180):\n",
        "    id_col = \"sample_index\"\n",
        "    time_col = \"time\" if \"time\" in X_df.columns else None\n",
        "    static_candidates = [\"n_legs\",\"n_hands\",\"n_eyes\"]\n",
        "    static_cols = [c for c in static_candidates if c in X_df.columns]\n",
        "    ignore = set([id_col] + ([time_col] if time_col else []) + static_cols)\n",
        "    ignore |= {\"label\",\"target\",\"class\"}\n",
        "    raw_dyn_cols = [c for c in X_df.columns if c not in ignore]\n",
        "\n",
        "    if time_col is not None:\n",
        "        X_df = X_df.sort_values([id_col, time_col])\n",
        "    else:\n",
        "        X_df = X_df.sort_values([id_col])\n",
        "\n",
        "    dyn_numeric, dyn_cols = _numericize_features(X_df, raw_dyn_cols)\n",
        "    for c in dyn_cols:\n",
        "        X_df[c] = dyn_numeric[c].values\n",
        "\n",
        "    groups = X_df.groupby(id_col)\n",
        "    sample_ids = []\n",
        "    lengths = []                     # <--- NEW\n",
        "    X_dyn_list, X_static_list = [], []\n",
        "\n",
        "    def fix_len(arr, T):\n",
        "        n = arr.shape[0]\n",
        "        if n == T: return arr, \"ok\"\n",
        "        if n > T:  return arr[-T:, :], \"trunc\"\n",
        "        pad = np.repeat(arr[-1:, :], T - n, axis=0)\n",
        "        return np.concatenate([arr, pad], axis=0), \"pad\"\n",
        "\n",
        "    n_ok = n_pad = n_trunc = 0\n",
        "\n",
        "    for s_id, g in groups:\n",
        "        g_dyn = g[dyn_cols].ffill().bfill().fillna(0.0)\n",
        "        arr0 = g_dyn.to_numpy(dtype=np.float32)\n",
        "        true_len = arr0.shape[0]           # <--- NEW (pre padding)\n",
        "        arr, tag = fix_len(arr0, expect_T)\n",
        "        if tag == \"ok\": n_ok += 1\n",
        "        elif tag == \"pad\": n_pad += 1\n",
        "        else: n_trunc += 1\n",
        "\n",
        "        if len(static_cols) > 0:\n",
        "            s0 = g[static_cols].iloc[0]\n",
        "            s0 = s0.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
        "            s = s0.to_numpy(dtype=np.float32)\n",
        "        else:\n",
        "            s = np.zeros(0, dtype=np.float32)\n",
        "\n",
        "        sample_ids.append(s_id)\n",
        "        lengths.append(min(true_len, expect_T))   # cap to T\n",
        "        X_dyn_list.append(arr)\n",
        "        X_static_list.append(s)\n",
        "\n",
        "    if len(X_dyn_list) == 0:\n",
        "        raise ValueError(\"No sequences assembled. Check 'sample_index' and that each sample has rows.\")\n",
        "\n",
        "    X_dyn = np.stack(X_dyn_list, axis=0)\n",
        "    X_static = np.stack(X_static_list, axis=0)\n",
        "    sample_ids = np.array(sample_ids)\n",
        "    lengths = np.array(lengths, dtype=np.int64)   # <--- NEW\n",
        "\n",
        "    print(f\"[build_sequences] Target T={expect_T} -> ok:{n_ok}  padded:{n_pad}  truncated:{n_trunc}\")\n",
        "\n",
        "    y = None\n",
        "    classes = None\n",
        "    if y_df is not None:\n",
        "        label_cols = [c for c in y_df.columns if c != \"sample_index\"]\n",
        "        assert len(label_cols) == 1, \"y_train must have one target column besides sample_index\"\n",
        "        target_col = label_cols[0]\n",
        "        y_map = y_df.set_index(\"sample_index\")[target_col].to_dict()\n",
        "        y_raw = [y_map[s] for s in sample_ids]\n",
        "        classes = sorted(list(set(y_raw)))\n",
        "        class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "        y = np.array([class_to_idx[v] for v in y_raw], dtype=np.int64)\n",
        "\n",
        "    return X_dyn, X_static, sample_ids, lengths, y, classes, dyn_cols, static_cols\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X_dyn, X_static, lengths, y=None, train=False, aug_p=0.0):\n",
        "        self.X_dyn = X_dyn\n",
        "        self.X_static = X_static\n",
        "        self.lengths = lengths\n",
        "        self.y = y\n",
        "        self.train = train\n",
        "        self.aug_p = aug_p\n",
        "\n",
        "    def _augment(self, x):  # x: [T,C]\n",
        "        # light, safe defaults\n",
        "        import numpy as np\n",
        "        T, C = x.shape\n",
        "        if np.random.rand() < self.aug_p:\n",
        "            x = x + np.random.normal(0, 0.01, size=x.shape)        # jitter\n",
        "        if np.random.rand() < self.aug_p:\n",
        "            scale = 1.0 + np.random.normal(0, 0.05, size=(1, C))   # channel scaling\n",
        "            x = x * scale\n",
        "        if np.random.rand() < self.aug_p:\n",
        "            w = np.random.randint(5, 20)\n",
        "            s = np.random.randint(0, T - w)\n",
        "            x[s:s+w, :] = 0                                        # time mask\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X_dyn.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_dyn = self.X_dyn[idx]      # np array [T,C]\n",
        "        if self.train and self.aug_p > 0:\n",
        "            x_dyn = self._augment(x_dyn.copy()).astype(np.float32)\n",
        "        x_dyn = torch.from_numpy(x_dyn)\n",
        "        x_static = torch.from_numpy(self.X_static[idx])\n",
        "        length = int(self.lengths[idx])\n",
        "        if self.y is None:\n",
        "            return x_dyn, x_static, length\n",
        "        return x_dyn, x_static, length, int(self.y[idx])\n",
        "\n",
        "class AttnPool(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(nn.LayerNorm(d), nn.Linear(d, d//2), nn.Tanh(), nn.Linear(d//2, 1))\n",
        "    def forward(self, x, mask):                 # x:[B,T,D], mask:[B,T] bool\n",
        "        a = self.proj(x).squeeze(-1)            # [B,T]\n",
        "        a = a.masked_fill(~mask, float('-inf'))\n",
        "        w = a.softmax(dim=1)                    # [B,T]\n",
        "        return (x * w.unsqueeze(-1)).sum(1)     # [B,D]\n",
        "\n",
        "# ---------------------------\n",
        "# Model (CNN + BiGRU head)\n",
        "# ---------------------------\n",
        "class PirateNet(nn.Module):\n",
        "    def __init__(self, c_dyn, c_static, hidden=64, rnn_layers=1, num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(c_dyn, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.rnn = nn.GRU(input_size=64, hidden_size=hidden, num_layers=rnn_layers,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "        self.attn = AttnPool(2*hidden)\n",
        "\n",
        "        static_out = 32 if c_static > 0 else 0\n",
        "        if c_static > 0:\n",
        "            self.static_mlp = nn.Sequential(\n",
        "                nn.LayerNorm(c_static),\n",
        "                nn.Linear(c_static, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(64, static_out),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "\n",
        "        head_in = (2*hidden)*3 + static_out  # mean + max + attn\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(head_in, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_dyn, x_static, lengths):\n",
        "        # x_dyn: [B,T,C] -> conv -> [B,T,64] -> BiGRU -> [B,T,2H]\n",
        "        x = self.conv(x_dyn.transpose(1,2)).transpose(1,2)\n",
        "        out, _ = self.rnn(x)\n",
        "\n",
        "        B, T, D = out.shape\n",
        "        device = out.device\n",
        "        # mask: True on real steps, False on padding\n",
        "        lens = lengths.to(device)\n",
        "        ar = torch.arange(T, device=device).unsqueeze(0).expand(B, T)\n",
        "        mask = ar < lens.unsqueeze(1)                         # [B,T] bool\n",
        "\n",
        "        # masked pools\n",
        "        h_mean = (out * mask.unsqueeze(-1)).sum(1) / torch.clamp(lens.unsqueeze(1), min=1).to(out.dtype)\n",
        "        out_masked = out.masked_fill(~mask.unsqueeze(-1), float('-inf'))\n",
        "        h_max = out_masked.max(1).values\n",
        "        h_attn = self.attn(out, mask)\n",
        "\n",
        "        feat = torch.cat([h_mean, h_max, h_attn], dim=1)\n",
        "        if x_static is not None and x_static.shape[1] > 0:\n",
        "            s = self.static_mlp(x_static)\n",
        "            feat = torch.cat([feat, s], dim=1)\n",
        "        return self.head(feat)\n",
        "\n",
        "# ---------------------------\n",
        "# Training / Evaluation\n",
        "# ---------------------------\n",
        "def train_one_epoch(model, loader, optimizer, device, criterion, scheduler=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    preds, trues = [], []\n",
        "    for batch in loader:\n",
        "        xb_dyn, xb_static, xlens, yb = batch\n",
        "        xb_dyn = xb_dyn.to(device)\n",
        "        xb_static = xb_static.to(device)\n",
        "        xlens = torch.as_tensor(xlens, device=device)\n",
        "        yb = torch.as_tensor(yb, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb_dyn, xb_static, xlens)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item() * yb.size(0)\n",
        "        preds.append(logits.detach().softmax(dim=1).cpu().numpy())\n",
        "        trues.append(yb.detach().cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
        "    f1 = f1_score(trues, preds.argmax(1), average=\"macro\")\n",
        "    acc = accuracy_score(trues, preds.argmax(1))\n",
        "    return total_loss / len(loader.dataset), f1, acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds, trues = [], []\n",
        "    for batch in loader:\n",
        "        if len(batch) == 3:     # test loader\n",
        "            xb_dyn, xb_static, xlens = batch\n",
        "            yb = None\n",
        "        else:\n",
        "            xb_dyn, xb_static, xlens, yb = batch\n",
        "            yb = torch.as_tensor(yb, device=device)\n",
        "        xb_dyn = xb_dyn.to(device)\n",
        "        xb_static = xb_static.to(device)\n",
        "        xlens = torch.as_tensor(xlens, device=device)\n",
        "\n",
        "        logits = model(xb_dyn, xb_static, xlens)\n",
        "        if yb is not None:\n",
        "            loss = criterion(logits, yb)\n",
        "            total_loss += loss.item() * yb.size(0)\n",
        "            trues.append(yb.detach().cpu().numpy())\n",
        "        preds.append(logits.detach().softmax(dim=1).cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    if trues:\n",
        "        trues = np.concatenate(trues)\n",
        "        f1 = f1_score(trues, preds.argmax(1), average=\"macro\")\n",
        "        acc = accuracy_score(trues, preds.argmax(1))\n",
        "        return total_loss / len(loader.dataset), f1, acc, preds\n",
        "    return None, None, None, preds\n",
        "\n",
        "\n",
        "def add_deltas(X):  # X: [N,T,C]\n",
        "    d1 = np.diff(X, axis=1, prepend=X[:, :1, :])\n",
        "    d2 = np.diff(d1, axis=1, prepend=d1[:, :1, :])\n",
        "    return np.concatenate([X, d1, d2], axis=2)\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "def main(args):\n",
        "    seed_everything(args.seed)\n",
        "    device = get_device()\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    X_train = pd.read_csv(\"pirate_pain_train.csv\")\n",
        "    y_train = pd.read_csv(\"pirate_pain_train_labels.csv\")\n",
        "    X_test  = pd.read_csv(\"pirate_pain_test.csv\")\n",
        "\n",
        "    # Build sequences\n",
        "    Xdyn_tr, Xsta_tr, ids_tr, len_tr, y, classes, dyn_cols, static_cols = build_sequences(X_train, y_train, expect_T=180)\n",
        "    Xdyn_te, Xsta_te, ids_te, len_te, _, _, _, _ = build_sequences(X_test, None, expect_T=180)\n",
        "    num_classes = len(classes)\n",
        "    print(f\"Train sequences: {len(ids_tr)}  Test sequences: {len(ids_te)}\")\n",
        "    print(f\"Dynamic channels: {Xdyn_tr.shape[-1]}  Static dims: {Xsta_tr.shape[-1]}  Classes: {classes}\")\n",
        "\n",
        "    Xdyn_tr = add_deltas(Xdyn_tr)\n",
        "    Xdyn_te = add_deltas(Xdyn_te)\n",
        "\n",
        "    # CV setup\n",
        "    skf = StratifiedKFold(n_splits=args.folds, shuffle=True, random_state=args.seed)\n",
        "\n",
        "    # OOF storage\n",
        "    oof_pred = np.zeros((len(ids_tr), num_classes), dtype=np.float32)\n",
        "    test_pred_folds = []\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(ids_tr, y), start=1):\n",
        "        print(f\"\\n========== FOLD {fold}/{args.folds} ==========\")\n",
        "        # Fit scalers on train fold ONLY (flatten over time for per-feature scaling)\n",
        "        T, C = Xdyn_tr.shape[1], Xdyn_tr.shape[2]\n",
        "        dyn_scaler = StandardScaler()\n",
        "        dyn_scaler.fit(Xdyn_tr[tr_idx].reshape(-1, C))\n",
        "        Xdyn_tr_scaled = dyn_scaler.transform(Xdyn_tr.reshape(-1, C)).reshape(-1, T, C)\n",
        "        Xdyn_te_scaled = dyn_scaler.transform(Xdyn_te.reshape(-1, C)).reshape(-1, T, C)\n",
        "\n",
        "        if Xsta_tr.shape[1] > 0:\n",
        "            sta_scaler = StandardScaler()\n",
        "            sta_scaler.fit(Xsta_tr[tr_idx])\n",
        "            Xsta_tr_scaled = sta_scaler.transform(Xsta_tr)\n",
        "            Xsta_te_scaled = sta_scaler.transform(Xsta_te)\n",
        "        else:\n",
        "            Xsta_tr_scaled = Xsta_tr\n",
        "            Xsta_te_scaled = Xsta_te\n",
        "\n",
        "        # Datasets & loaders\n",
        "        ds_tr = SequenceDataset(Xdyn_tr_scaled[tr_idx], Xsta_tr_scaled[tr_idx], len_tr[tr_idx], y[tr_idx], train=True, aug_p=0.5)\n",
        "        ds_va = SequenceDataset(Xdyn_tr_scaled[va_idx], Xsta_tr_scaled[va_idx], len_tr[va_idx], y[va_idx], train=False)\n",
        "        dl_tr = DataLoader(ds_tr, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
        "        dl_va = DataLoader(ds_va, batch_size=args.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
        "\n",
        "        # Model\n",
        "        model = PirateNet(c_dyn=C, c_static=Xsta_tr.shape[1], hidden=args.hidden, rnn_layers=1,\n",
        "                          num_classes=num_classes, dropout=args.dropout).to(device)\n",
        "\n",
        "        # Loss (class weights if imbalance)\n",
        "        class_counts = np.bincount(y[tr_idx], minlength=num_classes) + 1  # +1 smoothing\n",
        "        inv_freq = class_counts.sum() / class_counts\n",
        "        # normalize to mean=1 so loss scale stays reasonable\n",
        "        inv_freq = inv_freq / inv_freq.mean()\n",
        "        class_weights = torch.tensor(inv_freq, dtype=torch.float32, device=device)\n",
        "\n",
        "        #criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32, device=device))\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)  # fallback to 0 if your torch is old\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "        #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr, epochs=args.epochs, steps_per_epoch=len(dl_tr))\n",
        "\n",
        "        # Training loop with early stopping on macro-F1\n",
        "        best_f1, patience_left = -1.0, args.patience\n",
        "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}  # init in case no improvement\n",
        "\n",
        "\n",
        "        for epoch in range(1, args.epochs+1):\n",
        "          tr_loss, tr_f1, tr_acc = train_one_epoch(model, dl_tr, optimizer, device, criterion, scheduler)\n",
        "          va_loss, va_f1, va_acc, _ = evaluate(model, dl_va, device, criterion)\n",
        "\n",
        "          print(f\"Epoch {epoch:02d}: \"\n",
        "                f\"train loss {tr_loss:.4f} f1 {tr_f1:.4f} acc {tr_acc:.4f} | \"\n",
        "                f\"val loss {va_loss:.4f} f1 {va_f1:.4f} acc {va_acc:.4f}\")\n",
        "\n",
        "          if va_f1 > best_f1 + 1e-5:\n",
        "              best_f1 = va_f1\n",
        "              best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "              patience_left = args.patience\n",
        "          else:\n",
        "              patience_left -= 1\n",
        "              if patience_left <= 0:\n",
        "                  print(\"Early stopping.\")\n",
        "                  break\n",
        "\n",
        "        # Load best\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "\n",
        "        # Store OOF predictions\n",
        "        _, _, _, va_pred = evaluate(model, dl_va, device, criterion)\n",
        "        oof_pred[va_idx] = va_pred\n",
        "\n",
        "        # Predict test for this fold\n",
        "        dl_te = DataLoader(SequenceDataset(Xdyn_te_scaled, Xsta_te_scaled, len_te, None), batch_size=args.batch_size, shuffle=False)\n",
        "        _, _, _, te_pred = evaluate(model, dl_te, device, criterion)\n",
        "        test_pred_folds.append(te_pred)\n",
        "\n",
        "    # Report OOF score\n",
        "    oof_labels = y\n",
        "    oof_f1 = f1_score(oof_labels, oof_pred.argmax(1), average=\"macro\")\n",
        "    oof_acc = accuracy_score(oof_labels, oof_pred.argmax(1))\n",
        "    print(f\"\\nOOF macro-F1: {oof_f1:.4f} | OOF Acc: {oof_acc:.4f}\")\n",
        "\n",
        "    # Average test predictions across folds\n",
        "    test_pred = np.mean(np.stack(test_pred_folds, axis=0), axis=0)  # [N_test, K]\n",
        "\n",
        "    # Write OOF preds (optional)\n",
        "    pd.DataFrame({\n",
        "        \"sample_index\": ids_tr,\n",
        "        **{f\"prob_{cls}\": oof_pred[:, i] for i, cls in enumerate(classes)},\n",
        "        \"oof_pred\": [classes[i] for i in oof_pred.argmax(1)],\n",
        "        \"target\": [classes[i] for i in oof_labels]\n",
        "    }).to_csv(\"oof_predictions.csv\", index=False)\n",
        "\n",
        "    # Submission: match sample_submission columns if available\n",
        "    submit_col_id = \"sample_index\"\n",
        "    # Try to read sample submission for correct column names/order\n",
        "    label_col_name = \"label\"\n",
        "    if os.path.exists(\"sample_submission.csv\"):\n",
        "        sub_template = pd.read_csv(\"sample_submission.csv\")\n",
        "        submit_col_id = [c for c in sub_template.columns if c != label_col_name][0] if label_col_name in sub_template.columns else \"sample_index\"\n",
        "        if label_col_name not in sub_template.columns:\n",
        "            # try to detect\n",
        "            non_id = [c for c in sub_template.columns if c != submit_col_id]\n",
        "            if len(non_id) == 1:\n",
        "                label_col_name = non_id[0]\n",
        "    test_pred_labels = [classes[i] for i in test_pred.argmax(1)]\n",
        "    submission = pd.DataFrame({submit_col_id: ids_te, label_col_name: test_pred_labels})\n",
        "    submission.to_csv(\"submission.csv\", index=False)\n",
        "    print(\"Wrote submission.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--epochs\", type=int, default=60)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--folds\", type=int, default=5)\n",
        "    p.add_argument(\"--hidden\", type=int, default=64)\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.2)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--weight_decay\", type=float, default=1e-2)\n",
        "    p.add_argument(\"--patience\", type=int, default=10)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    args, _ = p.parse_known_args()  # ignores Jupyter/Colab's extra -f argument\n",
        "    main(args)"
      ]
    }
  ]
}